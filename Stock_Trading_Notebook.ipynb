{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CMPE 260 Reinforcement Learning"
      ],
      "metadata": {
        "id": "4ifCfMpF1lj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Project: Live Trading Bot**\n",
        "\n",
        "**Project Summary:**\n",
        "\n",
        "Trading has become an important sector for investors to invest in. \n",
        "\n",
        "In order to earn profit from stocks or cryptocurrency,  the investor has to make sure to dedicate enough time by checking the market rates for these stocks. investors cannot dedicate 24 hours to monitoring the market. This reduces their chances of doing a profitable transaction at any given point of time. \n",
        "\n",
        "Various aspects that restraining the efficacy of humans in trading in several ways. \n",
        "\n",
        "1.   The reaction time of the investors. \n",
        "2.   Price fluctuation\n",
        "\n",
        "\n",
        "**Team Members:**\n",
        "\n",
        "Akanksha Rawat\n",
        "\n",
        "Karishma Kuria\n",
        "\n",
        "Nisha Mohan Devadiga"
      ],
      "metadata": {
        "id": "5SvyxTSN1nVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install necessary libararies"
      ],
      "metadata": {
        "id": "nAgNN3Ra-R6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3 wandb mplfinance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwjP8qjJjIZe",
        "outputId": "d986c524-a139-473e-a1bd-ffb10a72a073"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-1.6.2-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 42.4 MB/s \n",
            "\u001b[?25hCollecting mplfinance\n",
            "  Downloading mplfinance-0.12.9b5-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.5.0)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 40.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.3.5)\n",
            "Requirement already satisfied: importlib-metadata~=4.13 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (4.13.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.12.1+cu113)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata~=4.13->stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata~=4.13->stable_baselines3) (4.1.1)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 62.6 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 64.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 78.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 106.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 94.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 71.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 87.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 80.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 79.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 99.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 101.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 103.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 102.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 103.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable_baselines3) (2022.6)\n",
            "Building wheels for collected packages: gym, pathtools\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616821 sha256=0413327bf76699fdc9356063fcc46d7dfc678590699d19aa5c2a4e719a89ac0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=6a97dc324d9036a40f9833d03941bc54c47f8c7e23b815fa8e72ba8bcb5cad03\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built gym pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, gym, GitPython, docker-pycreds, wandb, stable-baselines3, mplfinance\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.10 gym-0.21.0 mplfinance-0.12.9b5 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 stable-baselines3-1.6.2 wandb-0.13.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries"
      ],
      "metadata": {
        "id": "fu_6v9gG-jcy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fHm6OtaQijsz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cloning the github repository - Reinforcement-learning-Live-Trading for DEMO."
      ],
      "metadata": {
        "id": "IK8aVJ-E9m7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rm -rf /content/Reinforcement-learning-Live-Trading/Reinforcement-learning-Live-Trading"
      ],
      "metadata": {
        "id": "PeaE_22KJPP-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NishaMDev/Reinforcement-learning-Live-Trading.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkoAWC2susNF",
        "outputId": "2c3dacd0-c835-4aa6-8ebc-78d7dcdcab8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Reinforcement-learning-Live-Trading'...\n",
            "remote: Enumerating objects: 529, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 529 (delta 11), reused 0 (delta 0), pack-reused 502\u001b[K\n",
            "Receiving objects: 100% (529/529), 22.96 MiB | 19.97 MiB/s, done.\n",
            "Resolving deltas: 100% (235/235), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Stock data for AAPL and merge them into one dataset."
      ],
      "metadata": {
        "id": "5kcYbE7W-vnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the directory"
      ],
      "metadata": {
        "id": "ofdhR-4f9uMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Reinforcement-learning-Live-Trading"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaULbhfLu5rD",
        "outputId": "b802e751-3072-419f-f508-4da361005f94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Reinforcement-learning-Live-Trading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge all the stock data for AAPL into final csv file for learning."
      ],
      "metadata": {
        "id": "zhsdhETF9xqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UM1KepBnijs2"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([pd.read_csv(f, sep=',') for f in glob.glob('data' + \"/AAPL_2*.csv\")],\n",
        "                      ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3SpnZlA7ijs3",
        "outputId": "cd49b0c1-7387-41b8-cad0-c9dd582c2969"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date    Open    High     Low   Close       Volume\n",
              "0  11/16/2020  118.92  120.99  118.15  120.30   91,183,023\n",
              "1  11/13/2020  119.44  119.67  117.87  119.26   81,688,594\n",
              "2  11/12/2020  119.62  120.53  118.57  119.21  103,350,703\n",
              "3  11/11/2020  117.19  119.63  116.44  119.49  112,295,000\n",
              "4  11/10/2020  115.55  117.59  114.13  115.97  138,023,391"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-647dae23-2a4a-4905-a9eb-55d29c541b45\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11/16/2020</td>\n",
              "      <td>118.92</td>\n",
              "      <td>120.99</td>\n",
              "      <td>118.15</td>\n",
              "      <td>120.30</td>\n",
              "      <td>91,183,023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11/13/2020</td>\n",
              "      <td>119.44</td>\n",
              "      <td>119.67</td>\n",
              "      <td>117.87</td>\n",
              "      <td>119.26</td>\n",
              "      <td>81,688,594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11/12/2020</td>\n",
              "      <td>119.62</td>\n",
              "      <td>120.53</td>\n",
              "      <td>118.57</td>\n",
              "      <td>119.21</td>\n",
              "      <td>103,350,703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11/11/2020</td>\n",
              "      <td>117.19</td>\n",
              "      <td>119.63</td>\n",
              "      <td>116.44</td>\n",
              "      <td>119.49</td>\n",
              "      <td>112,295,000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11/10/2020</td>\n",
              "      <td>115.55</td>\n",
              "      <td>117.59</td>\n",
              "      <td>114.13</td>\n",
              "      <td>115.97</td>\n",
              "      <td>138,023,391</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-647dae23-2a4a-4905-a9eb-55d29c541b45')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-647dae23-2a4a-4905-a9eb-55d29c541b45 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-647dae23-2a4a-4905-a9eb-55d29c541b45');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlo4T6qhijs4",
        "outputId": "e377c815-867f-482c-a88f-52ebcea49f9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date       object\n",
              "Open      float64\n",
              "High      float64\n",
              "Low       float64\n",
              "Close     float64\n",
              "Volume     object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert data field as datetime."
      ],
      "metadata": {
        "id": "aALNVXss-AI7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Jn3Sp6Qhijs4"
      },
      "outputs": [],
      "source": [
        "df['Date'] = pd.to_datetime(df ['Date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzsgHmAlijs5",
        "outputId": "7fc16f1f-4e1c-427f-a374-57e63cbe714f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date      datetime64[ns]\n",
              "Open             float64\n",
              "High             float64\n",
              "Low              float64\n",
              "Close            float64\n",
              "Volume            object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LUFNqX01ijs5",
        "outputId": "e8404066-747d-4e4d-a378-56f3fbe4df4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Date    Open    High     Low   Close       Volume\n",
              "0 2020-11-16  118.92  120.99  118.15  120.30   91,183,023\n",
              "1 2020-11-13  119.44  119.67  117.87  119.26   81,688,594\n",
              "2 2020-11-12  119.62  120.53  118.57  119.21  103,350,703\n",
              "3 2020-11-11  117.19  119.63  116.44  119.49  112,295,000\n",
              "4 2020-11-10  115.55  117.59  114.13  115.97  138,023,391"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b95a484e-51bd-447c-ab7d-118f7c43f0e2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-11-16</td>\n",
              "      <td>118.92</td>\n",
              "      <td>120.99</td>\n",
              "      <td>118.15</td>\n",
              "      <td>120.30</td>\n",
              "      <td>91,183,023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-11-13</td>\n",
              "      <td>119.44</td>\n",
              "      <td>119.67</td>\n",
              "      <td>117.87</td>\n",
              "      <td>119.26</td>\n",
              "      <td>81,688,594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-11-12</td>\n",
              "      <td>119.62</td>\n",
              "      <td>120.53</td>\n",
              "      <td>118.57</td>\n",
              "      <td>119.21</td>\n",
              "      <td>103,350,703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-11-11</td>\n",
              "      <td>117.19</td>\n",
              "      <td>119.63</td>\n",
              "      <td>116.44</td>\n",
              "      <td>119.49</td>\n",
              "      <td>112,295,000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-11-10</td>\n",
              "      <td>115.55</td>\n",
              "      <td>117.59</td>\n",
              "      <td>114.13</td>\n",
              "      <td>115.97</td>\n",
              "      <td>138,023,391</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b95a484e-51bd-447c-ab7d-118f7c43f0e2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b95a484e-51bd-447c-ab7d-118f7c43f0e2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b95a484e-51bd-447c-ab7d-118f7c43f0e2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjQH239Eijs6"
      },
      "source": [
        "We need to convert the values from string to float type. Otherwise, it won't work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4fPY898Sijs7"
      },
      "outputs": [],
      "source": [
        "df['Volume'] = df['Volume'].apply(lambda x: float(x.replace(\",\", \"\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "L8j972XJijs8",
        "outputId": "596db21e-2763-4ab8-926a-8e1c98a21940"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Date    Open    High     Low   Close       Volume\n",
              "0 2020-11-16  118.92  120.99  118.15  120.30   91183023.0\n",
              "1 2020-11-13  119.44  119.67  117.87  119.26   81688594.0\n",
              "2 2020-11-12  119.62  120.53  118.57  119.21  103350703.0\n",
              "3 2020-11-11  117.19  119.63  116.44  119.49  112295000.0\n",
              "4 2020-11-10  115.55  117.59  114.13  115.97  138023391.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb8b1f23-5e75-4134-84e3-089f98b22d00\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-11-16</td>\n",
              "      <td>118.92</td>\n",
              "      <td>120.99</td>\n",
              "      <td>118.15</td>\n",
              "      <td>120.30</td>\n",
              "      <td>91183023.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-11-13</td>\n",
              "      <td>119.44</td>\n",
              "      <td>119.67</td>\n",
              "      <td>117.87</td>\n",
              "      <td>119.26</td>\n",
              "      <td>81688594.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-11-12</td>\n",
              "      <td>119.62</td>\n",
              "      <td>120.53</td>\n",
              "      <td>118.57</td>\n",
              "      <td>119.21</td>\n",
              "      <td>103350703.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-11-11</td>\n",
              "      <td>117.19</td>\n",
              "      <td>119.63</td>\n",
              "      <td>116.44</td>\n",
              "      <td>119.49</td>\n",
              "      <td>112295000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-11-10</td>\n",
              "      <td>115.55</td>\n",
              "      <td>117.59</td>\n",
              "      <td>114.13</td>\n",
              "      <td>115.97</td>\n",
              "      <td>138023391.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb8b1f23-5e75-4134-84e3-089f98b22d00')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb8b1f23-5e75-4134-84e3-089f98b22d00 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb8b1f23-5e75-4134-84e3-089f98b22d00');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_3uCQciijs8",
        "outputId": "3657a459-8de6-4573-bce7-770e3e08357f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date      datetime64[ns]\n",
              "Open             float64\n",
              "High             float64\n",
              "Low              float64\n",
              "Close            float64\n",
              "Volume           float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "q9hFxdNLijs8",
        "outputId": "21dc4ae2-eb14-492d-8d5e-60980f8c5041"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Date   Open   High    Low  Close       Volume\n",
              "502 2018-11-19  47.50  47.68  46.25  46.47  167683484.0\n",
              "501 2018-11-20  44.59  45.37  43.88  44.25  271301000.0\n",
              "500 2018-11-21  44.93  45.07  44.14  44.20  124496844.0\n",
              "499 2018-11-23  43.74  44.15  43.03  43.07   94495884.0\n",
              "498 2018-11-26  43.56  43.74  42.57  43.66  179994080.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13e46f82-4405-4ad4-ae0b-03b2608b4493\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>2018-11-19</td>\n",
              "      <td>47.50</td>\n",
              "      <td>47.68</td>\n",
              "      <td>46.25</td>\n",
              "      <td>46.47</td>\n",
              "      <td>167683484.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>2018-11-20</td>\n",
              "      <td>44.59</td>\n",
              "      <td>45.37</td>\n",
              "      <td>43.88</td>\n",
              "      <td>44.25</td>\n",
              "      <td>271301000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>2018-11-21</td>\n",
              "      <td>44.93</td>\n",
              "      <td>45.07</td>\n",
              "      <td>44.14</td>\n",
              "      <td>44.20</td>\n",
              "      <td>124496844.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>2018-11-23</td>\n",
              "      <td>43.74</td>\n",
              "      <td>44.15</td>\n",
              "      <td>43.03</td>\n",
              "      <td>43.07</td>\n",
              "      <td>94495884.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>2018-11-26</td>\n",
              "      <td>43.56</td>\n",
              "      <td>43.74</td>\n",
              "      <td>42.57</td>\n",
              "      <td>43.66</td>\n",
              "      <td>179994080.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13e46f82-4405-4ad4-ae0b-03b2608b4493')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13e46f82-4405-4ad4-ae0b-03b2608b4493 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13e46f82-4405-4ad4-ae0b-03b2608b4493');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df = df.sort_values('Date')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EWXVsETmijs9"
      },
      "outputs": [],
      "source": [
        "df.to_csv('data/AAPl.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "W87lcTc6ijs9",
        "outputId": "3918c472-8276-4d10-911d-4fcd7189489d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Date    Open    High     Low   Close       Volume\n",
              "0    2018-11-19   47.50   47.68   46.25   46.47  167683484.0\n",
              "1    2018-11-20   44.59   45.37   43.88   44.25  271301000.0\n",
              "2    2018-11-21   44.93   45.07   44.14   44.20  124496844.0\n",
              "3    2018-11-23   43.74   44.15   43.03   43.07   94495884.0\n",
              "4    2018-11-26   43.56   43.74   42.57   43.66  179994080.0\n",
              "..          ...     ...     ...     ...     ...          ...\n",
              "995  2022-10-28  148.20  157.50  147.82  155.74  164762406.0\n",
              "996  2022-10-31  153.16  154.24  151.92  153.34   97943172.0\n",
              "997  2022-11-01  155.08  155.45  149.13  150.65   80379352.0\n",
              "998  2022-11-02  148.95  152.17  145.00  145.03   93604625.0\n",
              "999  2022-11-03  142.06  142.80  138.75  138.88   97918523.0\n",
              "\n",
              "[1000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c40ce859-e2b6-4677-b19f-149cc10cebcd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-11-19</td>\n",
              "      <td>47.50</td>\n",
              "      <td>47.68</td>\n",
              "      <td>46.25</td>\n",
              "      <td>46.47</td>\n",
              "      <td>167683484.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-11-20</td>\n",
              "      <td>44.59</td>\n",
              "      <td>45.37</td>\n",
              "      <td>43.88</td>\n",
              "      <td>44.25</td>\n",
              "      <td>271301000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-11-21</td>\n",
              "      <td>44.93</td>\n",
              "      <td>45.07</td>\n",
              "      <td>44.14</td>\n",
              "      <td>44.20</td>\n",
              "      <td>124496844.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-11-23</td>\n",
              "      <td>43.74</td>\n",
              "      <td>44.15</td>\n",
              "      <td>43.03</td>\n",
              "      <td>43.07</td>\n",
              "      <td>94495884.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-11-26</td>\n",
              "      <td>43.56</td>\n",
              "      <td>43.74</td>\n",
              "      <td>42.57</td>\n",
              "      <td>43.66</td>\n",
              "      <td>179994080.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>2022-10-28</td>\n",
              "      <td>148.20</td>\n",
              "      <td>157.50</td>\n",
              "      <td>147.82</td>\n",
              "      <td>155.74</td>\n",
              "      <td>164762406.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>2022-10-31</td>\n",
              "      <td>153.16</td>\n",
              "      <td>154.24</td>\n",
              "      <td>151.92</td>\n",
              "      <td>153.34</td>\n",
              "      <td>97943172.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>2022-11-01</td>\n",
              "      <td>155.08</td>\n",
              "      <td>155.45</td>\n",
              "      <td>149.13</td>\n",
              "      <td>150.65</td>\n",
              "      <td>80379352.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>148.95</td>\n",
              "      <td>152.17</td>\n",
              "      <td>145.00</td>\n",
              "      <td>145.03</td>\n",
              "      <td>93604625.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>2022-11-03</td>\n",
              "      <td>142.06</td>\n",
              "      <td>142.80</td>\n",
              "      <td>138.75</td>\n",
              "      <td>138.88</td>\n",
              "      <td>97918523.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c40ce859-e2b6-4677-b19f-149cc10cebcd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c40ce859-e2b6-4677-b19f-149cc10cebcd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c40ce859-e2b6-4677-b19f-149cc10cebcd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "new_df = pd.read_csv('data/AAPL.csv')\n",
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8DwFODAPknY1",
        "outputId": "509536f8-be4b-46b9-de79-5e914bfc41e5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Reinforcement-learning-Live-Trading'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stock-Trading Inferences by running different algorithms - "
      ],
      "metadata": {
        "id": "UpAkHMN--Lj5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKCdvCjDijs-"
      },
      "source": [
        "# A - PPO ALgorithm - Proximal Policy Optimization(PPO).\n",
        "\n",
        "PPO is a policy gradient method where policy is updated explicitly. We can write the objective function or loss function of vanilla policy gradient with advantage function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment#1 - **PPO over 100000 steps**\n",
        "\n",
        "Experiment PPO algo with default params - gamma=0.99 , learning_rate=0.0007 , ent_coef=0.4"
      ],
      "metadata": {
        "id": "qF5E4N4KgdYR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "069f97044e2f4c7b96ca3c5da019d5ca",
            "7aeeb78053f8467ebe86ac237884a012",
            "259631baca6641b0923baa7d760a5b9c",
            "01316c1804dd480ab714d3978a72a83a",
            "fc0e854f234644aa8702575e15a67c03",
            "c78bdd79a91446879b8d8333bfeb6c32",
            "454ffe0fb1ab4a48b13ec4caf0339f94",
            "4f00f85464cb4664af2bcf7448823cb5"
          ]
        },
        "id": "8U5qxuhGijs-",
        "outputId": "42b2c151-3810-45ff-b625-f10046ae1b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668990466666856, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "069f97044e2f4c7b96ca3c5da019d5ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Reinforcement-learning-Live-Trading/wandb/run-20221124_212202-3ay74for</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/nishamdev/StockTrading/runs/3ay74for\" target=\"_blank\">laced-sun-1</a></strong> to <a href=\"https://wandb.ai/nishamdev/StockTrading\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
            "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Logging to runs/3ay74for/PPO_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Reinforcement-learning-Live-Trading/env/stock_trading_env.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  prev_cost + additional_cost) / (self.shares_held + shares_bought)\n",
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=1000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 110  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 18   |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=3000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 3000          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.1129457e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.84         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.9e+08       |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -9.8e-05      |\n",
            "|    std                  | 0.999         |\n",
            "|    value_loss           | 1.92e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 113  |\n",
            "|    iterations      | 2    |\n",
            "|    time_elapsed    | 36   |\n",
            "|    total_timesteps | 4096 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=5000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 5000          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4546386e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.84         |\n",
            "|    explained_variance   | 1.48e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.76e+08      |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -6.01e-05     |\n",
            "|    std                  | 0.999         |\n",
            "|    value_loss           | 9.83e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 113  |\n",
            "|    iterations      | 3    |\n",
            "|    time_elapsed    | 54   |\n",
            "|    total_timesteps | 6144 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=7000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 7000          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3444806e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.84         |\n",
            "|    explained_variance   | 5.19e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.92e+08      |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -6.18e-05     |\n",
            "|    std                  | 0.999         |\n",
            "|    value_loss           | 1.06e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 113  |\n",
            "|    iterations      | 4    |\n",
            "|    time_elapsed    | 72   |\n",
            "|    total_timesteps | 8192 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=9000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 9000          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.0598527e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.84         |\n",
            "|    explained_variance   | 2.32e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.04e+09      |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -6.67e-05     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.27e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 114   |\n",
            "|    iterations      | 5     |\n",
            "|    time_elapsed    | 89    |\n",
            "|    total_timesteps | 10240 |\n",
            "------------------------------\n",
            "Eval num_timesteps=11000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 11000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.4130505e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.84         |\n",
            "|    explained_variance   | 1.85e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.35e+08      |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.000156     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 9.27e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 6     |\n",
            "|    time_elapsed    | 106   |\n",
            "|    total_timesteps | 12288 |\n",
            "------------------------------\n",
            "Eval num_timesteps=13000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 13000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.7817302e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.84         |\n",
            "|    explained_variance   | 8.34e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.01e+08      |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000133     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.13e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 7     |\n",
            "|    time_elapsed    | 124   |\n",
            "|    total_timesteps | 14336 |\n",
            "------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 15000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.044639e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.84        |\n",
            "|    explained_variance   | 5.96e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.64e+08     |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -9.68e-05    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.06e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 16000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 8     |\n",
            "|    time_elapsed    | 141   |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "Eval num_timesteps=17000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 17000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.4460834e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.84         |\n",
            "|    explained_variance   | 6.56e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.53e+08      |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.000131     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 9.78e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 9     |\n",
            "|    time_elapsed    | 158   |\n",
            "|    total_timesteps | 18432 |\n",
            "------------------------------\n",
            "Eval num_timesteps=19000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 19000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.3570028e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.84         |\n",
            "|    explained_variance   | 2.98e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.07e+08      |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -6.64e-05     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.66e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 10    |\n",
            "|    time_elapsed    | 175   |\n",
            "|    total_timesteps | 20480 |\n",
            "------------------------------\n",
            "Eval num_timesteps=21000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 21000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7392158e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 4.17e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.95e+08      |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -5.7e-05      |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.14e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 22000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 11    |\n",
            "|    time_elapsed    | 192   |\n",
            "|    total_timesteps | 22528 |\n",
            "------------------------------\n",
            "Eval num_timesteps=23000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 23000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.536161e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | 3.58e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.38e+08     |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -1.86e-05    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.61e+08     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 24000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 12    |\n",
            "|    time_elapsed    | 209   |\n",
            "|    total_timesteps | 24576 |\n",
            "------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 25000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.0373214e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.46e+08      |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -2.12e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 8.09e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 26000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 13    |\n",
            "|    time_elapsed    | 226   |\n",
            "|    total_timesteps | 26624 |\n",
            "------------------------------\n",
            "Eval num_timesteps=27000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 3e+06       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 27000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 7.49444e-06 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.85       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.64e+08    |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.000181   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 8.04e+08    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 28000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 14    |\n",
            "|    time_elapsed    | 244   |\n",
            "|    total_timesteps | 28672 |\n",
            "------------------------------\n",
            "Eval num_timesteps=29000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 29000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.2340973e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.39e+08      |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -8.64e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.38e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 30000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 15    |\n",
            "|    time_elapsed    | 261   |\n",
            "|    total_timesteps | 30720 |\n",
            "------------------------------\n",
            "Eval num_timesteps=31000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 31000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2485834e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.1e+09       |\n",
            "|    n_updates            | 150           |\n",
            "|    policy_gradient_loss | -5.43e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.67e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 32000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 16    |\n",
            "|    time_elapsed    | 278   |\n",
            "|    total_timesteps | 32768 |\n",
            "------------------------------\n",
            "Eval num_timesteps=33000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 33000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3104291e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.61e+08      |\n",
            "|    n_updates            | 160           |\n",
            "|    policy_gradient_loss | -4.37e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.35e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 34000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 17    |\n",
            "|    time_elapsed    | 296   |\n",
            "|    total_timesteps | 34816 |\n",
            "------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 35000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.6519777e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.43e+08      |\n",
            "|    n_updates            | 170           |\n",
            "|    policy_gradient_loss | -0.000103     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.4e+09       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 36000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 18    |\n",
            "|    time_elapsed    | 313   |\n",
            "|    total_timesteps | 36864 |\n",
            "------------------------------\n",
            "Eval num_timesteps=37000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 37000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7164275e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.89e+08      |\n",
            "|    n_updates            | 180           |\n",
            "|    policy_gradient_loss | -5.72e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 8.91e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 38000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 19    |\n",
            "|    time_elapsed    | 330   |\n",
            "|    total_timesteps | 38912 |\n",
            "------------------------------\n",
            "Eval num_timesteps=39000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 39000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.6841805e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.55e+08      |\n",
            "|    n_updates            | 190           |\n",
            "|    policy_gradient_loss | -5.86e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.12e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 40000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 20    |\n",
            "|    time_elapsed    | 348   |\n",
            "|    total_timesteps | 40960 |\n",
            "------------------------------\n",
            "Eval num_timesteps=41000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 41000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8515275e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.93e+08      |\n",
            "|    n_updates            | 200           |\n",
            "|    policy_gradient_loss | -6.98e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.09e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 42000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=43000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 43000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 21    |\n",
            "|    time_elapsed    | 371   |\n",
            "|    total_timesteps | 43008 |\n",
            "------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 44000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.1932723e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.95e+08      |\n",
            "|    n_updates            | 210           |\n",
            "|    policy_gradient_loss | -9.13e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.09e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 45000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 22    |\n",
            "|    time_elapsed    | 388   |\n",
            "|    total_timesteps | 45056 |\n",
            "------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 46000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.6967685e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.67e+08      |\n",
            "|    n_updates            | 220           |\n",
            "|    policy_gradient_loss | -1.3e-05      |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.09e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=47000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 47000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 23    |\n",
            "|    time_elapsed    | 406   |\n",
            "|    total_timesteps | 47104 |\n",
            "------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 48000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.184803e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.86        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.76e+08     |\n",
            "|    n_updates            | 230          |\n",
            "|    policy_gradient_loss | -0.000108    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 9.07e+08     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=49000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 49000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 24    |\n",
            "|    time_elapsed    | 423   |\n",
            "|    total_timesteps | 49152 |\n",
            "------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 50000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.038331e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.86        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.48e+08     |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.000107    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 7.82e+08     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=51000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 51000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 25    |\n",
            "|    time_elapsed    | 440   |\n",
            "|    total_timesteps | 51200 |\n",
            "------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 52000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.120813e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.86        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.8e+08      |\n",
            "|    n_updates            | 250          |\n",
            "|    policy_gradient_loss | -0.000161    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 1.02e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=53000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 53000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 26    |\n",
            "|    time_elapsed    | 458   |\n",
            "|    total_timesteps | 53248 |\n",
            "------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 3e+06       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 54000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 4.81616e-06 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.86       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.19e+08    |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.000143   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 9.24e+08    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=55000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 55000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 27    |\n",
            "|    time_elapsed    | 475   |\n",
            "|    total_timesteps | 55296 |\n",
            "------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 56000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.2405736e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.71e+08      |\n",
            "|    n_updates            | 270           |\n",
            "|    policy_gradient_loss | -8.87e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 6.8e+08       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=57000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 57000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 28    |\n",
            "|    time_elapsed    | 492   |\n",
            "|    total_timesteps | 57344 |\n",
            "------------------------------\n",
            "Eval num_timesteps=58000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 58000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.6979833e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.17e+08      |\n",
            "|    n_updates            | 280           |\n",
            "|    policy_gradient_loss | -0.00011      |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.46e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=59000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 59000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 29    |\n",
            "|    time_elapsed    | 510   |\n",
            "|    total_timesteps | 59392 |\n",
            "------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 60000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2759119e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.87e+08      |\n",
            "|    n_updates            | 290           |\n",
            "|    policy_gradient_loss | -1.23e-06     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 8.04e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=61000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 61000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 30    |\n",
            "|    time_elapsed    | 527   |\n",
            "|    total_timesteps | 61440 |\n",
            "------------------------------\n",
            "Eval num_timesteps=62000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 62000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.0104926e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.85e+08      |\n",
            "|    n_updates            | 300           |\n",
            "|    policy_gradient_loss | -4.31e-06     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.04e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=63000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 63000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 31    |\n",
            "|    time_elapsed    | 544   |\n",
            "|    total_timesteps | 63488 |\n",
            "------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 64000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.9318908e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.08e+08      |\n",
            "|    n_updates            | 310           |\n",
            "|    policy_gradient_loss | -0.000124     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.12e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=65000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 65000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 32    |\n",
            "|    time_elapsed    | 561   |\n",
            "|    total_timesteps | 65536 |\n",
            "------------------------------\n",
            "Eval num_timesteps=66000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 66000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.998531e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.86        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.44e+08     |\n",
            "|    n_updates            | 320          |\n",
            "|    policy_gradient_loss | -6.66e-05    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 1.23e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=67000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 67000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 33    |\n",
            "|    time_elapsed    | 578   |\n",
            "|    total_timesteps | 67584 |\n",
            "------------------------------\n",
            "Eval num_timesteps=68000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 68000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.0542017e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.27e+09      |\n",
            "|    n_updates            | 330           |\n",
            "|    policy_gradient_loss | -4.12e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 2.25e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=69000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 69000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 34    |\n",
            "|    time_elapsed    | 595   |\n",
            "|    total_timesteps | 69632 |\n",
            "------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 70000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.3076656e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.17e+08      |\n",
            "|    n_updates            | 340           |\n",
            "|    policy_gradient_loss | -1.76e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.19e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=71000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 71000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 35    |\n",
            "|    time_elapsed    | 612   |\n",
            "|    total_timesteps | 71680 |\n",
            "------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 72000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.9307786e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.39e+08      |\n",
            "|    n_updates            | 350           |\n",
            "|    policy_gradient_loss | -2.53e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.58e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=73000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 73000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 36    |\n",
            "|    time_elapsed    | 629   |\n",
            "|    total_timesteps | 73728 |\n",
            "------------------------------\n",
            "Eval num_timesteps=74000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 74000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.281123e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.87        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.67e+08     |\n",
            "|    n_updates            | 360          |\n",
            "|    policy_gradient_loss | -0.000158    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 1.1e+09      |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=75000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 75000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 37    |\n",
            "|    time_elapsed    | 646   |\n",
            "|    total_timesteps | 75776 |\n",
            "------------------------------\n",
            "Eval num_timesteps=76000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 76000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.390351e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.87        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.55e+08     |\n",
            "|    n_updates            | 370          |\n",
            "|    policy_gradient_loss | -2.69e-05    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 1.45e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=77000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 77000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 38    |\n",
            "|    time_elapsed    | 663   |\n",
            "|    total_timesteps | 77824 |\n",
            "------------------------------\n",
            "Eval num_timesteps=78000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 78000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2651144e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.17e+08      |\n",
            "|    n_updates            | 380           |\n",
            "|    policy_gradient_loss | -3.26e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.23e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=79000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 79000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 39    |\n",
            "|    time_elapsed    | 680   |\n",
            "|    total_timesteps | 79872 |\n",
            "------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 80000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.8515933e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.47e+08      |\n",
            "|    n_updates            | 390           |\n",
            "|    policy_gradient_loss | -9.27e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 7.62e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=81000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 81000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 40    |\n",
            "|    time_elapsed    | 697   |\n",
            "|    total_timesteps | 81920 |\n",
            "------------------------------\n",
            "Eval num_timesteps=82000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 82000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8810097e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.48e+08      |\n",
            "|    n_updates            | 400           |\n",
            "|    policy_gradient_loss | -7.4e-05      |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.24e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=83000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 83000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 117   |\n",
            "|    iterations      | 41    |\n",
            "|    time_elapsed    | 715   |\n",
            "|    total_timesteps | 83968 |\n",
            "------------------------------\n",
            "Eval num_timesteps=84000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 84000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.2706517e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | -2.38e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.79e+08      |\n",
            "|    n_updates            | 410           |\n",
            "|    policy_gradient_loss | -0.00011      |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.37e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=85000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 85000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=86000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 86000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 42    |\n",
            "|    time_elapsed    | 738   |\n",
            "|    total_timesteps | 86016 |\n",
            "------------------------------\n",
            "Eval num_timesteps=87000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 87000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.2357943e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.55e+08      |\n",
            "|    n_updates            | 420           |\n",
            "|    policy_gradient_loss | -2.59e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 9.71e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 88000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 43    |\n",
            "|    time_elapsed    | 755   |\n",
            "|    total_timesteps | 88064 |\n",
            "------------------------------\n",
            "Eval num_timesteps=89000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 89000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.0224785e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.63e+08      |\n",
            "|    n_updates            | 430           |\n",
            "|    policy_gradient_loss | -4.33e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.16e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 90000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 44    |\n",
            "|    time_elapsed    | 772   |\n",
            "|    total_timesteps | 90112 |\n",
            "------------------------------\n",
            "Eval num_timesteps=91000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 3e+06       |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 91000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 3.94386e-07 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.87       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.11e+09    |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -1.79e-05   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 1.32e+09    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=92000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 45    |\n",
            "|    time_elapsed    | 789   |\n",
            "|    total_timesteps | 92160 |\n",
            "------------------------------\n",
            "Eval num_timesteps=93000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 93000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.6457925e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.59e+08      |\n",
            "|    n_updates            | 450           |\n",
            "|    policy_gradient_loss | -9.32e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.57e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=94000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 46    |\n",
            "|    time_elapsed    | 806   |\n",
            "|    total_timesteps | 94208 |\n",
            "------------------------------\n",
            "Eval num_timesteps=95000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 95000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.294546e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.87        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.24e+08     |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | -6.81e-06    |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 1.05e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 47    |\n",
            "|    time_elapsed    | 823   |\n",
            "|    total_timesteps | 96256 |\n",
            "------------------------------\n",
            "Eval num_timesteps=97000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 97000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.2358727e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.4e+08       |\n",
            "|    n_updates            | 470           |\n",
            "|    policy_gradient_loss | -9.55e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 9e+08         |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=98000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 48    |\n",
            "|    time_elapsed    | 840   |\n",
            "|    total_timesteps | 98304 |\n",
            "------------------------------\n",
            "Eval num_timesteps=99000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 99000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0543736e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.75e+08      |\n",
            "|    n_updates            | 480           |\n",
            "|    policy_gradient_loss | -4.98e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.8e+09       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 116    |\n",
            "|    iterations      | 49     |\n",
            "|    time_elapsed    | 858    |\n",
            "|    total_timesteps | 100352 |\n",
            "-------------------------------\n",
            "mean_reward:3003000.00 +/- 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x576 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5MAAAHgCAYAAAA4xjnPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd1hT1xvA8W8S9lBQQVyoKFVx1oGzVdy4WuusHc6qqHXVba21daHWUVcddVer4h61LtRiVX6OuhWsilsQZc8k9/eHkJIQdiAg5/M8Po935J7DJUDee855X1lYWJiEIAiCIAiCIAiCIGSB3NgdEARBEARBEARBEAoeEUwKgiAIgiAIgiAIWSaCSUEQBEEQBEEQBCHLRDApCIIgCIIgCIIgZJkIJgVBEARBEARBEIQsE8GkIAiCIAiCIAiCkGUimBQEQRAEQRAEQRCyTASTgiAIgiAIgiAIQpYZNJg8d+4ca9as0dq3a9cu6tevj6urK5MmTUKtVhuySUEQBEEQBEEQBMEIDBpMzpo1i7///luzfe/ePby8vJDL5dSpU4fVq1fzyy+/GLJJQRAEQRAEQRAEwQgMGkzeuXOHevXqabZ///13LCwsOH78ODt37qRXr15s2bLFkE0KgiAIgiAIgiAIRmDQYDIyMhI7OzvN9okTJ/Dw8KBIkSIANG7cmEePHhmySUEQBEEQBEEQBMEIDBpMOjk5cffuXQCeP3/OtWvXaNmypeZ4REQEJiYmhmxSEARBEARBEARBMAKDRnadO3dmzZo1xMfHc+nSJSwsLOjQoYPm+I0bNyhfvrwhmxQEQRAEQRAEQRCMwKAjk5MnT6ZLly7s2LGDkJAQVqxYgYODA/B2VPLAgQN4eHgYsklBEARBKBDu3r3LoUOHtPadPXuWTz75hFatWrFixQoj9UwQBEEQskcWFhYm5UVDarWayMhIrKysMDU1zYsmBUEQBCHf6NGjBzKZjB07dgDw9OlTGjZsiLm5OQ4ODgQEBLBs2TL69Olj5J4KgiAIQuYYdGRy/fr1hIeH629ILqdo0aIikBQEQRAKpatXr9K0aVPN9vbt21Gr1fj5+XH+/HnatWvH2rVrjdhDQRAEQcgagwaTY8eOpUqVKvTt25fDhw+jVCoNeXlBEARBKLDCw8MpXry4ZvvYsWN88MEHlCpVCoB27dpx7949Y3VPEARBELLMoMHkX3/9xeDBg7l06RKfffYZVapUYfz48Vy8eNGQzQiCIAhCgePg4KApjxUWFsbFixe18gjEx8cbq2uCIAiCkC0GzeZao0YNatSowYwZMzhz5gw7duxg+/bt/Prrr7i4uNCrVy969OhBhQoVDNmsIAiCIOR7Hh4erF69miJFiuDn5weglfH8zp07lClTxljdEwRBEIQsy/UEPPHx8fzxxx9s3rwZX19fABo2bMinn35Kz549sbCwyM3mBUEQBCFfCAkJ4csvv+T8+fOYmZnx/fff4+XlBUBcXBzVqlWjZ8+eeHt7G7mngiAIgpA5Bp3mqs+lS5fw9fXl4sWLSJKEm5sb8fHxjBo1ijp16nD27Nnc7kK+EhgYaOwuvBPEfTQccS8NR9xLw3kX76WDgwN//PEHDx8+5PHjx5pAEkCSJPbv38+kSZOM2MOC5118nxiDuI+GI+6l4Yh7aTi5eS9zJZgMCAjgxx9/pFatWnTq1Ik//viDzz//nDNnzuDn58fJkyc5ffo0Dg4OjB07Nje6IAiCIAj5yokTJ5AkiaJFi2JmZqZ1zNLSkpo1a2Jvb2+k3gmCIAhC1hl0zeSKFSvYsWMH165dw8zMDE9PT+bPn0/r1q1RKBRa59aqVQsvLy++/vprQ3ZBEARBEPKl7t274+TkRPfu3enZsyc1a9Y0dpcEQRAEIUcMGkxOnToVd3d3fvrpJ7p27YqdnV2657///vuMHz/ekF0QBEEQ8rFLLy5x+eVl1JI6zXNqOdSiBCXysFd547fffmPHjh2sXbuW5cuXU7VqVT799FO6d++uKQ8iCIIgCAWJQRPw3L9/HxcXF0Nd7p0UGBiIq6ursbtR4In7aDjiXhqOuJfp2313NwP+GJDheV/X+5ovnb58Z+9lZGQke/fuZefOnZw9exaZTMYHH3xA79696dy5M1ZWVsbuolFk9ABaEARByHthYWHpHjfoyKQIJN9SqVRp1gszMzMjJiYmj3v07hH3MfvMzc1TTTsXhLyw5eYWY3chX7C1teWLL77giy++4NmzZ/j4+LBjxw68vLz45ptv6NixI3369KF58+bG7qogCIIgpMugwSS8TW9+4MAB/vnnHyIiIlCrtacyyWQyli1bZuhm8w2VSkVcXBxWVlbIZLJUx21sbArtU2dDEvcxeyRJIiYmBgsLCxFQCnnuUeQjY3ch31GpVCQmJpKQkIAkSVhYWHD69Gl27NhBjRo1WLVqFW5ubsbupiAIgiDoZdBg8smTJ3Tu3JmHDx9StGhRIiIisLe3JywsDLVaTfHixbG2tjZkk/lOfHw8VlZWxMbG8ujRI5RKpdbx2NhYnj9/bqTevTvEfcw+mUxGZGQkderUEXVehTwjSRLPIp9p7RtQcwAKeeqHGg1LNYRcrYBsXOHh4ezdu5ft27dz4cIFTExMaNu2LdOnT6ddu3bI5XIOHz7MlClTGD58uKZGsyAIgiDkNwYNJqdPn87r1685evQoLi4uVK5cmXXr1tGoUSOWL1/O+vXr2bdvnyGbzJfi4+P5999/kcvlqUYnZTKZ3hFLIWvEfcwZlUrF4cOH6dy5s7G7IhQSYfFhxCj/m5pubWrNTy1/SvPn+F2sL3bw4EF27NjBsWPHiIuLo27dusydO5fu3bunKgnSqVMnXr9+zTfffGOk3ua9jNbl6CPWKRuGuI+GI+6l4eTHe1lUz9ru8Gz87spV8fEUee89ZOHhml1BEydiN3lyrjRn0GDy1KlTDBw4kAYNGvDmzRvNfnNzc8aOHcvdu3eZPHky27ZtM2Sz+c6rV69EoCPka3K5nKioKF69emXsrgiFxNPIp1rbpW1KF7rfk1988QWlS5dm6NChfPrpp7z33nvpnl+9enV69OiRR70TBEEQ3gWKq1e1Akm1vT2hnTqRWynODBpMRkdHU6FCBQBNQebIyEjN8caNG/Pdd98Zssl8SalUFroPSULBI5fLiYyMFO9VIU88i9Ke4lraprSRemI8e/bsoXnz5pn+matXrx716tXL5V4JgiAI7xKFv7/WtrJlS9S5uKxJbsiLlSpVihcvXgBgbW2Nvb09169f1xx//PgxpqamhmwyX3oXP5x369aNrVu3Grsbee758+c0bdqU27dvp3nO7du3adq0qVjDKQjpEMEktGjR4p38+yAIgiDkH4qLF7W2VfXr52p7Bh2ZbNKkCSdPnmTChAkAdOnShWXLlmFiYoJareaXX36hXbt2hmxSMIA3b97w66+/cu7cOUJDQ7GxscHFxYXPP/8cd3d3Y3cvyy5fvszXX3/NoUOHUtUt+/zzz/Hw8GDgwIFG6p0gFE5PIp9obZexLWOknhjfhQsX0s14nvw3VBAEQchHoqLS3m9jY9i2VCpkjx8jFSsGRYpk6aUm//uf9qVy+bO8QYPJYcOG4evrS1xcHBYWFnz//fc8fPiQ2bNnA9CsWTPmzp1ryCYFA5g6dSpxcXFMnjyZsmXL8ubNG65cuUJERESutqtWq5EkSZSoEIRCQHdksoxN4Qsmw8LC6NWrF//73/+QJAmZTIYkvU1bm/x/EUwKgiDkT/I0ZqDJnz9HbcBEQfIHD7D29ESeNNtTVaMGsT/9hKphwwxfK3v6FPnT/3IUSObmqGrWhKAgg/VPl0GDyerVq1O9enXNtp2dHXv37iUsLAyFQoGtra0hmxMMIDIykqtXr7J48WLqJw2DOzk5Ua1atVTnJiQkMG/ePI4dO4a1tTU9evTgs88+0xz//fffOXz4ME+fPsXGxoZGjRoxYsQIzff90KFDLFq0iB9++IEVK1bw6NEjNmzYQLly5VizZg1Hjx4lIiKCihUrMnjwYBom/dAolUqWLl2Kr6+vptyMh4cHI0eOzPHXn5iYmG7b+pw/f54lS5bw4sULqlatSteuXXPcD0F416Wa5mpb+Ka5Tp8+nWvXrrF69WoaNGhAnTp12L17N+XLl+fnn3/mypUr7Nq1y9jdFARBEPSQPXuW9n4DBpMW48drAkkAxY0bWI4dS9TZsxm+NtUU1zp1ICmPTW4xaDCZFt2phoVN06ZN8rS9s2f/zvS5lpaWWFpa4ufnR61atTA3N0/z3O3btzNw4EDWr1/PuXPnWLx4MbVr16ZGjRrA2yfro0aNonTp0rx48YJFixaxaNEiraRLCQkJbNiwgQkTJmBnZ0fx4sWZNWsWT58+5fvvv8fBwYFz584xYcIE1q5di6urKzt37uTMmTP88MMPODk5ERISwr1797J/g1LIqG1dL1++ZPLkyXTu3Jlu3bpx7949li5dapC+CMK7TF8218Lmzz//5Msvv6R79+68fv0aeJsIy8XFhcWLF9O7d2+mTJnC6tWrjdxTQRAEQZc8jWBS/vw5KkO1ERCA6fHjqfYrbt6EmBiwskr39SY6yXdUDRoYqGfptJmTF2e3xMenn36ak2YFAzIxMWHq1Kl4e3uzf/9+XF1dqVWrFh4eHlqjzADu7u50794dgB49euDj48PFixc1wWSvXr0055YqVYphw4YxadIkvv32W+Tyt7meVCoVY8eOpWrVqgA8efKE48eP4+Pjg5OTEwDdu3fn4sWL7Nu3j3HjxvHixQvKlStH7dq1kclkODk5Ubly5Qy/tuS+phQfH6/5f2ba1rVnzx5KlizJmDFjkMlklC9fnsePH7NmzZoM+yMIhZUkSWKaK2/Xpyf/Xk1ORhcdHa053qZNG2bNmmWUvgmCIAjpSy+YNBTzBQvSPCYLCUEqXz7d1yvOn9faVub3YHLYsGGp9iVnqkteB6K7H0Qwmd94eHjQpEkTrl69yo0bN7hw4QLbtm1j8ODB9O3bV3NepUqVtF5XokQJrXqily5dYtOmTQQFBREVFYVarSYxMZHQ0FAcHBwAUCgUWiN+AQEBSJLE559/rnXthIQETUr8Dh06MHr0aHr37o27uzuNGzemdu3aGX5dS5cuTTW1OmWAmJm2dQUFBVG9enWt93NyMC0Ign7h8eFEJ/4XNFkoLLC3sDdij4zD0dFRU9vV1tYWW1tbAgMDNcffvHmDSmWo59uCIAiCIaU7zdUA17b85htM//gjzXPkISGo0gkmZa9fo7h8WWufqlGjHPctIzkKJq9evaq1HR4ejpeXF/b29gwaNEgzenTv3j3WrFlDeHg4K1euzEmTQi4xNzfH3d0dd3d3BgwYwJw5c1i3bh19+vTRPEE3MdF+u8hkMk0mwhcvXjBu3Di6dOnCV199RZEiRQgICGD69OkolUrNa8zMzLQS7qjVamQyGWvXrk11/eQpt1WqVMHHxwd/f38uXrzIzJkzcXFx4eeff9aMeOpTqlSpVFOsU7aRmbYFIbeFxoZy9MFRIhMiMz65gAqJDdHaLmNbplCWyGjQoAHnzp3TbLdu3ZqlS5fi5OSEWq1mxYoVBTKDtiAIWRAejvmaNUimpiQMGgTW1sbukZBJuTkyaTVoECZ/p79MTRYSku5xk1OnkKUYzFPVqIFUsmSO+5aRHAWTzs7OWtvDhg3D0dGRXbt2aX1QqF69Ol26dOGTTz5hxYoVrFixIlPXX7hwIQcOHODevXuYmZlRv359pk+fjpubm+YcSZKYO3cuGzduJCwsjHr16rFgwQKtBDJhYWFMmDCBI0eOANC+fXvmzZunFWjcvHmT8ePHc/nyZezt7enXrx8TJkwwyAeelGsYY2NjsbS0zPE1c1vFihVRqVQkJCRkqjbo7du3USqVjBw5UhMs/p3BDwXAe++9hyRJhIaGpluc29raGg8PDzw8POjQoQODBw/myZMnqd6DWZHZtlMqX748p06d0mRdhLfvHUHIjujEaDy2efAo4pGxu5KnCuN6SYCvvvqKvXv3ajKe//jjj3Tt2pWhQ4cCb2d/iIzngvAOi47GxtMTxa1bAJj5+BB14AAU8twiBYJajfzBA72HZDkNJqOjMwwkIRPB5MmTWtvKli1z1K3MSntYJxsOHTpEhw4d9AZgMpmMjh07cvjw4Uxfz8/Pj4EDB/Lnn3+yf/9+TExM+Pjjj7WmVi5ZsoTly5fj7e3NyZMncXBwoGvXrkRG/veUf9CgQVy7dg0fHx98fHy4du0aQ4YM0RyPiIiga9euODo6cvLkSebOncvSpUtZtmxZNu9EwREeHs7XX3/Nn3/+yb1793j27BknT57kt99+o169elhn8olZuXLlUKvV7Nixg2fPnnHs2DF27NiR4eucnZ1p27Yts2bNwtfXl6dPn3L79m22bt3KqVOngLdZYo8dO8bDhw958uQJx44dw8rKCkdHx5x86ZlqW9fHH3/MixcvWLJkCUFBQfj6+rJ3794c9UMovE4/Ol3oAkkovDUmGzdujLe3NxYWFgCUKVOG8+fPc+bMGc6ePcv58+dTLScQBOHdYTlxoiaQBFBcv451794QG2vEXgkZkV+9im2NGiju3NF/PIfBpFxPUsmYJUuIHzFC+zw9waTs+XNMjh/H5OhRTE6c0DqW2KpVjvqVWQbN5ipJEnfv3k3z+J07d1KtpUzP7t27tbZXrVqFs7Mz58+fx9PTE0mSWLlyJaNHj+ajjz4CYOXKlbi6uuLj40P//v25e/cux48f58iRI5rpQ4sWLcLT05PAwEBNttDY2FhWrlyJpaUlbm5uBAQEsGLFCkaMGPFOT8eytLSkevXq7Nixg6dPn5KQkICDgwNt2rShX79+mb5O5cqVGT16NFu2bGH16tXUrFmT4cOHa2VyTcvUqVPZuHEjK1asIDg4mCJFilCtWjXq1q0LgJWVFVu3buXx48fIZDLee+895syZo/lAlhMZta3LycmJ2bNn8/PPP7Nv3z6qVKnC0KFD+eGHH3LcF6HwCYrIvbpP+VmnSp2M3YV8Qy6XU7NmTWN3QxCEXGayfz9mW7ak3n/+POY//UT8t98aoVdCSorz5zFbtw7FjRsglxM/ahSJXbti9dVXaU5xBZC9fAkqFWSzbrpCJ5hMbNuWxL59kelUC5AFB2u/7vRprHv2RJYiuWQyydIyU3UpDcGgwWTHjh1Zv349zs7ODBgwQDOqFR0dzbp169iwYQM9evTI9vWTk7okT08NCgri5cuXtEwxjGtpaUmTJk24cOEC/fv3x9/fHxsbG626gY0aNcLa2poLFy7g6uqKv78/jRs31pp+2qpVK2bNmkVQUBAVKlTIdp/zOzMzM4YOHaqZZpUWfbXPdEdue/Toker72yrFU5GOHTvSsWPHVNcxMTFh4MCBDBw4UG/bXbp0oUuXLlr7YtN5ile3bl3OplGLZ4vOL/KM2i5VqlSqazVp0oQmTbTLvbRr1y7N/ghCWnQznDYs1ZBajrWM1Jvcp5ApaOHcgvYu7Y3dlTyR1u+hjDRt2tTAPREEwagkCYt589I8bL5qFQnDhiEVK5aHnRJSkgcGYt2pE7IUeT6svvqKBF9fFAEB6b5WplIhCw5GKlUq222npE7KOSMlJa/UtJOUwC2Z+bJlegNJAGWzZmCAQZfMMGgwOXfuXIKCgvjuu++YMWMGJZMWfb58+RKVSkWjRo2YM2dOtq8/adIkatasqRlhfPnyJYAmU2gyBwcHnicNOQcHB1O8eHGt0UWZTEaJEiUITorwg4ODKV26dKprJB9LK5gM1Pnmw9vgLCoqKt1gJ71jQuaJ+5h90dHRhIWF8eTJE8qVK6f3vSxkT1bu5d3n2jM52jm0o3PZzobuUv6iyvw9yu77Ul+NWGPo1KmT1t+elGut05Ncg1IQhHeD4ty5t6NdaZBFRmK+aBFxP/6Yh70SUjL18dEKJJOZbd2aqdfLAwJQZTeY1BmZVCf9DdMNJuW6I5M6iVBTStCpVJCbDBpMFi1alMOHD3Po0CGOHz/O48ePAWjbti1t2rTB09Mz21NGp0yZwvnz5zly5IhWNlBj0veBJSYmJt3XFJQEPPmduI85k5iYiJ2dHWXLlgXyz4fvgi556nxmRV7RzuBar3I9XJ3F9wKyfi/zowMHDmhtJyQk8N1335GQkMAXX3yhlfF88+bNmJubiynzgvAOMtOpRZ3YsSPKRo2wnDZNs8986VJQq4mbNu2/ESVJwnz2bEwPHEDp4fE22DQx6Ed3IYni4sVMnads2BCVmxvyZ88w/fPP/15/5Qqq5s2z17bOg1NV0t8GdTojk7KQEK3gUjI1RdmiBZiZkdihA8qk5X95IVfekWlNZ8yuyZMns3v3bg4cOKA1Spg88hkSEkK5cuU0+0NCQjTJWRwdHQkNDdV6IixJEq9evdI6J0RnUWvydk6TvAiCIKTladRTre3CmuX0XdWsWTOt7SlTpmBhYcGJEydSlR8aNGgQnTp14vjx43h4eORlNwVByEWyoCBM9+/X2hc/eDCq+vUxX7IEeYoAwXz5cmQhIcSuXg2A6c6dWMyfD4Dizh1UtWqRKGq1G55ajeLSpQxPixs9mvjvvwfA7NdftYJJkytXSMhO25KUemTyvffeHtINJlPEKvIUiZwA1NWqEbNzZ3Z6kGMGzeaaGyZOnMiuXbvYv38/7yXd3GTly5enZMmS+Pr6avbFxcVx7tw5zRpJd3d3oqKi8Pf315zj7+9PdHS01jnnzp0jLi5Oc46vry+lSpWifDrFQQVBELJLLal5HqWdAa6UTfamyAgFw86dO+nRo4feOraWlpb07NkzU1mwBUEoICQJy7FjkalUml2qKlVQffghWFsTp2cmgum+fcj/+QeTgwexGjxY+5hOUCoYhvzff5GHhWntk4oU0dpO+OQT4idP1myr3n9f67ji8uVstS179gxZdLRWu1LSQJZUooR2P1+9wuTPPyE6GoVOWTpV9erZat8Q8nUwOW7cOLZu3cqaNWuws7Pj5cuXvHz5kqioKODt2kcvLy+WLFnC/v37uXXrFsOGDcPa2pru3bsDbwvet27dmjFjxuDv74+/vz9jxoyhXbt2milU3bt3x9LSkmHDhnHr1i3279/P4sWLGTZs2DudyVUQBON5FfOKRHWiZruoeVFszGyM2CMht8XExGjW+uvz/PlzsRZcEN4hplu3YqpTriF+xAhI+myZ2KcP0Rs3ah2Xxcdj26IF1nrWvCn++Sf3OlsAKfz8sJgwAbN160DPesdksjdvMN29G5MjR5AFJWVRj43F5ORJZM+fo/jf/7TOT/TwIHrfPhI7dCChZ0+ijh4ldt06SPEgUFW9OlKKOuzyx49TJcjJDN1RSVXlypr3B6amqO3ttY5b9+qFTZs2mPj5ab9OBJP6rV27lsjISD766COqVKmi+bc0RarcUaNG4eXlxfjx4/Hw8ODFixfs3r0bW1tbrevUqFGDbt260a1bN2rUqMGqVas0x4sWLcqePXt4/vw5Hh4ejB8/nuHDhzNCp76LIAiCoehOcS1jUzhrLxYmzZs355dffmHfvn2pju3bt49Vq1bRPJtrbjJj4cKFeHh4UK5cOSpVqkSvXr24pTNVSpIk5syZQ9WqVXFycqJjx47cvn1b65ywsDAGDx6Ms7Mzzs7ODB48mDCdp/o3b96kQ4cOODk5Ua1aNby9vbNUGkwQCrzISCxSrImEtxk2Ez/7THvfRx+R2CmT5ZLUakP1rmCLisJy8GBsOnXCfPVqLMeOTbUuNZk8MBCbxo2xGjAA6969KVK7NkWcnSlaqhTWn3yCrbs7ZknTipOp6tdH9f77xGzdSuzq1aiSEn9qMTdH7eamtSs7wb7uesnkTK7JdKe6Aihu3cL08GHt1xkxmMzXq3h1/zjpI5PJmDx5MpNTDD3rsrOzY7XOG0VX9erV+eOPP7LcR0EQhOx4GinWSxY2CxYsoEuXLvTv3x9HR0cqVqwIwIMHDwgODqZixYrMS6d8QE75+fkxcOBA6tatiyRJzJ49m48//pgLFy5gn/T0e8mSJSxfvpzly5fj6urKvHnz6Nq1K//73/80D2kHDRrEkydP8PHxAWDkyJEMGTKE7du3AxAREUHXrl1p0qQJJ0+eJDAwkOHDh2NlZcXXX3+da1+fIOQn5r/8gjxFZmbJ0pLYpUtBnnocR9moEaYHD2Z4TfnLlxAVBTaFexaLlZcXpjoJzsx++40ELy/kd+5gun078tBQAExOnED+4oXWubKIiP/+HxmJiU4QqKpfP1P9UL7/vlZGVcWVKyhbt87S1yLXma6q1kk8Jzk4QAalScC4I5P5OpgUBEF4V+nWmCxtK4LJd13p0qXx8/Nj/fr1WhnPq1evzujRo+nbt2+uZqnevXu31vaqVatwdnbm/PnzeHp6IkkSK1euZPTo0XyUlAlw5cqVuLq64uPjQ//+/bl79y7Hjx/nyJEjmjJdixYtwtPTU5OBd+fOncTGxrJy5UosLS1xc3MjICCAFStWMGLECLF8RHj3hYW9zc6aQvzXX6NOeoCkS+/IVxrkDx6grlkzR90riBSXL2O2di2y6OhUgSS8DcrkDx5g3bGjJpDMrswGk6r334cNG7T6mCUxMZjp/F5W6Yx26mZ01UddooRmnaUxGHSa6/Dhw7mYTmrdS5cuMXz4cEM2KQiCUCClCibFyGShYGFhgZeXF7t27dKs49+1axdDhw7N83JHUVFRqNVq7OzsAAgKCuLly5e0bNlSc46lpSVNmjThwoULwNsEdjY2NpoEdgCNGjXC2tpa65zGjRtrfT2tWrXi+fPnBCWvVxKEd5j56tVao19qOzvi0/n8q6pdG8nMLFPXlt+/n+P+FThhYVh/9BFmW7diqmeZAIBMkrCYMiXngWTt2kjFi2f63JR01z9mxNTHB1l4uGZbXawYylattM7RN81VlzGnuIKBRya3bt1KixYtqJ9GRB8UFMS2bdtYvny5IZsVCqARI0ZQsWJFvvnmm2xf49dff8XX15ctW7YYsGf5W2a+5p9++okHDx6wbNmyPOyZkFW6wWQZW7FmUshbkyZNombNmpoRxuTkQA46H14cHBx4/vxt5uHg4GCKFy+uNZdzJdIAACAASURBVLook8koUaIEwUk1z4KDgyldunSqayQfS1niK6VAnbVDmZXd1wnaxH3MOgcfH0pu20ZsxYo8/O47VEWKgCTB5s1a5z3r04cXwcGgU3Q+pZolSmD+TPvvwnUfH0pu345jipIPb/73P17ojF69ywIDA7Hz9aVoZGSG55qms1ztdatWPJg5E8cdO3Dcvh3TV6+QJ2gX81CZmxP49ddEZfJnwSQ+njoptqUXLzL3c6RWU+zIEVymT9fa/bJTJ54mzVhJViY+nozyvL90c+N5JtrN7s94RjWf83Sa6+vXr/WmRBeMp2nTpuke9/T05Ntvv0339TNnzsyVumj79+9n9+7dPHnyBLlcjpOTE82aNWOwTqrsgiIwMJC1a9dy69YtoqKisLe3p2rVqowcORInJydjd69Qexr5lAvPLqCWcpbc4MWLF1xVXc34ROBGyA2tbZGAR8hLU6ZM4fz58xw5cgSFQmHs7gAZf2DRJ3lqrZAz4j5mkVKJ4soVbLy9AbB49Ajr5cuJXbeOp/v3Y5EiIJDMzLCdMAHbpBkAaZE1awY6pYGcW7fGLDAQUgSTjhER2BaS71Xy+9J8z55svT5u4kTUpUsjlS2LwsODynI5TJ9O3HffEQeYz5+PxezZQNKa1t9/p1RWkqCpVEhyObKkxEgmkZG4OjtrZX1NRanE6osvUgW+kkyG9dixuOo8bDPX8/AtIiAAi+nTMTl2DJW7OzZTp+KawTra3PwZz3EwefbsWfxSpKc9cOAA9/UMwYeFhbF7925q1KiR0yYFA9qfombR2bNn8fb21tpnrOD/4MGDLF68mJEjR1K/fn2USiX379/nxo0bGb84hxITEzFNke7ZEN68ecOoUaNwd3dn/vz5FC1alBcvXvD3338TnaK+kJD3Tjw8Qbe93YzdDTHNVcgzkydPZvfu3Rw4cEBrlLBkyZIAhISEUK5cOc3+kJAQHJPW4zg6OhIaGookSZrRSUmSePXqldY5ISmKaydfI/mYIBRYUVFY9+qFwt8fWWKi1iGz3buJnzyZYkePau1Xtm4NGQSSAAl9+2KWIpiMTQpU1S4uWucp/vkHYmMhj6fFG5NcJ+t0svhRozBfskTvMcnKivhx40Df57mk313x48ejLlcO+b17JH72Wap7nSGFAsnBAVmKkk+ykBCksmX1ny9JWI4erXcEVdm2LZKewDHR0xOLFPVI48aNQ3J0JHblyrej4PlgDXqOg8m//voL76Q3vEwm48CBAxzQszAW0KQHF/KP4inmhSdn6ku5b+/evWzdupWXL19SsmRJPv/8c7p06QJAt25vP4Anj1w6OTmxa9cunjx5wtKlS7l16xYxMTE4OzszaNCgDEdBU/Lz86N58+Z8/PHHmn0VKlTQWsuT7Pjx46xatYo3b95Qv359Jk2apFkDdPv2bVatWkVAQACJiYlUrlyZ4cOHaz3UaNq0KWPHjuXixYv4+/vTtWtXRowYgZ+fH+vWrePBgwcUL16cNm3aMGDAAE2geerUKdatW8fjx48xNzenUqVK/PjjjxQrVixVH69fv05kZCRTp07VvL5UqVK8r1P09t9//+Xnn3/m2rVrmJub06xZM0aPHo1NGk+cVCoVK1eu5GBSFrj27dujFqnDs+THv380dhcAKGWT0UQWQci5iRMnsmfPHg4cOMB7772ndax8+fKULFkSX19f6tatC0BcXBznzp3jh6QPM+7u7kRFReHv769ZN+nv7090dLRm293dne+//564uDgsLCwA8PX1pVSpUpQvXz6vvlRBMDiLOXMwOXs2zeO2DRpgq7MvsVvmHlaqmjYlduZMzHbuRNmsGQmDBgF6gslr17CtVo2YjRtR5WIpobwgDwhA9vr12wREerLcJlPolCcCSOjenbhp0zDdtQv5kyepjivd3fUHkinJZCR++mmW+52S5OAAKYJJeXAwqjSCSVMfH8z0LFNSuboSN3eu3teoq1Ujdu5czNavR1W3LvHjx2v1Pz/IcTA5atQoBg8ejCRJVK5cmUWLFmmCjWQymQxLS0vNH5XCpumxzAdRhnC2Tdq/6LLi9OnTLFy4kJEjR+Lu7s6FCxdYsGABxYoVo1mzZqxdu5ZOnToxceJEmjZtijzpF0FsbCyNGjVi8ODBmJubc+LECaZMmcKmTZsy/UGiWLFiXL58madPn1KmTNrT/168eMGJEyeYM2cOcXFxfPfdd6xevZoJEyYAb4uEt2/fntGjRyOTyfDx8WHcuHFs376dokWLaq6zbt06hgwZosk0eOHCBWbMmMHo0aOpXbs2L1++ZP78+SQmJjJixAhCQ0OZPn06Q4cOpUWLFsTGxqY7alqsWDHUajW+vr60adNGbzbD2NhYxowZg5ubG2vXriUiIgJvb29mz57N7KRpGLp+//139u/fz8SJE6lcuTK7du3i6NGjVKlSJVP3ubB7FfOKq8GZm5aamxqVbkRR86IZnygIOZD8u2/Lli3Y2dlp1khaW1tjY2ODTCbDy8uLhQsX4urqSuXKlVmwYAHW1tZ0794dgCpVqtC6dWvGjBnD4sWLARgzZgzt2rXTTKHq3r073t7eDBs2jHHjxnHv3j0WL17MhAkTRCZXoeBSqTBNMd00MyQrKxLbt8/0+QkjRpCgU+Nc7eyMJJMhS1GnVR4WhuXUqUTpFK4vSEy3bMFy1ChkKhWJHToQs3kzpJxyHx9P2UWLsLl8GcXdu1qvjbh/Hynpwb2yaVPMksoSpaTKwgBGTqgdHUm5UECmuy5WrUb2/DlYWWG2fr32oZIliVm9GtUHH6QbTCcMHUrC0KEG7LVh5TiYtLS01GRsu3r1KiVKlMDKyirHHROMb9u2bbRv317zIcLZ2Zm7d+/y22+/0axZM01dMltbW63RTFdXV6152X379sXPzw9fX1/69euXqbYHDBjAvXv36NmzJ2XLlsXNzQ13d3fatGmDicl/b1uVSsXUqVM1I3cfffQRhw4d0hyvV6+e1nXHjh3L6dOnOX/+PO3atdPsb9WqldZDkJkzZ9KnTx86duwIQNmyZRk2bBg//PADw4cP59WrVyiVSjw8PDTrHV3SmR5Ro0YNvvzyS2bOnMnChQupWrUq77//Pu3atdO8/tixY8TFxTFt2jSsra0BmDBhAl9//TVPnjyhrJ4nXdu3b+ezzz6jVVL2r9GjR+Pv75+JOywAnH58Gon//kCXtCpJs7LNsn29yKhIbG10n0unr6xtWUbUG5HxiUKB5+3tTefOnXFLI3nG7du3NQ+HcsPatWsBNGU/kk2cOFFTq3nUqFHExsYyfvx4wsLCqFevHrt379bMXEm+zoQJEzSzUzw9PbXqYxYtWpQ9e/Ywbtw4PDw8sLOzY/jw4YwYId7nQsGl+Osv5Okk0NEn0dMTkv6eZ5u5OVLZssh0ErMobtxAFhxs1JIQ2SV78gTL8eORqVQAmB4+jNm6dSR89ZXmHIspUyi6dWuq16rLltUEkgDxw4Zhunt3qmnHyjwKJnWzrWqCycRELL/5BtNdu5ClsZwpetcu1O/A8j+DJuC5dOkSXbt2TfO4Uqlk7ty56SZ0EfKPhw8faoKpZLVq1dJaI6tPbGws69at4++//yY0NBSlUklCQgKVK1fOdNslSpRg9erV3L9/nytXrnD9+nXmzZvH9u3b+eWXXzTnlSxZUmsKaIkSJXjz5o1m+82bN6xZs4bLly/z+vVr1Go18fHxvNApYFu1alWt7bt373L79m1+++03zb7k14aGhlK5cmXq16/P559/jru7Ow0aNKBFixaaAFufIUOG0Lt3by5dusTNmzc5ePAgmzZtwtvbm/r16/Pw4UMqVaqkCSQBatasiVwu58GDB6mCyaioKEJDQ7Wm7Mrlctzc3DRZFYX0+T7y1druXa03Mz6Yke3riSQWQnrmzp2Li4tLusGkt7d3rgWTYWFhGZ4jk8mYPHmyJrjUx87OjtWrV6d7nerVq/NHOpkVBaGgMfPxydL5qgoViJ861SBtJ7ZqhXmKeobJTPz8SPzkE4O0kZcspk9HFhurtc9y/Hgke3sSP/4Y0wMHMP/1V72vTVWHsXZtYn/5BctBgzSjt1KRIqh0BhNyi24wL09aH272yy+YbdqU5uuUdeq8E4EkGDiYHDBgAIcOHWLBggWaNWvJbt68ydChQ7l165YIJgu4jKYpLVu2jAsXLjB8+HDKlSuHhYUFP/74I4k6T40yw8XFBRcXF7p168bVq1cZNmwYJ06c0KydTDlKmUxKMRVk5syZvH79WpMx1czMjJEjR6JUKrVeo1vfTa1W079/f71rNO3s7FAoFCxevJibN2/i7+/PgQMH+OWXX1i2bFm6wUTRokVp2bIlLVu2ZOjQofTr148NGzakWU4n2bs2Nezem3s8jXpq7G5wMuik1nbL8qm/34KQV6Kiogye/EsQBAOIjsY0RXJCAGWDBm/XNg4ZgvzuXUwPHUJ+/z6RgMVnn5HYsSNksnZkRuKnTEEWH4/Ztm1a+xV//VXggkmFvz9mu3bpPWY1aBCqWbOQ64zCpqQbTELSutT4eCwnTAC1+m3yojxKIKnWCSaTRybNfv893dcVtO9begwaTHp7ezNjxgzOnj3Lzz//TJs2bZAkiYULFzJv3jxKlizJ3r17DdlkgZByDWNsbGyeF6bOrgoVKnD9+nU6d+6s2Xft2jWt7H8mJiaokqYppDynffv2mnIh8fHxPH36VCs7YHZUrFgReHsPM+vq1auMGTOGJk2aAG/L04RmoqBtlSpVCAoK0ju1NJlMJqNGjRrUqFGD/v378/nnn3PixIlMj0yZmppSpkwZXr16Bby934cOHSI6OlozOnn9+nXUarXeumw2NjYUL16cGzduaKbzSpLErVu3KFGiRKb6YAwjj49k0420n9YZi6WJJQ1LN8z4REHIghs3bnD9+nXN9rlz51I9zIK3o4br1q0TI9uCkN9IEpZjxiCLiNDsUpcoQfQff0DSA22Vk5MmGc79XJihkpy9M7FbN6yTlh4BmJw5Y9B28oKZnhHWlBQPHqR7XF2tmt79iX36kNijB6jVeRZIQuqRSVlwMPLAQBQ3b6b7usQUCSYLOoMGk4MHD6ZVq1Z4eXnRq1cvevfuTUBAAJcuXeKLL75g9uzZaWalFPKfPn368O2331KlShXc3d05f/48R48e1UoGU6pUKS5dusT777+PqakpRYoUoVy5cpw5c4YPPvgAExMT1q1bR4JOYdiMzJ8/nxIlSlCvXj0cHR159eoVGzduxMLCQlNgOzOcnZ35888/cXNzIy4ujuXLl2fqyX///v0ZP348Tk5OtGrVCoVCwf3797l16xbDhw/nxo0bXLx4kYYNG2Jvb09gYCAvX77UBLy6zp49y/Hjx2ndujXlypVDkiTOnj3L+fPnGThwIABt27Zl7dq1zJw5k0GDBhEZGcm8efNo3rx5mkFtz5492bx5M87Ozri4uLBnzx5CQ0PzbTD5T/A/+TKQBGhapikWJoUzSZiQew4ePKiV8Xz9+vWs10nCkCwz00cFQchbpps3a5XsgLeBC3pmRuU2ZaNGSCYmyJIeSCn+/RfZs2dIpQtIaanYWEzTqPiQWao0gkkg4+ytuSDVNNfgYEwzGDhT1q+P5Oycm93KUwb/SahUqRKHDx/G09OTbdu2IZPJ+OGHH/j6668N3ZSQyz788EPGjBnDtm3bWLJkCU5OTowbN45mzf5LUDJixAiWLl1K165dcXBwYNeuXYwcOZI5c+YwbNgwbG1t6dmzZ5aDyQYNGnDo0CH27t1LeHg4RYoUoUqVKixevBhnZ+dMj05OnjyZefPmMWDAAEqUKMHAgQMztW6oYcOGzJ8/nw0bNrBt2zYUCgXlypWjQ4cOwNtRwevXr+Pj40NUVBSOjo70799fK6lPShUqVMDS0pJly5YRHByMQqGgVKlSDB8+nJ49ewJgYWHBokWLWLJkCYMGDdIqDZKW3r17ExoaytyklNLt2rWjbdu2BAUFZer+5LVD/x7K+CQj6ePWx9hdEN5B/fr1o3379kiSRMuWLZkyZQpt2rRJdZ61tTUVK1bUO3VfEAQjiYrC4vvvtXapqlUjLpfWNWfIxgZV3bqYpEi0Z+LrS+JnnxmnP1lkcvQosshIzbba0ZHIa9cwX7YMi5kzM3y9umxZ1Do5LoxNrZuAJyQE0z17tPbFzpuHybFjmB47hmRpSdyM7OdmyI9kYWFhUsanZd6jR48YPnw4fn5+dOnShUuXLhEaGsrUqVMLRSa3mJgYQkJCiIqK0nu8IE1zzc/EfcyZiIgI/v33X9zd3ZHJZHk2ta7ZlmbcePVfCZUaJWpgb5F20qK8YK4wp0OlDvSv2T/Ha1NFAh7DeRfvpZ+fH1WqVMFB58OHkH3v4vvEGMR91M9s6VIsp03TbEvW1kT5+qLWqdGaUm7fS/OZM7FYsECzrapW7W2JkJRlNfIpq88+wzRFxv34oUPf1leUJGxr1kxVL1KSyYj66y8s5s5FFhFB3JQpqBo1yutup0sWHEyRdN4PklxOZEAAkr098oAApJIltbLR5pXcfF8a9BHoxo0bmTZtGmZmZmzatInOnTsTHh7O+PHjmTZtGocOHWLlypV6138JgvBuCwoP0gok5TI5+7rto7hl8XReJQjvDjMzswwDyU2bNvHll1/mUY8EQUhTbCzmy5Zp7Yr38ko3kMwLiT16YP7TT5rMpYrbtzHds4fEFGsp86WICEyOHdPalZg0MwuZDGWLFpht2aJ1PLZyZdQ1ahCjsz8/kYoXR5LLkanVeo8rP/wQKWnpUVrrPQu6tCtkZsPo0aNp1qwZ586d0yRtKVq0KKtXr2bTpk3cu3ePDz74wJBNCoKQzynVSmb4zaD2+tpa+xuWbigCSaFQ8fT05IcfftCb2frly5f07Nkz3WntgiBkjez1ayxHjsS2WjUshw2DLGSVN1+5EvnLl5ptydqaBC+v3OhmlqirVHmbaCYF87lz3yaeycdM/P2RpVjypKpYEdX772u2lS1apHpNZN26edG1nFEoNMGiPol93v0lNAYNJpcvX87WrVv1Pnnt3Lkz586d01tqQRCEd9eUM1NYdHFRqv0dXDoYoTeCYDxeXl4sWbIEDw8PrQyvO3fupHHjxly4cIHly5cbsYeC8O5QXLmCzQcfYLZpE/LnzzHbuhXTFLWj0yO/efNtgJZCwoABSMXzxwPQ+IkTkVJMa1Xcu4fiyhUj9ihjivPntbaVzZtDiqUlyqRsuCkViGASkNKYcSIVLUpiiooI7yqDTnPtk0H0XaJECTZu3GjIJgVBMBJJkljgv4Cdd3YSo4xJ8xx99SRlyOhYqWNud1EQ8pWZM2fSoUMHhg0bRuvWrRkzZgy3bt3iwIEDtGjRgmXLllGmTBljd1MQCj6lEqtPP0X+4oXWbpMzZ0js1y/j13p5aY2iqYsVI37kyFzoaPaoK1VC6emJ6cGDmn0KPz9USWXCjCYmBsWVK6hq1oQiRbQOmegEk7prHyUHB5QNG2Jy4cLbbRsbIuvVoyCsMFc7OuotBZLQsycUgvweBh2ZhLd1/GbOnEm7du2oW7cu/kkZp16/fo23tzd37941dJOCIBjB6cenmXVuFgFvAngS+UTvP32BpJnCjG+bfIuLnYsRei0IxtWkSRPOnj1LrVq1mDdvHgcPHmTSpEns2bNHBJKCYCDywMBUgSSA4s6dDF9r+vvvKK5d09oXu3BhmqNPxqI7LdTkr7+M05Fk4eHYNGuGTceOFKlRA5Pjx/87lpiI4tIlrdOVehLpxC5ciLJOHdRlyxK7aBGqokVzu9cGkdZ7I+Hzz/O4J8Zh0JHJoKAgPD09ef36NW5ubjx8+FBTwqFYsWLs3r2bkJAQFqTIQiUIQt6TpJwncf776d9Zfs2mTpvwcPbA1sw2x+0LQkEUERHBhAkTuHjxInXr1iUgIIC1a9dSrVo1unTpYuzuCcI7QX7vnv79gYFv102mVY8wLg4L3emtXbuizIcF5pUpyrQBmJw7l/7XlsvMf/0Vxf37AMgiIrDu3h21oyPKFi1I/OQTZClKuqmdnJDKl091DXX16kSfOgWS9HYKbGBgXnU/RyQnp1T7Evr0QV27tp6z3z0GHZmcPn06kiRx/vx5du7cmeoDa4cOHThz5owhm8yX1Pl8EbQgKJMKHuekFMa/b/7N9LlFzIqw86OddKncRQSSQqHl6+tLkyZN2LdvHzNnzuT48eP89ddfvPfee/Tr14/Bgwdnqg6uIAjpk/+r/++TLDEReVLAo4/ZmjVa5SkkM7N8WxNQXaUKakdHzbYsOtqo6yZN9u9PtU8eHIzZjh1Y9+6ttV/VsKHWeslUclimK68lenpqbcePHEnszz8bqTd5z6DB5KlTp/jqq6+oUKGC3g+p5cuX59mzZ1m65tmzZ+nduzfVqlXDzs6O33QWT3t5eWFnZ6f1r3Xr1lrnxMfHM378eFxcXChdujS9e/fm6VPt6XePHz+mV69elC5dGhcXFyZMmEBCivnymWVubo5KpRIBpZBvJddCValUFM3BFJJ7YdpPfjd23Mi1Adf0/vt3yL+0qZi6ULsgFCaffPIJDg4OnDp1iuHDhyOTyahQoQKHDh3ixx9/5ODBgzRp0sTY3RSEAk+RRjAJIE9jqqvi0iUsfvxRa1/CwIFIzs4G7ZvByGSpRyeNNNVVFhSEyT//ZPp8ZcOGudibvKdq3JjoPXuIHzWKqAMHiPvhBzAx6OTPfM2gX2l8fDx2dnZpHg8PD0cuz1r8Gh0djZubG59++ilDhw7Ve06LFi1YtWqVZtvMzEzr+OTJkzl8+DC//vor9vb2TJ06lV69enH69GkUCgUqlYpevXphb2/P4cOHefPmDV5eXkiSxPz587PUX4VCQenSpbl06RJyuTzV1xsdHa03LbyQNeI+Zp0kSSiVSkJCQoiPj6dUqVKUKFGC8PDwbF1Ld2SyUelGlLQuaajuCsI7Z+LEiYwfPx6FnuLiw4cPp23btnjlg9IDglDQpTXNFd7WZVR+9JHWPtmrV1h98YVW0h3J1pb4b77JtT4aguqDD2D3bs22ws8PjNBn0wMHsnS+8h18aKb08EDp4WHsbhiFQYPJatWqcfbsWQYMGKD3+KFDh6hVq1aWrtm2bVvatm0LwLBhw/SeY25uTsmS+j/EhoeHs3nzZpYvX45H0jd51apV1KxZk1OnTtGqVStOnjzJ7du3uX79OmXLlgVgxowZjBw5kmnTplFEJyNVRqysrGjQoAHnz58nJiZGa7rvq1evKJFOPRohc8R9zD5ra2tKly5Nw4YNs/xwJ9mL6BdEJUZptouYFcHRyjGdVwiCMGnSpHSPu7q6cvTo0TzqjSC8u9Ka5gr6RybNvb2R68yci120KN36gfmBsnFjrW2FkZJc6gaT8V5eqKpUwUpP3dzEVq0KzVrCwsKgwaSXlxdDhgyhWrVqdO3aFXi7fjAgIIB58+Zx8eLFVNNUDeHcuXNUrlyZokWL0rRpU6ZNm6apdfnPP/+QmJioVd+ybNmyVKlShQsXLtCqVSv8/f2pUqWKJpAEaNWqFfHx8fzzzz98+OGHWe6ThYUFLfQUYA0MDMTV1TXrX6SgRdxH4wp8o70ovpJ9pRytvxSEwiIhIYHff/+dv/76i5CQEGbMmEHt2rUJCwvjjz/+4MMPPxRZXQUhJyIikAcHp3lYN6Or7OFDzDZs0NoXP2IEid2750bvDErt4oIklyNLWlolf/YMYmLAyirP+iB79UpTziNZ/LBhSOXKEffkCRYpkm6qatYk5tdfC9yaSCF9Bg0me/TowZMnT5g9ezazZ88GoFu3bgDI5XJmzJiBp84i1Zxq3bo1nTt3pnz58jx69IiZM2fSpUsXTp06hbm5OcHBwSgUCorrFJp1cHAgOOmXTXBwsCb4TFa8eHEUCoXmHH0Cs5llKruvE7SJ+2g4Wb2X54LOaW07KhzF9yOJuA+Gk917mV8fNL1+/ZrOnTtz69YtHB0dCQkJ0STcKVKkCLNmzeLOnTvMyKcJPwShINBNsKN2dNQKLuX37iEPDMR0+3ZMDx1Ccfu29vnlyxP33Xd50tccMzNDKlcOWVCQZpf84UPUbm551gWT06e1tlW1ayOVKwdA/PjxyB8+xPTgQZTNmxO7bBmksxxOKJgMvjp0zJgx9OjRg/3793P//n3UajUVK1akc+fOVKhQwdDNaYJVgOrVq1OnTh1q1qzJn3/+metp1rPzgUWMqBmGuI+Gk517GfE8Qmu7rnNd8f1AvC8N6V28l9OnT+fx48ccOXKEypUrU7lyZc0xuVxOly5dOHbsmAgmBSE7YmLeJtFJGsxIpqpbF65d00xjlSmV2DZokOZl4iZPBp3cG/mZqmJF5CmDyQcP8jaY9PXV2k5MMRMQc3Ni164lNrnUh/BOypVUQ2XLlk1zfWNuK1WqFKVLl+Z+0pMpR0dHVCoVoaGhWmvsQkJCaJw019zR0ZELOkP0oaGhqFQqHB3FOjBB0HXvjXZyg8r2ldM4UxCEZEeOHGHIkCE0bNiQ169fpzpeqVIltmzZYoSeCULBZrptG5YTJiCLjEx1TF25MpK9PWbbtmV4HVXVqiT26JEbXcw1ahcXOHVKs51e6RODkyRMUrQNoNSzxEsEku+2XMtbGxUVRVhYmN7i6OWShr9zQ2hoKM+fP9ck5KlTpw6mpqb4+vrSI+kXxNOnT7l79y4Nk1ITu7u7s2DBAp4+fapZq+Lr64u5uTl16tTJtb4KQla8iH7BBN8J3Hx106DXTUxMxPRs1oocP4l8orUtgklByFhkZKTW2nxd8fHxqFSqPOyRILwDwsOx/OYbZDExeg+rK1UicfBgTI4fRx4SkuZlVJUrE7NuHejJtpyfqStW1NqWP3ig+b/i8mUU586hbNUKddWqBm9b/u+/2nU5LSze1pAUChWDBpNxcXF4e3uzefNmvU9dk6V3TFdUVJRm7ECs5AAAIABJREFUlFGtVvPkyROuXbuGvb099vb2zJ07ly5dulCyZEkePXrEDz/8gIODA506dQKgaNGifPHFF0yfPh0HBwdNaZDq1atrEuS0bNmSatWqMXToUGbOnMmbN2/47rvv+PLLL7OcyVUQcsukU5PYfy91UeD8oJJdJWN3QRDyPRcXF65cuULfvn31Hj958iTVqlXL414JQsFmevx4moEkgMrVFcnZmZjt27Hu1ElzrlSkCHHTppHYpQuSrW2eJq0xJLWLi9Z28sik4swZrD/5BJlSiTRnDlEnT6J+7z2Dtq07xVXZuDFYWBi0DSH/M2gw+c0337Bt2zY6duxI48aN0605mVlXrlyhc+fOmu05c+YwZ84cPv30UxYuXMitW7f4/fffCQ8Pp2TJknzwwQesX78eW1tbrdcoFAr69+9PXFwcH374Ib/88oum1pdCoWD79u2MGzeO9u3bY2FhQY8ePfhRp3itIBjT/57/z9hd0MvFzgUbMxtjd0MQ8r2+ffsybdo0mjRposkwLpPJiImJYd68eZw8eZKlS5cauZeCULCYHDqU5jFVtWqokpY0qerWJfrgQcwXLUJdvDjxY8ciOTvnVTdzjW4wqbh/HyQJy6lTkSmVAMiiojD/6SdiU9RkNwSTv/7S2i6sdRYLO4MGkwcOHODLL79k8eLFBrvmBx98oMl2p8/uFMVa02Jubs78+fOZP39+mueUK1eO7du3Z6uPgpDblGolz6OfG7sbqZgrzJnedLqxuyEIBcKQIUO4c+cOQ4YM0TzwHDBgAGFhYahUKgYNGsRnn31m5F4KQgESH4/psWNau6KOHIH4eOSPH5P48cda01ZVdesSs3lzXvcyV6l1klvKnjzBdM8eFNeva+03276duBkzkJycDNa2/Kb2shtV06YGu7ZQcBg0mJTJZNQWhUgFweBeRr9ELak128UsinG0l2GKmz98+DDbmZbL2JbB0sTSIP0QhMJg0aJF9O7dmz179mhlPO/atStNmjQxdvcEoUAx8fPTSrqjdnRE5e4OcjmFZvWxpSXq0qX/y1arVmPp5aX3VLO1a4n/9lvDtBsfr7U+E0BVpYphri0UKAYNJjt06MCpU6fo37+/IS8rCIXe08inWttlbcsaLOmN9EoSCXQEIQ81bNhQkwAur509e5alS5dy9epVnj9/zvLly7VGQyVJYu7cuWzcuJGwsDDq1avHggULtNZyhoWFMWHCBI4cOQJA+/btmTdvntbSlps3bzJ+/HguX76Mvb09/fr1Y8KECchEVkfBgEz37dPaVnp6glxupN4Yj7piRU0wCSCLj9d7ntn69cRPmgQmOf/4L793D5n6v4fc6rJlwUYseSmMDPoT98033/DgwQNGjhzJxYsXefHiBSEhIan+CYKQNU+jtIPJ0raljdQTQRAKsujoaNzc3Jg7dy6WlqlnFSxZsoTly5fj7e3NyZMncXBwoGvXrkSmGP0ZNGgQ165dw8fHBx8fH65du8aQIUM0xyMiIujatSuOjo6cPHmSuXPnsnTpUpYtW5YnX6NQOMiePcP099+19iV26GCk3hiX7rrJtMhDQ5EHBBikTYXOdcSoZOFl0JHJBklFYK9fv55uraysZHMVBCF1MFnWJu3yAoIg5E+SJLFhwwY2b97Mw4cP9eYDkMlkhIaG5lof2rZtS9u2bQFS1YOWJImVK1cyevRoPvroIwBWrlyJq6srPj4+9O/fn7t373L8+HGOHDmCu7s78HbqrqenJ4GBgbi6urJz505iY2NZuXIllpaWuLm5ERAQwIoVKxgxYoQYnRQMwvznn5ElJGi21eXKoUxKbFXYKOvXx0zPWlBVhQpIxYphcvmyZp/i5k3Ubm45blN+547WtqEzxQoFh0GDSTGFRRByh+401zK2ZYzUE0EQsuu7775j+fLl1KxZk549exok47khBQUF8fLlS02mWQBLS0uaNGnChQsX6N+/P/7+/tjY2GhN023UqBHW1tZcuHABV1dX/P39ady4sdbIZ6tWrZg1axZBQUFprtHO6v2oXbs2a9eu1XusefPmXL16NUvXS5ZW0r9Ro0axcePGbF3z1KlTeutWb9iwgdGjR2frmosXL6Zfv36p9v/zzz+a0mdZ1bdvX5YsWaL3WHbfr7Vr1+b06dN6j+Xk+6TWKUERP2YMmJoWyu9TYs+eJJw5g9muXWh9Cn/48P/s3XlcVFX/wPHPnRl2FFQEEcUVNbXCPfXnliTuS2o9lmUuua+5JKVpuaVZZrmHpj71mCvmkltJZaViaS5pSrniwqIisg4zc39/ACMXhv3CsJz36+VL7nrO3JmB+73nnO9J+Zfem2+m/MugwO/T6tUp/zIQ36eS8X3K7n3K6jWkUTWYDAgIUPN0giCkuhN7R7Fc1Vl0cxWEkmbLli307t2bjRs3WrsqFoWHhwNQuXJlxfrKlStz925KNumIiAgqVaqkeHAsSRJubm5ERESY96latWqmc6Rty2/CL0FIT0pMNP9sqloVfVnOhOzgQML69SRNnAgdOli7NkIZo2owKQhC4RAtk4JQ8iUmJub7CbeQWVJqkpHQ0NAst+WHpfMBPHr0KN/nvHnzJk5OTpnWpwXw+REeHm6xrjdv3sz3OR89epTl68+vpKSkLM9ZkPcpvdsvvUR46usu0++To2O+z1lY75P4PpWM71NB3icRTApCCZApmHQWwaQglDTt27fn9OnTFrtSFQceHh4AREZGUr16dfP6yMhI3N3dAXB3d+f+/fvIsmxunZRlmaioKMU+GZPtpS2n7aMGOzs7AHx8fLLclh+Wzgfg4uKS73N6e3tbPG/aNc8PDw8Pi+eMi4vL9zldXFyyfP35ZWdnl+U5C/I+pZFtbHCZMIHylSoB4n3Kr8J6n8T3qfS/T1J0dLSc75KFPEtLUCAUTFm6jsnGZNw/d0fmyVc1fHw4drqC/xGGsnUtC5u4luopjdfy7t279O/fnxdffJHXX39d1cAqP7y8vFiyZIl5ahBZlmnQoAEjR45k6tSpQEprqo+PDx988IE5AU+rVq04dOiQedzkyZMn8ff359SpU/j4+LB+/Xrmzp1LaGgo9qnj2j7++GMCAwO5ePGiqrkVSuPnxBpK0nV0mDgR282bzcvJvXoRbyH5jLUUx2vp3Lo12kuXLG6L27oVg79/7k8WH4/jqFHY7N1rXmVo25a4/fsLWs1MiuO1LKkK81qWvcl4BKGEuRd3TxFIVnasrFogKQhC0WnSpAn//vsvCxcupEGDBnh4eODp6an4l3GsodpiY2M5d+4c586dw2QyERYWxrlz57h16xaSJDFmzBiWL1/Onj17uHjxImPHjsXJyYkBAwYAUL9+ffz8/JgyZQohISGEhIQwZcoU/P39zTcqAwYMwMHBgbFjx3Lx4kX27NnDp59+ytixY0WSPqFApIcPsdm1S7GuTI+VzCVjo0ZZbrP7/PM8nctx5EhFIAlgbNw4X/USSgfRzVUQiolEQ6IiaExz7dE1xbJIviMIJVO/fv2sHkydOXOGXr16mZcXLVrEokWLGDRoEKtXr2bSpEkkJCQwffp0oqOjadasGbt27aJcuXLmYwIDA5kxYwb9+/cHoFu3bixZssS83cXFhaCgIKZNm0anTp1wdXVl3LhxjB8/vuheqFAq2a5dixQba142eXhg8POzYo1KBlPdullu0/3yC9L9+8ip3YSz9fgxugwtkCY3N/Tp5pkVyh7VgsmEhAQ+++wzWrRooUgrLgilQbIxma1/b+Vq9FXVz30v7h7BN4K5G3c3V/uL8ZKCUDKttpA2v6i1a9cuyxTwkJKZNSAgINvs7K6urqxbty7bcho1asSBAwfyXU9ByCQmBts1axSrksaMAZ1oF8mJsUmTbLdrLlzAmIsssJobN5DkJw+9TW5uxP7wA3KNGgWuo1ByqfYNdHBwYNmyZYqnk4JQWkwLnsamC/mbD0ltIpOrIAiCUNbYbtqEJt2DEJOrK/phw6xYo5LD0LEjJm9vNFlkJtWeO5frYDI949NPi0BSULeba+PGjbl6Vf2WG0GwJlmW2XVlV847FpFn3Z+1dhUEQRAEoUjZfPedYlk/ejSUL2+l2pQwdnbEHjqEzbZtmGrXRhMWhkO63gfa8+dzdZqMwaRJBJICKgeTs2fP5o033qB169b45yUzlCAUY4+SHvFY/7hIyrLX2me5zVZrS8+6PRlQf0CR1EUQBEEQigW9Hu3p08pVIvFOnsienugnTQJA++uvim3ac+dydY6MwaRolRRA5WByxYoVVKhQgUGDBlG1alVq1qyJg4ODYh9Jkti2bZuaxQpCoco4ltHNwY0xTcaodn6tpKWRWyPaV28vsrQKgiAIQgbac+eQ0k3IbvLyQk43F6qQNxmzr2quXIGEBMhwz55Rxm6yomVSAJWDyb///htJkqhWrRoANy30zbZ2JjtByKu7scpgsl7FekxtOdVKtREEQRCEskV74oRi2ZA6x6mQTy4uGGvWRHv9OgCSyYT24kWMzZple1imbq7e3oVVQ6EEUTWYPJ/LPteCUJLcib2jWBZTcwiCkFeJiYkEBQVRr149muVwwyYIgpIuJESxbGzZ0ko1KT1MzzxjDiYBnDt3JnHGDJJmzgSNhWnoZVm0TAoWWfi0CIKQnggmBUEoKHt7eyZNmiQeugpCXsky2pMnFauMomWywIxPP51pnf2SJdh98IHF/aUHDxRzfMqOjshuboVWP6HkUD2Y1Ov1bN68mTfffJO+ffty9uxZAKKjo9myZQu3b99Wu0hBKFQZu7l6OntaqSaCIJRkdevWJTw83NrVEFJJYWFI6VpmhEIiy+h++AGboCCkGzew++QTHF9/HZudOyHdnIVZkW7eRJPueyM7OGQa8yfkndHX1+J6+08/xWbz5kzrLWZyFUPXBFTu5vrgwQN69erFxYsXcXd3JzIy0jw5cvny5VmwYAF///0377//vprFCkKhEi2TgiCoYfr06UyfPp2ePXvSqFEja1enTLPZtAmHt95CMhpJnD6dpHfftXaVSi27BQuwX7o003qbPXvQHzmCoWNH0GpBo0HWapGrVUsZu5caqOj++ENxnLFpU7CxKYqql2qGTp0wNm6M9sKFTNscZs0ieeBApMRE5AoVgJSgPj0xXlJIo2rL5Jw5c7h16xYHDx7kt99+Q073xEmj0dC7d2+OHDmSp3P++uuv/Oc//+Gpp57C1dWVr7/+WrFdlmUWLVpEgwYNqFKlCj169ODSpUuKfaKjoxk5ciTe3t54e3szcuRIc5Cb5q+//qJ79+5UqVKFp556isWLFyvqL5RdIpgUBEENv/zyC25ubrRv355u3boxduxYpk6dqvg3bdo0a1ezTLCfNw/JaATA7rPPIC7OyjUqpRITsVu9OsvNtt98g+Po0Ti++SaOw4fj9MYbOPv54TB5srnVUpPhns7YpEmhVrnM0OmIPXKEuO3bScjQyCPFxODi6Un5WrVwGDYMTCaRfEfIkqotkwcPHmTUqFG0atWKBw8eZNpep04dvvrqqzydMy4ujoYNGzJo0CBGjx6dafvy5ctZuXIlK1euxMfHhyVLltCvXz9OnTpFuXLlABgxYgRhYWHs2LEDgIkTJzJq1Ci2bt0KQExMDP369aNNmzYcPXqU0NBQxo0bh6OjIxMmTMjrZRBKGdHNVRAENWzYsMH884kTJziRIUMlpGQ8X2qhFUdQUXQ0mqgo86KUlITmxg1MDRtasVKlk+6XX5DyEajbbtqE0dcX/dChaP/+W7HN2KCBWtUTHBwwvPACvPACmrAw7L74ItMutrt2YejRw3I311wwGo0kpZvWJS9sbW2Jj4/P17GCUk7X0s7ODq1Wm69zqxpMPn782DwtiCVJSUkYU58E5laXLl3o0qULAGPHjlVsk2WZ1atXM3nyZPr06QPA6tWr8fHxYceOHQwdOpTLly/z/fffc/DgQVqmZv9atmwZ3bp1IzQ0FB8fH7Zv305CQgKrV6/GwcGBhg0bcuXKFVatWsX48ePFdCZlWJIhiaiEdDcdSHg4elixRoIglFQPHz60dhUEQHvtWqZ1mnv3RDBZCHQHD+b7WPu338bQvDmay5cV600imCwUyX36WAwmAWy//DLT+MjcBJNGo5HExEQcHR3Ny3mJAwoS4AhK2V1LWZZ5/Pgxzs7O6HR5Dw1VDSZr167NmTNnGDJkiMXtR48e5amnnlKtvBs3bhAeHs7zzz9vXufg4ECbNm04efIkQ4cOJSQkBGdnZ1qly/z13HPP4eTkxMmTJ/Hx8SEkJITWrVvjkG6y1s6dO7NgwQJu3LhBzZo1VauzULLcjVO2Sno4eWCjFWM1BEEQSirN1auZ1klhYVaoSSkny9hkCCYTFixA//rrYDRiu3Ur2jNnwGQy/7M5fNicMVTS67GfOxfNv/8qzmGsX7/IXkJZYmzdGpObm6LVPo3ul18yrTPlYtx3UlISjo6OGI1GQkNDSU5OztMQssTERJG0TCU5XUtJknj48CFNmjShfPnyeTq3qsHkkCFDmD17Nm3atDEHeJIkER8fz5IlSzh69Ciff/65auWlXZTKlSsr1leuXJm7d1OCgIiICCpVqqRoXZQkCTc3NyIiIsz7VK1aNdM50rZlFUyGhobmq975PU5QKorr+OeDPxXLFXQVSuX7Vxpfk7WIa6me/F5LHx8flWuirh9//JFjx44RGRnJ+PHjqVevHrGxsZw9e5ZGjRrh6upq7SqWapaCSY3INK8OgwGb7duRHjwAW1s06YJ02cEB/bBhkPrgXj9qVObDd+zAccQI87LNDz8otpuqVYPUIUyCyrRaknv1wu7LL3PcVS5fHlOtWrk+9T///IPRaMxzK6NWq81XS5mQWW6upU6n4+DBg/Tr1w+bPCS5UvUdGjVqFH///TejRo0yj1ccNmwY0dHRGI1GRowYwauvvqpmkVaVnxuWtK61QsEU1XU8f1k5J1ytSrVK3fsnPpPqEddSPaXxWiYkJDB48GCCg4PN6/r370+9evWwtbVlyJAhvPnmm7z99ttWrGXpZzGYvHPHwp5CXtm/+y52a9da3Gbo0MEcSGYluX9/jMuXo81iPlbRKlm4kl96KVfBpPGZZ3I9LYjRaESv14vuqiWARqMhISGBx48fU7FixVwfp3q4v2zZMv7zn/8QFBTE1atXMZlM1KpVy5zgRk0eHilj1yIjI6levbp5fWRkJO7u7gC4u7tz//59ZFk2t07KskxUVJRin8jISMW505bT9hFKvotRFxlxcAQXoy7m+xwik6sgCPk1b948fvnlF9atW0fr1q1pnG6uPFtbW/r27cvBgwdFMFnINBbGTEqiZTJXdEePYrd4MbKrK0ljx2Ls0MG8TRMaim0WY+4Akvv2zbkASSJp+HAcJ0+2uFmMlyxcxtatSQwIwHbdOky1aqH7/XfL+z37bK7PaTKZxOwIJYgsy3lOmFQobcetWrVSjFEsLDVq1MDDw4Pg4GCaNm0KpPQJPn78OB988AEALVu2JDY2lpCQEHOdQkJCiIuLMy+3bNmSuXPnkpiYiL29PQDBwcF4enpSI5fZqoTib84vcwoUSIIIJgVByL/du3czYsQIBgwYYDHjuY+PDzt37rRCzcoW0TKZNV1wMDbbtpnHLaaRbWww1a+P3bJlSImJANgcOoShbVv0AwciV6mC7ZdfIplMFs+b3Ls3yS+/nKs6JA8YgDx7NtLjx5m2iZbJwpf09tskpT7Qchg5Ettt2zLtY/T1LepqlQrffvstmzZtIiIigqFDh1KlShWWLVvG999/b+2qFYiqwWSvXr0YMGAAvXv3pkLqJKcFFRsby9XUX/wmk4mwsDDOnTtHhQoVqF69OmPGjOGTTz7Bx8eHunXrsnTpUpycnBgwYAAA9evXx8/PjylTpvDpp58CMGXKFPz9/c1dqAYMGMDixYsZO3Ys06ZN459//uHTTz9lxowZIpNrKXI+0nK3mbx4uvLTKtREEISy6P79+9TP5mZYkiQSU2/UhULy+DGa1HwJ6Ykxk6A9cwbHgQORDIZcH6P79Vd0v/6a5XbZ2ZnEWbPQjxyZ626RODujf+UVi91lTSomcRRylty/v+VgMg8tkyXR/PnzOXDgAJAy1tDDw4MOHTowfPhwRbLOvIiJieHjjz9mwoQJdOrUCUdHR7RaraLX5vr16wkODs7zNIrWplHzZLdv32by5MnUr1+fgQMH8s033/DYwpOlvDhz5gzt27enffv2JCQksGjRItq3b8/ChQsBmDRpEmPGjGH69Ol06tSJe/fusWvXLvOYTYDAwEAaN25M//796d+/P40bN2Ztul9SLi4uBAUFcffuXTp16sT06dMZN24c48ePL1DdheIj2ZhMeFz+M4I56hwZ9vQwOtXopGKtBEEoS6pVq8blDNMcpHfixAlq165dhDUqeyx1cQVSWsFiYoq4NsWL7fLleQoks2Ns2JCYv/4i5vJl9KNHgyZvt5uJAQEWWyGN9eqpUj8hdwydLN/zmOrUKeKaFL3mzZuzZ88etm/fzptvvsmuXbtYsWJFpv0MBkOuuvHeu3cPo9FI27ZtcXNzw9HRETs7O9Ua36xJ1ZbJ06dP8+eff7Jr1y52797NmDFjsLe3x8/Pj/79++Pv75/niL5du3ZER0dnuV2SJAICAggICMhyH1dXV9atW5dtOY0aNTI/hRBKn/D4cGSefNkrO1YmdKTIuikIQtEZOHAgK1asoGfPnuYWyrTeL+vXr2f37t3mIRqC+qSoKBxmz85yu+b2bUx5TIlfWthERmKzb59q50ucMwfZyyv/J3B1JW7XLpz9/c0ZYQ3t2oGLi0o1FHLF1hZD+/bofv7ZvMrk6QllIJmOra0tlSpVAlLmvD99+jTHjh2jYsWKBAcHM2jQIDZu3Mi9e/c4dOgQMTExLF++nFOnTgHQokULpkyZgru7O/v37zc3gg0cOBCAHTt2cPr0aXM31/3797NhwwYA2rZtC8A777xDjx49ivql55nqYyZ9fX3x9fXlgw8+ICQkhF27drFnzx727duHk5MT3bp144tsBmgLQmG481g5HsbTydNKNREEoax66623+OOPP+jZsyd169ZFkiRmzpzJgwcPCA8Pp2vXrowdO9ba1SydEhJw6tkT7d9/Z7mL5vbtMtuN0m33bkWrpLFuXRJTA28pNhaHmTMVYxhj9+xBrlgR2//9TzkPpL09+n79MPj7F7hOspcXsT/8gN3y5SmJebJIyiMUrsRZs3Du0sW8rH/zzQKfs01qsFRUfsumK3Zu2dnZYUj9jty9e5cjR44wf/58bGxssLGxYebMmdjZ2ZmnQPzkk0+YOXMm69evx8/PDzc3N9566y0CAwNxd3fPNAWUn58f165d49dffzW3gDo7Oxe43kWhUCdvadmyJS1btmTRokX897//ZdasWezcuVMEk0KRuxt3V7FctZxIpCMIQtGytbVl+/btbN++nd27dyNJEgaDgWeffZZ+/frx8ssvi3H6hcTmwIFsA0kAqawm4TGZqBwUpFilHzkSQ58+T3bx8cFh/Hg0d+6QOGMGxvbtAUhMbW0pLLKHR6GXIWTP2LIlCR9/jO2mTRiaNiVpzBhrV6nIXbx4kSNHjtCsWTMAkpOTee+998zTZ4SEhPDvv/+ybds2PD1TGivmzp3Lyy+/zO+//06LFi0on9rrwdXV1dzimZ6dnR0ODg5otVqL24uzQg0mb926RVBQELt27eLcuXNoNBrap/4CEoSidPuxMrlCVScRTAqCYB0DBw40d3USiobuu+9y3CetO2VZo7l6Fdt006PJTk7o//MfxT7Gli2JPXEiJYmOeOBR5uiHD0c/fLi1q1GkTp48iZ+fH0ajEYPBwP/93//x1ltvsWvXLtzd3RXzMN64cQM3NzdzIAng5eWFm5sb169fp0WLFtZ4CUVG9WDy7t277N69m6CgIH5PnZ+mVatWLF68mL59+1K5cmW1ixSEHImWSUEQrG3cuHEMGDCADh06oMljQhKhAJKTsTl8WLmqWzdMNWpgt2aNeV1ZnR5Ec+GCYtnYrBlYGjsqPrNCGfLss8/y9ttvo9PpcHNzQ6d7EjKlTSMopFA1mOzevTsnT57EZDLh6+vL+++/z4svvohXQQZhC4IK7sYqg0kxZlIQhKK2Z88etmzZQqVKlejTpw/9+vUzJ1oQCo/211+R0mVqNbm5Ef/VV+iOHFEGk//8Y43qWZ02YzDZuLGVaiKUFbkdw5iQkJDvqTgKyt7enmrVquVq3xo1ahAVFcXdu3fNrZO3b98mKiqKWrVq5bpMnU6HKYu5WoszVR8zRUdHExAQwOnTpwkODmbChAkikBSKhduxym6uXuXE51IQhKIVGhrKl19+Sdu2bdmyZQu9evWiUaNGvPvuu/zxxx/Wrp7qAgMDeeaZZ8xztP32229WqYfN/v2KZUPXrqDVYmzUSLFee+YMJCUVZdWKBe155RzMIpgUhLxp0aIFderU4f333+fSpUtcunSJ999/n3r16pnHWeaGp6cn9+7d4/Lly0RHR6PX6wux1upRNZj87bffmDZtWp6icEEoCplaJp1Fy6QgCEXL3t6ePn36sHHjRkJDQ1m3bh3PPvss69ev54UXXsDX15d58+ZZu5qq2LVrFzNnzmTq1Kn8/PPPtGzZkoEDB3Lr1q1CKU8KD8d+zhycevXCdvVqSDfvm+6HHxT7JnfvDoBcvTqmdC0PUlIS2rNnC6V+xZn2r78UyyKYFIS8kSSJDz/8EFdXVyZMmMCECROoVKkSH374YZ6SqnXs2JHWrVszadIkevTowZEjRwqx1uqRoqOjc55pM4/+/vtvDh8+zM2bNwHw9vamS5cuNGjQQO2iSpzQ0FB8fHysXY0SLy/XUZZlqqyoQpLxyRPnm2NuUt6ubM4nlpH4TKpHXEv1lKVr+fjxY7755hvmzZtHbGwsDx48sHaVCqxz5840atQ7FR9lAAAgAElEQVSIzz77zLyuadOm9OnThzlz5qhWTmhoKL5vvonuzz8V62N378bYsSMkJVHe0xMpXdexR2FhkJpy32H4cGx37jRvS5g3D/2ECarVr7iTHj6kfLoGAFmnI+b2bbCzs2KtSray9LsrJ/Hx8eh0Oi5duqQYc5hb1uzmWtrk5lrGxMRw+fJl/P39FcmEcqLqmElZlpk2bRpffvklsiybEwyYTCbmzp3LsGHD+Oijj0Tqc6FIPUh8oAgky9mWE4GkIAhWl5CQwKFDh9i1axfff/89CQkJ1K5d29rVKjC9Xs+ff/7JhAxB2fPPP8/JkydVL09KTMy0zubwYYwdO6K5eVMRSJq8vMyBJICxVStIF0zqjh8vU8FkxuQ7pnr1RCApCEKeqBpMLl++nA0bNvDKK68wfvx485OZ0NBQVq5cyYYNG6hevTqTJk1Ss1hByFbGaUFE8h1BEKxFr9dz5MgRgoKCOHjwIHFxcXh5eTF8+HD69++Pr6+vtatYYPfv38doNGbK3l65cmUiIiKyPC40NDRf5emTksj4vD3xzBlCQ0Nx+eUXyqVbH1uliqIch6pVUYycPH6c0CtXysz0F1W//Zb006JH16jBtXy+D8IT+f0slza2trbY2dmRmJiIVqvN1zkSEhJUrlXZldO1jIuL49GjR1y/fp3Y2Fjz+pxa2lUNJv/73//Su3dvVq5cqVj/1FNPsWLFCmJiYti8ebMIJgXVnI88z55/9pCQnPUX5NZj5RgdMS2IIAjWMHr0aL777jseP36Mu7s7gwYNon///jz33HPWrlqxkJ+ugaGhodhaaEkrFxaGj48PthnGHNk3aqQsp1YtZGdnpNQbJ5uHD6mv0WCqWzfPdSlpbDZtwjEwULHOsU0b0UWzgEQ31yfSurmGh4eLbq5WlptrmZycjIuLCzVr1rReN9ewsDDGjRuX5fYOHTpw6NAhNYsUyrBr0dd44ZsXSDRm7uKUHdEyKQiCNRw6dIi+ffvSv39/2rVrV2rnmqxUqRJarZbIyEjF+sjISNzd3VUvL/boUaSkJMrVqWPu0qq5dQtiY9Fcu6bY11injvJgnQ5DixbYBAebV2lPniz1waT2999xtPBg3/jMM1aojSAIJZmqf8kqV67M2WwyoZ09ezZTtxdByK9D1w7lOZAEqF6+eiHURhAEIXuhoaF89tlndOjQodQGkpDStc3X15fgdAEaQHBwMK1atVK/QEdH5AoVMNWsqVit+ecfNFevKtaZLGSbNzZvrljWnjmjehWLG5tvv820ztCiBcb/+z8r1EYQhJJM1ZbJfv36sXLlSqpVq8aoUaMoXz4lycnjx49Zu3YtX3/9dbYtl4KQF2GPw/J8jIPOgb4+fQuhNoIgCNlL6+YVHR3Njz/+qMh43rFjR1xdXa1ZPVWNGzeOUaNG0axZM1q1asWGDRu4d+8eQ4cOLbQyTfXqoU0XPGovX0bz77/KfSwkODI2bapYVgSTslwqx09mnC7l0XPPwe7dkM9xbYIglF2qBpPvvPMOFy5cYOHChSxevNjcnSUiIgKj0UinTp0ICAhQs0ihDMs4d+TA+gN5xj3rLjq2Wlue934en4piLIMgCNaxfPlyPvzwQ5KSkpDTzYVob29PQEAAEydOtGLt1PPiiy/y4MEDPvroI8LDw3nqqafYtm0b3t7ehVamqX59OHjQvOw4alTmfSy1TGYMJs+fR7pzB8fRo9GePYv+jTdInDu31ASV0p07aC9eNC/LGg1XFyygtr29FWslCEJJpWow6eDgQFBQEN999x1HjhwxT07s7++Pv78/Xbt2VbM4oYy7E3tHsTy40WA6eHewUm0EQRCyt3nzZubOnUuHDh0YM2YM9evXB+Dy5cusWbOGuXPnUqFCBV577TUr11QdI0aMYMSIEUVWnrFevWy3mzw9wckp03rZwwOTlxea2ymZvyW9HscxY9D9/DMAdsuXY2jaFFONGsg1aiBXqKB+5YuQ7uhRxbKxWTOM5cV0WYIg5I+qwWSa7t27071798I4tSCY3Y5VTvlR1VlkaRUEofhas2YNHTp0ICgoSDHfcs2aNenSpQt9+/Zl9erVpSaYLGqm1OA8y+0WWiXTGJs0MQeTALqfflJsdxoyBADZxYXY/fsxNW5cgJpaV8Zg0vD881aqiSAIpUHpzQAglGom2ZSpm6uns8jSKghC8XX16lV69OihCCTTSJJEz549uZohYYyQe8YcpmMw1aiR9bEZurpmRXr0CLvPPstTvYoVWUaXITGSwc/PSpURBKE0EMGkUCJF66NJNiWbl13sXHC2dc7mCEEQBOtycXHh+vXrWW6/fv06Li4uRVeh0sbFBZOXV5abTdl0gzXkMpgE0P3yS56qVZxI9++jefjQvCw7OeU6kBaEsuby5cu0a9eO0aNHW7sqxZoIJoUSKSIxQrEsurgKglDcde3alS+++IKtW7cqku/Issy2bdsIDAykW7duVqxhyad//XWL62UbG/QvvpjlcUZf31yXoblzBykiIucdi6GM9TZ5eooMroKQhb1799KvXz+uXr2a7YNANRgMhkI9f2EqlDGTglDYwhPDFcsimBQEobibM2cOp06dYsyYMcyePZvaqdNUXL16laioKBo0aMCcOXOsXMuSLWnGDIxPP4304AHJL76I9vx5tMePY+jaFTmbbq64umKsVw/tlSu5Kkf7++8YSmBuCCkyUrEsi7m/BcGipKQkjhw5wqpVq0hKSmLfvn2MHz+euXPnotfrWbhwoXlfk8lE//79efnll/nPf/6DLMv873//Y/fu3URFRVGtWjUGDx6Mv78/AHfv3mXAgAHMnTuXPXv2cOHCBcaNG8cLL7zAJ598wtmzZ3n06BFVq1bllVdeoUePHuayEhISWLp0KT/99BP29va89NJLnD9/HhcXF2bNmgVAcnIyX3zxBYcPHyYmJoZatWoxZMgQ2rdvXyjXqsQHk4sWLWLx4sWKde7u7lxJ/YMgyzIffvghmzZtIjo6mmbNmrF06VKeeuop8/7R0dHMmDGDg6kpxbt27cqSJUtK1ZxfpU1EgmiZFAShZKlYsSLBwcF8+eWXioznTz/9NP7+/gwZMgQ7Ozsr17KEkyRFkGds1Qpjq1a5OtTQpk3ug8k//iiRwaQmXPkgVk6dwk0QilLbtm3zdVz9+vXZsGGDxW3Dhg3j8uXLFrf9+uuveS4rODiYKlWqUKdOHfz9/Zk9ezajR4+mS5cuvPvuu8TGxuLsnDK86syZM9y/fx+/1PHH69atIzg4mKlTp+Lt7c2FCxdYvHgx5cqVo02bNuYy1qxZw/jx4wkICECn06HX66lXrx6vvvoqTk5O/P777yxZsgQPDw+aN28OwOeff86ZM2dYuHAhbm5ubNy4kbNnzyoCxQULFnD79m3mzp1L5cqVOX78OLNnzyYwMBCfHMaW50eRBJP37t3j0aNH5jToavPx8WHfvn3mZW26LhvLly9n5cqVrFy5Eh8fH5YsWUK/fv04deoU5cqVA1LSl4eFhbFjxw4AJk6cyKhRo9i6dWuh1FcouMhE5dNVEUwKglAS2NnZMXr0aDEGpxgytm0LGzdmWh//5ZeQnIzjyJHmdbpTp0gqwrqpJVM3VxFMCoJF+/btM7ckNmnSBHt7e44dO0a7du1wcnIiODiYXr16AXD48GGaNm2Km5sbCQkJfPPNNyxbtgzf1O7zVatW5eLFi+zcuVMRTA4YMIBOnTopyn311VfNP3t5efHHH39w5MgRmjdvTnx8PPv372f27Nm0bNkSgICAAPr162c+JiwsjO+//54dO3ZQpUoVczknT57k22+/Zdq0aapfK1WDyY0bNxISEsKqVavM66ZPn8769esBaNy4MUFBQVSqVEnNYtHpdHh4eGRaL8syq1evZvLkyfTp0weA1atX4+Pjw44dOxg6dCiXL1/m+++/5+DBg+Y3ZtmyZXTr1o3Q0NBCieCFghNjJgVBEAQ1GVq3trje6OMDjo6KddrTp8FoLHHjDTUZu7mKYFIQMgkLC+PcuXPmYQeSJNGlSxf27dtHp06d6Ny5M4cPH6ZXr17o9Xp++uknJk2aBKQkUtPr9UydOlWRudtgMJiDuzQNGjRQLBuNRr766it++OEHIiMjSU5OJjk5mSZNmgBw+/ZtDAaDonelg4MDtdJNe3TlyhVkWWbw4MGKc+v1epo1a6bC1clM1WBy/fr1tErXneTYsWMEBgYycOBAGjZsyNKlS1m6dCmLFi1Ss1iuX79OgwYNsLW1pXnz5rz33nvUrFmTGzduEB4ezvPp5lBycHCgTZs2nDx5kqFDhxISEoKzs7Oi3s899xxOTk6cPHlSBJPFlBgzKQiCIKhJrlYNU5UqaO7dU6w31akD9vaYKlZE8+ABAFJsLJrz5zHlIXFPcSBaJgUhZ3v37sVoNNK/f3/zurSkaeHh4fj7+zNq1CgiIyP566+/SE5OpkOHDkDK+EnA3D01PZ1OGXbZ29srlrds2cKWLVuYPHkytWvXxtHRkbVr1/IwXQbmnJhMJiRJIjAwUFFeYmJioWULVzWYvHHjBkNSJ/YFCAoKwsvLizVr1qDRaHj06BFBQUGqBpPNmzdn1apV+Pj4EBUVxUcffUSXLl04ceIE4aljAypnGGBeuXJl7t5NmaMwIiKCSpUqKZ4eSJKEm5sbETlkawsNDc1XnfN7XFELTwjnauxVRdbB4iIsPkyxbHhgINRQMq5rcVRSPpMlgbiW6snvtRQPAYX8kt3dIUMwiYMDAMYWLdAcOmRe7dS3L8kvv4yxZUuS0910FmciAY9QHOR2DGNCQgIOqd+/nGQ1ljKvDAYDBw4cYPTo0ZnGdn7wwQfs37+fYcOG4eXlxZEjR7hw4QLt2rXDMbX3Qs2aNbG1teXevXt5bgk8d+4cbdu2pWvXrkBKAHvz5k3zsDwvLy90Oh2XLl3CK3UapMTERK5du2ZerlevHrIsc//+fUX5ebmWeaVqMGk0GrGxsTEvBwcH4+fnh0aTMgNJ7dq1uZfxl3QBvfDCC4rl5s2b4+vry//+9z9atGihalkZ5eeGpaR0nd3/736G/DgEg6lkpCpu3ag1rvYiYVJ+lJTPZEkgrqV6xLUUrEH/yis4nDtnXjaktjakbbNJF0xqoqOxW7sW1q4lISoK/ahRRVrX/BAJeAQhe8ePHyc6OprevXtnasnz8/Nj9+7dDB06lC5durB3717u3bvHggULzPs4OTkxaNAgVqxYgSzL+Pr6Eh8fz19//YVGozEPu7OkevXq/PDDD5w9exZXV1d27NjB3bt3zcGko6MjPXr0YPXq1bi6ulKpUiU2bdpkbo0E8Pb2pkuXLixYsIAJEyZQr149YmJiCAkJoUaNGnTs2FH1a6bqPJM1atTgp59+AlIyG12/fl3RxTQiIsJ8QQqLs7MzDRo04OrVq+bm5cgMT+IiIyNxT/0F6u7uzv379zPN+RUVFWXepyz6/I/PS0wg6aBzwMVOTPQtCIIgFIx+8GBM3t7m5aRx48w/G3r3xpBFt1b7OXPQ/PNPodevoDK2TIpuroKgtHfvXpo2bWqxS2inTp24e/cuISEh+Pv7c/PmTZycnMw5V9K8+eabDBs2jC1btjB48GCmTJnCjz/+iKenZ7ZlDxkyhKeeeoqpU6cyduxY7O3t6dKli2Kf8ePH8+yzz/L2228zYcIE6tSpYx7ql+bdd9+lR48erFq1ildeeYUZM2Zw/vz5TGM21aJqy+SwYcOYPn06f//9N3fu3MHLy0txEU6cOJFpsKnaEhMTCQ0NpV27dtSoUQMPDw+Cg4Np2rSpefvx48f54IMPAGjZsiWxsbGEhISYx02GhIQQFxenGEdZ1vx9/29rVyHX2ldvr+imLAiCUBwtXryYXr160bBhQ4vbL126xJ49e3j77beLuGaCmbMzsYcOYXPgAEZfX4ypiS8AkCSS5sxBly5zonlTYiIO48YRd/AgFNe/RyZT5m6uIpgUBIUlS5Zkuc3Ly0vRRTer7rqSJDFw4EAGDhxocbunp6fFY8uXL5/jUEBHR0fee+8987Jer2fbtm20TpdATKfTMXz4cIYPH25eV2K6uY4YMQJbW1sOHz6Mr68vkydPNg8uffjwIZGRkQwbNkzNIpk1axZdu3alWrVq5jGT8fHxDBo0CEmSGDNmDJ988gk+Pj7UrVuXpUuX4uTkxIABA4CUOWv8/PyYMmUKn376KQBTpkzB39+/zHaxepj4kOikaPOyTqOjQ/UO2RxR9OLi43BydKJOhTrMbDXT2tURBEHI0Ycffkjt2rWzDSYXL14sgkkrkz090Wdxr2Lo2BH9iy9iu2tXpm26kyfRnj6NsZAyJuaH9tgxHKZPB0ki8d13kYxG8za5fHnIkABEEITi7cqVK1y/fp2GDRsSHx/PV199RXx8PJ07d7ZanVSfZ/L111/n9ddfz7S+QoUK/Pjjj2oXx507dxgxYgT379/Hzc2N5s2bc+TIEbxTu6lMmjSJhIQEpk+fTnR0NM2aNWPXrl2K7raBgYHMmDHDnLWpW7du2T6ZKO2uRV9TLPtU8GFnv51Wqo1lYjyVIAilTWxsrCLvgFAMSRIJa9akJNyxscFu2TJ0x4+bN2suXCj6YFKvx37GDHSnTqF/9VX0Y8emrDeZcBw3Ds3NmwA4ZZgqwCSS7whCifTNN99w8+ZNdDoddevWZeXKlVYdmqd6MAkQExPDH3/8QWRkJB07dizUF5hT9iZJkggICCAgICDLfVxdXVm3bp3aVSuxrkZfVSzXdKlpnYoIgiCUcBcuXOD8+fPm5ePHj2MwZB6PHh0dzYYNG8RDspLA1hZDjx5AynyT6YNJ7ZUrJBdxdew+/hi7jRsBcHjnHYzPPouxbVs0166ZA0lLRBdXQSh56tWrp1rmWrWoHkx+/PHHfPLJJ8THxyNJEkFBQeYkN40bN2bBggWqd3UV1HX1kTKYrO1a20o1EQRBKNn27dvH4sWLgZSHm19++SVffvmlxX3Fg82Sx1SvnmJZc+VK0VbAaMQ+9fOVxjYwkIS2bdGeOZPtoWJaEEEQ1KBqMLlhwwbmz5/P66+/TqdOnRg6dKh5W6VKlejevTu7d+8WwWQxl7FlsraLCCYFQRDy44033qBr167Isszzzz/PO++8k2lKK0hJJ1+rVq1Mk1oLxZsxQ0uytoiDSe2xY5nW2ezfT4Jen2MwacowobogCEJ+qPpXa+3atfTt25fly5fz4MGDTNufeeYZVq9erWaRQiHIOGZStEwKgiDkT5UqVczp2Pfu3Uv9+vWpLFqESg1TnTrIkoSUOr2YdPMmJCRAIWVNzMh28+ZM6yS9Ht1PP4mWSUEQioSq80xev36dDh2yzvrp6urKw4cP1SxSKAQZu7nWcq1lpZoIgiCUHv/3f/8nAsnSxtERuXp186Iky2j+/bfwy42Oxn7KFItZZQFstm9He/ZstqcQc0wKgqAGVVsmXV1dicwwh1F6ly5dwkN0qyjWYpJiiIx/8h7aaGyoVq6aFWskCIJQMo0bNw5Jkli+fDlarZZx48bleIwkSaxYsaIIaieoxVivniLRjTY0FFPjxoVapuOoUdgcOpTldttt2xTLso0NUrIyNZDs5VUodRMEoWxRNZjs0qULmzZtYsSIEZm2Xbhwgc2bN1ucNkQoXAmGBPb+s5ebMVlndUsTFR+lWK7hUgOdRozhEQRByKuff/4ZjUaDyWRCq9Xy888/I+UwoX1O24Xix+TjA99/b14u7CQ8mmvXsg0kLTG0b4/+9ddxHDYMyWjEWLMmhmx6kgmCIOSWqlHCrFmzCA4OpnXr1nTp0gVJkvj666/ZtGkT+/fvp2rVqsyYMUPNIoUcyLLM0O+GcvDqwXwdL5LvCIIg5E/6KUEsLQulQ6aMrqGhhVqezf/+l2ldwrx5mOrVw/G115D0+kzbjU2aYOjTh9jg4JTpS7p2BVvbQq2nIAhlg6rBpIeHBz/++CPz5s1jz549yLLM9u3bKVeuHAMHDmTu3LlUrFhRzSKFHBy+fjjfgSTkfbxkcjKcO6clKSnfReZKWJgzkZFaatUy4ekpK7bduiVx9KiOhATlE347O+jY0UCtWiYAzp/X8MMPOpKScm4JcHSU6d7dQJ06JvO6xEQwGjPvK8sQHy/x8KFkcXv6/eLiJOLiJHx8jFSrJme9cwbh4RL376vTgnHjhj3JyfkbPl29uoly5VSpRoEYjSmfveyud1a0WrC3V67T61POl1cJCRri4vJ2jIMDaFQdvV5yJCfDX39piI9XfparVjVlcUTJ0r59e9577z38/PwA2LJlC23atKFGjRpWrpmgpkwZXS9cSPkFn76V2WTCZts2tBcvpmzLDYMBzT//IMXEYGzWjKSRI5G9vbHdskWxW8LHH6MfPhyAuP37cRw8GE14uLKOrVunVOOZZzA980weX6EglC3z58/nwIED9OzZM9M89atWreLrr7+mTZs2fPTRR1aqYfGiWjCp1+s5deoUVapUYfny5SxfvpyoqChMJhNubm5oyurdkgV6PRw/ruX+/eyvSYXK8RirHePnkBgu/KXN143yWZcPIZ8PH7WSlhfrvZjr/e/dk/DzcyYsrCje6wbmnz77LJ7XX0+5879xQ6J9+3I8emQ50HJykvnhh1gSE6FLF2f0+twHZB99JPPbb4+xtYXXXnPk5El1u//WqWPExSXrmww7O+jRI5mbNzWsW2enYsn5H9tjYyOzZk0CXbokc/my1hyEGY3K61q3rhFvbxlZTgn4spr9YPt2G44c0ZGc/CQgtzC/OwCJiRK3b2uIiQFZzn9grdHIdO5sYPPmeGxtYcwYB4KCbEhOzs85m+b5iIoVTXzySQJ9+2bxQrNgMkFAgD1bttgSH5/nYq1Cp0t5oLNmTTySBN26OXPxojbTfhMmJFEaRkT89ddfREU9GTowbtw41q5da7VgcuPGjezYsYNz584RExPD2bNnM9UlOjqaGTNmcPBgykPIrl27smTJElxdXc37/PXXX0yfPp3Tp09ToUIF3njjDWbMmKHoovvtt9+ycOFCrl27Rq1atZg1axa9evUqmhdaxEz16yuWtX//jc0335A8aFDKiuRkHEaOxDYoKN9l6E6exHbNGgydO6MJCzOvlx0c0A8caF42tmhB7PHj2K5ahe369WgePkTfrx+G55/Pd9mCUBZ5eHjwww8/MHnyZBxSszMbDAYOHjwo8r9koNrdsE6no2/fvixcuJA6deoA4ObmptbpSw1ZhldeceT7722y31EywTA/qH48ZVmlBt3xTcdjp805ELHR2uBXw4/mns1zfe5Fi+yLKJBUWrrU3hxM7ttnk2UgCSktgUFBNuj15CmQBIiJkTh0yIY7dyTVA0mAf//NfFOd0fHjxWv8anKyxPDhjjg5ycTFZX09NRqZPn2SOXFCx/37ErNmJTJxorIr1po1tsycWTTp9NMzmSSOHLHhs8/scHOT2bataLt+PXigYdo0B3r1eow254+A2YYNtqxdq+ZDhcJnMMDBgzasW2dH9eomi4FkaeLt7c3Ro0fp2bMnzs7OyLJs1TGR8fHxPP/883Tv3p133nnH4j4jRowgLCyMHTt2ADBx4kRGjRrF1q1bAYiJiaFfv360adOGo0ePEhoayrhx43B0dGTChAkAhISEMGzYMAICAujVqxd79+7ljTfe4NChQzRvnvu/KSWFXLkyyc8/j83Ro+Z19gEBGFq3xmbvXuwCA9HcuFHgciSTCZsjRxTrknv3hvLllfWpWJGkWbNIeucdpEePkCtUKHDZglDW1KlTh6ioKI4ePUqPHj0AOH78OLa2tjz77LPExMQAKclF165dy5UrV0hOTqZu3bqMGzeOxqlJuM6cOcOkSZP49NNPado05YHz7t27WblyJRs3bsSrFCTCUu3OVKPR4O3tTWxsrFqnLJVu3rTLOZAEqHrqSSCpkhqPX2J++/mqnjNNbCzs3JmL11UIbt7UoNenDP+4eTPnYPbWLU2+ujAChIVJpf4GOD+yCyQhJWALCnoSpM2da0+fPslUry5z5IiOTZts+e4763x+0hw4oMNkss6NflSUhshIiSpVctf9LTYWFi8uWYFkeufPp7Ril3YjR47knXfeMQdmkiQxcuRIRo4cmeUxkiRx//79QqnP2LFjgZSbG0suX77M999/z8GDB2nZsiUAy5Yto1u3boSGhuLj48P27dtJSEhg9erVODg40LBhQ65cucKqVasYP348kiSxevVq2rVrx7Rp0wCoX78+x44dY/Xq1axfv75QXpu1JX70Ebq2bZESEwHQREdT3te30MvVv/JK1hs1GhFICsVS27Zti7S8X3/9NV/H9ezZk3379pmDyX379tG9e3fu3Llj3ic+Pp6uXbsyefJkJElix44dTJs2ja1bt+Li4kKTJk145ZVXmDdvHps2beLhw4d8/vnnTJ06tVQEkqDymMnRo0ezYsUKBg8eLObSykJYmH3OOwFU/EfdgvVO3Nw4n6UOdtjZpdywpo3ti46WePToyf8GA9jYQNeuyYwbp8/VeK6gIBtiY5/ciJcrJ9O4cT765ebSH39o0OufVOzuXYkaNWTu3FFW9oUXkrGxQRGo3LkjZerC+PLLery9M4/TunBBy4ED6Y/VZCrD3l7OdI3s7GQqVpRzzG9gby+TlCRx6ZImX101XVzkAo8vS0rSY2eXt9a4a9c0JCbmP/AymST2709pRV68OJffiWxIkoyNTcr4x7w0/KQfq/fnn5l/HTo65n4cK4DJZMp1l/6EBGX33Dt3NFSp8uQ7ExsLb73lwC+/6DJ1cU9Kgujokjt04PZtCWdnZf3r1DHi7p5yvWvUKB1jJseMGUOTJk345ZdfiIiIIDAwkI4dO5p77xQ3ISEhODs706pVK/O65557DicnJ06ePImPjw8hISG0bt3a3O0LoHPnzixYsIAbN25Qs2ZNTp06lSlg7ty5M/Vq7goAACAASURBVOvWrSuy11LUTHXqkDR1KvYLFmS7n7FmTfRDh+b6F5VcoQJ2X3yB9ty5zGV6e2Ns1y5f9RUEIWcvvPACK1as4NatWzg6OnLy5EmmTJlCYGCgeZ9mzZopjnnrrbf46aefOHHiBP7+/kBKj49Tp06xaNEi7t27R5s2bejevXuRvpbCpGowGR8fj6OjI02bNqVHjx7UrFlT8QcHUp66Tpw4Uc1iS5R795Q37bVqGfH1Vd4phoTouO2SYRqPyAYQ/gz1GxhxyMO9d+g/GuKiKsKZYcj3fZifh4bJX3/V4ekpM2BAzs14mzcrX9fIkUnMnl14WXjatNFx8aKTefnOHQ01ahi5e1f5B3ry5CTKlZMVweTdu5lbJidNSqJhw8w3sIcP6xTB5N27Gu7cUZZx+vRjqlbNW9CR0f37EjduaCzmZTh/XsPkyY4Wj3v9dT3z5iUWqOy0Foe8ePllRw4dytySqNPJPPecEZ1ORpJSgsZjx7QWW/y++sqWf/+1HBD176+nX79kKlaUscuiAU6rlfH0lKlUSc5yDGZO2rd35tw5yy3NnTsns3Nn3gYj5uVavvSSI4cPP7mGt29LNE035HL+fPtcd7mdOjWRmTMLOetVAdy8qaFZsyeZmu7c0WRK3LRwYSL+/k/GjRZyQswi89xzz/Hcc88B8MUXXzBo0CAGphvjVpxERERQqVIlRVdcSZJwc3MjIiLCvE/VqlUVx6U9PI6IiKBmzZqEh4dneqBcuXJl8zlKK0Pq+5zl9tatiQ8MzPP8jsm9elG+fn2kDJnt9IMGld3sXYJQBMqXL0+HDh3Yt28f5cqVo0mTJlSpUkWxz8OHD/niiy84ffo0Dx48wGQykZSUxL1798z76HQ65s6dy+DBg6lQoQKfffZZUb+UQqVqMDl37lzzz2njKzIq68FkeLjy5nDAgGTefVf5B+Ktt+zZEH5LeeAfI+HEFA5ei6FChdwHLt9+q2PIEKecd8zCsWO6HIPJ69clTp1SfpQGD85nP9JccnfXK4LJu3c1gDH1/yeqVpUpV055ve7cyRxMZtW6l3H91asaReIkrVbGw6NggSRApUoylSpZbsmtWDHr81sr62XduiYsTXPWoIGJffuU6UxXrrTl3Xczj4XMqrvwxIlJfPBBwQLk3Hr++eQsg8lXXy3cz3DG9y59i3dyMmzdmrtuvxUrmpg4MQkb6/YSzla1asrXGh4u4eBQOjO4Zufhw4eqn3P+/PksXbo023327t1LuxLQghWazycI+T1ObbYGA1nlSY338eHi8uUQH5+vJyW127WjYrq5LAGutG6NXsXXXlyuY2kgrmUKW1tb7OzsSExMRJuXpAAqS0hIyNP+RqMRo9FIQkICfn5+LFmyBAcHB4YMGUJCQoJi+/vvv8/Dhw8ZNWoUVapUwcbGhunTp5OQkKAo98yZM5hMJh4/fsy9e/fQ5fcpeAHkdB3i4uJ49OgR169fVwxbzOkhuaqv5OzZs2qerlTK2DKZ8SYrZZ0MiRlaJh954+go4+qat8Cld28D7dsb+Pnn/L3VV6/m/NTzwgXlL4hWrZ5Mv1FY3N2VA65u306ZhuPePeUNqqenCTs7sLWVzQl3Hj9W7uPoKOPiYrmcjC2OGRMMVaki5ylpSn54emZ9La11A+7jYznwbdAg8/qxY/V4eKSMjdy6NeuWNnt7mbVr4+ndO29ZTQuiUycDn36aeb2rq4nu3Qs7mFR+tm7ffvLZ+uUXHQ8f5vzdc3U18cUXCVl+fosLOztwdzcREZHymmRZ4to15RfHy6vgD2VKisOHD3P48GFu3kz5Pe/t7U3Xrl3NU4jkxZgxY3jppZey3adatWq5Ope7uzv3799XJAqSZZmoqCjc3d3N+0RGRiqOS1tO28fDw8PiPmnbs5LXHhKQv54VhcbbG1mSkCx0MdF264ZPhvko80I7ZQqkCyYN7dpRo2PHfJ8vo2J1HUs4cS2fiI+PR6fTER4ergiecjuGMSEhIVMPx6Kg1WrRarU4ODjQtm1bbG1tiYmJwc/PD1tbW8X2CxcuMGXKFDp16gTAgwcPePDgATY2Nua637lzhxUrVjB16lROnjzJkiVLWL16dZEGlLm5lsnJybi4uFCzZk08PT1zfW5VX4W3t7eapyuVMrZMWppb0MvLBBEZWiZjquPlZcrTeDBIGZbx9ddxfPONLTduZL45tbeXcXFJCVJdXGQiIlIyS6a5di3nG9qMXRULc6xkGnd35Y3+nTsaIiIkxZQUFSuazPMHenrK3Lhh+eJ5emZ9XStUkLGzk7Oci7Iogjl7e3BzMxEVlfm9sNYNePr5NtOrXz/zeklKaYEfMCCZatVMfPxx5n7aTk4yf/0VQ7rZB4pEq1ZGHBzkTHOSTp2alGnuSbVlbpl8Uofdu5XNjC+9pLfYWlu5cuE/zFBL1apPgsmM7OzkPPW4KKkSExMZMmQIR44cQaPRmLtLHT16lA0bNvDCCy+wefNm7LLq221BpUqVqFSpkir1a9myJbGxsYSEhJjHTYaEhBAXF2debtmyJXPnziUxMRH71C9JcHAwnp6e5mlGWrRoQXBwsKIXUnBwsGIsZqlkZ4fs4YGUrntbGlP16gU6tbFtW5LGj8d2zRrkKlVI+OSTAp1PEITckSSJTZs2ASktrRl5e3tz6NAhGjZsSGJiIitXrsQmXVcho9HIvHnz8PX1pW/fvnTq1InXXnuNDRs2ZJuMrSQpXvMMlAHh4cqbRC8vSy2TJriauWXSyyd/gUu5cvDmm7lLnZiUBNOn25sTg4SFaUhISJlYPSuhocq72awCDTVlbJm0lBjH0/PJzWnVqiaLwXTG/TKSpJRjM7aiPDlv0dwAe3nJpJuuLl351mqZzCqYzP5BQvfuBj7+OPP6V1/VF3kgCSmBeseOBsW42FGjkhg/vvBTjWZ8EHD7toY//9Qwe7YDx44pfzUPHJic60yvxVXVqjJ//pnVtrw/KCuJFi1axOHDh3n77bcZO3Ys5VOndHj8+DGrV6/mww8/5MMPP2TOnDmFUn54eDjh4eH8809KgrfLly/z6NEjqlevToUKFahfvz5+fn5MmTKFT1Ob7KdMmYK/v7+5pWXAgAEsXryYsWPHMm3aNP755x8+/fRTxTyTo0ePpnv37ixbtowePXqwb98+jh07Zp67sjQzVauGxlIwqcLD9sT580kMCABHx7xlGhMEoUCcnLIeLhYQEMCSJUsYNmwYbm5uDB8+nOjoaPP2zZs3ExYWxubNmwFwcXFh1qxZTJs2jVatWvHss88Wev0Lm+rB5MWLF1m7di1//vknMTExmEzKm05JkvgzqzuKUs5kytwyaSmYdHV/BA5PPogYbSDOHS+vwu/+Z2eX0lp669aTP1Q3bmho0CDroCVjy2RWgYaaMgaTd+9KmRLjpL+22QVdOQVknp4y167l71i1VK1q4uxZZUCr1njN/PDwkHF2lhUZfIFsPycATZsa6d9fz86dT74H1aqZmDrVeslj3n8/kWvXNFy/rmH8+CTeeSepSO7TMn52bt/WMHKkI1euKN9nFxeZDh2KrutvYbH0uy5NUT2UsbadO3cyePBgZs6cqVhfrlw5ZsyYwa1bt9i+fXuhBZMbNmxg8eLF5uW07rErV67k1VdfBSAwMJAZM2bQv39/ALp168aSJUvMx7i4uBAUFMS0adPo1KkTrq6ujBs3jvHjx5v3adWqFRs2bGD+/PksXLiQWrVqsWHDhlI5x2RGpmrV4PffM68vYMukWTY3tYIgqGPWrFm53u7j48MXX3yh2N61a1fzz0OHDmXo0KGK7S1btuTnn39WoabFg6rB5PHjx+nXrx/ly5enSZMmnDt3jvbt25OUlERISAgNGjTAtwjmXSquIiMlDIYngZeLi5wpoyGAqVyYcsWj6iBriixwqV3bxK1bT+p59Wr2wWRoqDKYrFu3KILJzN1cs2uZzK71Mafrmt327G6Q1WRpbK2Hh/W6OEpSStfb2FhlBXIaKytJEBiYwIQJSdy5o8HRUaZlSyOOlpPVFol69UycOFH08+Nm/FxlNUdq797JOU4xUxJk910pqu+RtUVGRtKkSZMst/v6+rJt27ZCKz8gIICAgIBs93F1dc1xCo9GjRpx4MCBbPfp06cPffr0yXMdSzo5i/GpqgWTgiAIxYyqOaUXLFhA9erVOXXqFKtWrQJS5ls5ePAgBw4c4Pbt2wwYMEDNIkuUjMlbLAUIAOGJGYLJmOrZ7q+22rWVXRWzS8ITHY1iLJ+NjWxxvka1Va6cuWUy4/VNf7OeXUCYXaCZ0/aiGrNo6Wbb2tkv049PTZObjKKSBL6+Jrp3N9Cxo3UDSWsqVw7Kl8/+8/P000bee69oMtsWtuxaH639WS4qXl5e2T6N/vnnn0vNJNZllclCMGmqUAGLT44FQRBKAVWDyT///JPXXnsNFxcX88Tdad1cW7VqxZAhQ1iQw4S+pVlYmPLmO6vg8Pbj28oVj1LGWhRV4FK7trJe2SXh+fdfZctU7dqmImkts7eXqVjxST2NRilTN9D0WVAL0s21IMeqxdKNuLW7Bj79tPKhg5NT2eiqqKbsWuTGjEni2LFYKlcuHde1IA90SotXXnmFb7/9lgkTJnDp0iWSk5NJTk7m0qVLTJw4kb179zJ48GBrV1MoAEvBpCxaJQVBKMVUDSYlScIlNUe9Y2pzw4MHD8zb69aty6VLl9QsUlWBgYE888wzeHh40KFDB3777TdVz5/blslbjzNkcn2U8oeoqLqCZeyqmF3LZMYurkWRfCdNxmDqjz+ynmog+1aR7G9ki0MwWRxbJsePV45zXLAgb/M4Cdm/h717F+7UJEUtu4dh1v4sF5W33nqLwYMH89VXX9G2bVuqVKlClSpVaNu2Lf/9738ZPHgwU6ZMsXY1hQKw2DIpgklBEEox1acG+X/27jw8pquPA/h3si/EICsRKUEWIUQTjQoJpaGoNVJUrRVKUdqk1qKWeu1iqVhatUVESau0iF2lVRGqIkqskUTISraZef9IM9xkkswkk0yW7+d58jDnnnvvmZPJnPu759xz4uLiAAD6+vpo2rQpIiIi5A/yX7hwAQ0aNFDnKdUmLCwMAQEBWLFiBTp27Ijg4GAMHjwYv//+O5qoqSF4fR05oPiLq+J7JivvmcnX3blTfFfj7duVP/lOgUaNpII1LjMzi64xqej/hZW0LX+74t+TSCSrtBk2Ff3uNf2c2VtvSfD11y/x44+66NhRAj+/mhX8VIbibmTo68vQvn3FL7FTmUr6O6sta0xqaWlh3bp1mDBhAn799Vc8eJB/47BJkybo0aMHnJycNFxCKi9FvZAMJomoJlNrMOnt7Y2DBw/KZ6IbOXIkFixYgPv370Mmk+HcuXOYOnWqOk+pNkFBQfjggw8wcuRIAMDy5ctx4sQJbNu2TW0z68l7Jie4AKb/YLEWsHRd0Xy5kkIX5f89M/nfLPIVztZWeNF3754WzM0Vnzy3UFGbN6+8C2BVhqdaWsogEsnkS54UUGZG1OLOY2EhU+oZQXVQFNBqevijSARMmpSDSZMqfhmNmqq4z1b79hKosNRgtVDSeqm1oWfyxYsX8PX1ha+vL4YPH87AsYaSKbhhLuPzkkRUDchkZbuuVOsw188++ww7duxA7n8RxtSpUzFr1iw8f/4c6enpCAgIwJdffqnOU6pFTk4OoqKi4O3tLUj39vbGpUuX1HYeHR0ZjIwkgFYuoJODPOQgR1L0R4ZCv8zU8q9PpQojo6IXdzk5IoU/hYOzyu2ZLP5Db2Qkw38jrgHkTwxjbl40vzIzolpY5AeiRc9fee9VUWBhaFg7enNqsuJ6lz08qv9SIIoo+pvV0ZFp/MZIZTAyMsLVq1chkdSsHmcqRMG6QgwmqbbS1tZGdrbmlv4i5eXl5SE9PR1aWlrQVnHyE7X2TIrFYsHSHyKRCDNmzMCMGTPUeRq1S05OhkQigZmZmSDdzMwMiYmJxe4XGxur0nkCA4GAAGDwqSzce6HkTjnGwPNm6N37KWJj41Q6X3nY2TXH48f1VdpHT08Kff1YxMZWzsVSgwYPALRQuK1580zcvi38/bzxRgskJNQTpNnapiv1e2zWzBH//iucdrRJk+eIjb2nWqHLoVMnO5w/LwaQX9eNG99SW12r+lmm4qlSlyYmRgAci6Q7OMQhNrbylyupaDY2toiONhWkNWv2EnfuKK6zsn4uW7RQ/L2gaR4eHrhw4YJ8BAzVTDlDhkDvvyVeZCIRct9/X8MlItIMfX19iEQiPH/+HHp6ehCpuIhzZmamvIOKyqe4upTJZJBIJHj27BlycvJHmtWpU0elY6s1mKxtynLBEhsbC0MDPUCJYNIQDYGI5bBvrYdFiyR4443Ku0BatkwLo0ZJcPOmcncn6tWTYd68bLRv36yCS5YvNjYWI0ea4/LlHBw4oIucnFdfUE2bSrFyZdHfz/LlWhg9WoLY2Pz31LKlBN98o6XU73H1ain8/aXyocouLnlYsEAfTZtW7u9k3DgJEhJEmDMnGx06qKeuY2Njq+zFd3Wjal3a2QETJ2Zj+3Y9vHwpgomJDOPGZWPQIKsKLKXmLFokwoMHEvnMy9bWUqxaJVVYZzXxc/nNN99gwIABmDNnDsaMGQMbGxv5zOdUc2R/8QW0Y2KgFReHrOnTIbOp3NFFRFWJnp4ebGxscOHCBZUDymfPnlXZuVaqm9LqUiaTITs7G+3atZNPoqosUUpKilrHF8XExGDXrl2Ii4tDSkpKkfG3IpEIhw8fVucpyy0nJwdWVlbYunUr3n/tDuKMGTNw48YNHDlyRG3nio2NRdNmTZUal6yrrQstkWYvNHJyAGWGUOvqApV5TfT6hWZeHvD6yLHSnjUrGHGh6jNpMll+fYhEqBGLyBeoiRftmlLWupRI8j/HOjqolKV1NK3ge0VPT+GoQAA183NpaWkJmUwmvzuspaUF3UIPXotEIjx+/FgTxauWauLnRBNYj+rDulQsPj4ecXFxKvU0PnnyBJaWlhVYqtqjtLrU1taGtbU1mjZtqvKx1dozuXfvXkyaNAm6urqws7ODWCwukqesD3dWJD09Pbi4uCAiIkIQTEZERKBv377qP5929YlEqkPQpKOT/6Ossk5sIhKVfV+ikmhr144gskB1+F6pCP3791d5mBcRUU1gZWUFKyvVRt0wMFefiqxLtQaTS5cuRZs2bRAaGoqGDRuq89AVbtKkSfj444/h6uoKd3d3bNu2DU+ePMGoUaM0XTQiIqoBNm7cqOkiEBERqZVag8knT55g8uTJ1S6QBIABAwbg2bNnWL58ORISEuDg4ICQkBDY8FkHIiIqh6ysLBw5cgT37t1Dw4YN0aNHDw7dIiKiGkGtz0x269YNXl5emD17troOSUREVG3Fx8ejV69euHfvnvwxDyMjI+zduxedO3fWcOmIiIjKR61Tpnz99df44Ycf8Pvvv6vzsERERNXSokWLcP/+fUycOBH79u3DkiVLYGBggC+++ELTRSMiIiq3cg1zHTx4cJG0unXrolevXrCzs4O1tXWRhS9FIhFC/lt/iYiIqCY7deoU/Pz8sGjRInmaubk5xo4di0ePHqFx48YaLB0REVH5lCuYvHnzpsKZ6aytrZGVlYXbt28X2caZ7IiIqLZISEiAu7u7IK1jx46QyWR4+PAhg0kiIqrWyhVMXrt2TV3lICIiqnEkEgkMDAwEaQWvs7KyNFEkIiIitVHrbK5EREQkFBcXh8uXL8tfp6WlAchf96tOnTpF8ru6ulZa2YiIiMpDrRPwFHb27FlMnjwZgwcPxqxZs/DgwYOKPF2VFhwcjDZt2sDCwgJdunTBhQsXNF2kKm/JkiUQi8WCn5YtW8q3y2QyLFmyBPb29rC0tETv3r3xzz//aLDEVcP58+cxdOhQODg4QCwWY9euXYLtytRbSkoKxo8fDxsbG9jY2GD8+PFISUmpzLdRJZRWl/7+/kU+o927dxfkyc7OxsyZM9GsWTM0atQIQ4cOxaNHjyrzbWjcypUr4eXlhSZNmqB58+bw9fXFjRs3BHlq8udyyZIleOedd+Q/AwcOBAB8/vnngvTu3bvjnXfe0XBpqwe2qapjm1o2bFPVh22qelS1NrXcweTSpUthZWWFp0+fCtJ37dqFfv364YcffsDx48exYcMGeHt74/79++U9ZbUTFhaGgIAAfPbZZzhz5gzc3NwwePDgWh1cK6tFixaIiYmR/7x+wbBmzRoEBQVh2bJlOHnyJMzMzNC/f3+kp6drsMSal5mZCUdHRyxduhSGhoZFtitTb2PHjkV0dDRCQ0MRGhqK6OhofPzxx5X5NqqE0uoSALp27Sr4jO7fv1+wPTAwEOHh4di6dSuOHDmC9PR0+Pr6QiKRVMZbqBLOnTuHMWPG4NixYzh8+DB0dHTw/vvv4/nz5/I8NfVzGRQUhPXr1xf5UZRekEYlY5tadmxTVcc2VX3YpqpHVWtTy73OZO/evYvcXcjOzkaLFi2gpaWF77//Hq6urvj1118xceJE+Pr6YvXq1eU5ZbXTrVs3ODk5Ye3atfK09u3bo1+/fpg3b54GS1a1LVmyBIcPH8bFixeLbJPJZLC3t8e4ceMwY8YMAMDLly/RokULLFy4EKNGjars4lZJjRs3xjfffINhw4YBUK7eYmJi4O7ujqNHj6Jjx44AgIsXL8LHxwd//PEHWrRoobH3o0mF6xLIv4v67Nkz7Nu3T+E+qampsLOzQ1BQEIYMGQIAePjwIZydnREaGopu3bpVStmrmoyMDNjY2GDXrl3w8fHh55JUwja1bNimlh/bVPVhm6o+mm5Ty90zeefOHbi4uAjSTp8+jfT0dEyZMgWenp4wNjZG//79MWTIEJw6daq8p6xWcnJyEBUVBW9vb0G6t7c3Ll26pKFSVR9xcXGwt7dHmzZtMHr0aMTFxQEA7t27h4SEBEG9GhoawsPDg/VaAmXqLTIyEnXq1BHMQNmxY0cYGxuzbhW4ePEi7Ozs4OrqiilTpiApKUm+LSoqCrm5uYL6tra2RqtWrWp1XWZkZEAqlUIsFgPg55KUxza1fNimqhe/u9SPbarqNN2mljuYfP78OSwtLQVpZ8+ehUgkQs+ePQXpLi4uePLkSXlPWa0kJydDIpHAzMxMkG5mZobExEQNlap66NChAzZs2IDQ0FCsXbsWCQkJ6NGjB549e4aEhAQAYL2qSJl6S0xMRMOGDQXL+IhEIpiamrJuC+nevTs2bdqEQ4cOYdGiRbh8+TL69u2L7OxsAPl1qa2tjYYNGwr2q+2f04CAADg7O8PNzQ0AP5ekPLapZcc2Vf343aVebFPLRtNtarlnc7WwsEB8fLwg7eLFizAyMoK9vb0gXUtLC3p6euU9JdUShSei6NChA1xcXLB79268+eabGioV0SsFE6kAgJOTE1xcXODs7Ixjx46hb9++GixZ1fXll1/i999/x9GjR6Gtra3p4hDVGmxTqapjm6q6qtCmlrtn0tXVFXv27JHP/nP9+nVcuXIFXbp0KfKmYmJiat0CzQ0bNoS2tragmx4AkpKSYG5urqFSVU916tSBvb097ty5AwsLCwBgvapImXozNzdHcnIyZLJXj1PLZDI8ffqUdVsKKysrNGrUCHfu3AGQX5cSiQTJycmCfLX1cxoYGIgDBw7g8OHDsLW1lafzc0nKYpuqPmxTy4/fXRWLbWrJqkqbWu5g8osvvkB8fDxcXV3Rq1cv+Pj4QCQSYerUqYJ8MpkMP/30k2Bsbm2gp6cHFxcXRERECNIjIiJqXV2UV1ZWFmJjY2FhYYGmTZvCwsJCUK9ZWVm4ePEi67UEytSbm5sbMjIyEBkZKc8TGRmJzMxM1m0pkpOTER8fL/8id3Fxga6urqC+Hz16JH/wvTb54osv5I3e68sRAPxckvLYpqoP29Ty43dXxWKbWryq1KZqBwQEzC/7WwFMTU3h6emJuLg4PH78GE5OTli5ciU8PDwE+c6ePYszZ87A398fb7zxRnlOWe3UrVsXS5YsgaWlJQwMDLB8+XJcuHAB69evR7169TRdvCpr9uzZ0NPTg1Qqxe3btzFz5kzcuXMHq1atglgshkQiwerVq9G8eXNIJBLMmjULCQkJWL16NfT19TVdfI3JyMjAzZs3kZCQgJ07d8LR0REmJibIyclBvXr1Sq03U1NT/PnnnwgNDYWzszMePXqEadOmoX379rVuKvOS6lJbWxsLFixAnTp1kJeXh2vXrmHy5MmQSCRYvnw59PX1YWBggCdPniA4OBhOTk5ITU3FtGnTYGJigq+++gpaWhW61G+VMWPGDOzduxc7duyAtbU1MjMzkZmZCSA/OBCJRPxcktLYppYN29SyYZuqPmxT1aOqtanlXhqElBMcHIw1a9YgISEBDg4OWLx4MTp16qTpYlVpo0ePxoULF5CcnAxTU1N06NABs2bNkj+LK5PJsHTpUuzYsQMpKSlwdXXF//73Pzg6Omq45Jp19uxZ9OnTp0i6n58fNm7cqFS9paSk4PPPP8cvv/wCAPDx8cE333wjnymstiipLleuXIlhw4YhOjoaqampsLCwQOfOnTFr1ixYW1vL82ZnZ2P27NkIDQ1FVlYWPD09sWLFCkGemq64z80XX3yBwMBAAMr9PfNzSQXYpqqObWrZsE1VH7ap6lHV2lQGk0RERERERKSy2tEfTERERERERGrFYJKIiIiIiIhUxmCSiIiIiIiIVMZgkoiIiIiIiFTGYJKIiIiIiIhUxmCSiIiIiIiIVMZgkogqhbOzMwYOHKjpYhAREVV7bFOpqmAwSVRGu3btglgslv9YWFjA3t4eAwYMwKZNm5Cenq7pIhIREVULbFOJqicdTReAqLoLCAjAG2+8gdzcXCQmJuLcuXMIDAxEUFAQ9uzZg9atW2u6iERERNUC21Si6oXBJFE5devWDW+++ab89fTp03H6aG61iAAAIABJREFU9GkMHToUfn5+iIyMhKGhoQZLWHvIZDJkZWWxvomIqim2qVUH21RSBoe5ElWALl26YObMmXjw4AFCQkLk6devX8fEiRPh4uICCwsLNGvWDKNHj8aDBw/kef7991+IxWKsX7++yHGvX78OsViMrVu3Fnvue/fuQSwWY9WqVfjuu+/g4uICc3NzeHl54a+//hLk7d27N3r37l3kGP7+/nB2dlZ4zODgYLRt2xZWVlbo168f7t+/D5lMhhUrVsDJyQmWlpYYOnQokpOTFZbv9OnT6NKlCywsLODq6oo9e/YUyZOdnY2lS5eiffv2MDc3h4ODAwIDA/HixQtBPrFYjGnTpiEsLAweHh4wNzdHWFhYsXVDRETVD9tUtqlUdbFnkqiC+Pr6YsGCBTh58iRGjhwJAIiIiMDt27cxdOhQWFlZ4e7du9i2bRsuX76MixcvwsjICM2bN4ebmxtCQkLwySefCI4ZEhICPT09DBgwoNTzh4WFITMzE6NGjYJIJMKaNWswYsQIREVFQVdXt0zv6cCBA8jJycG4ceOQkpKCtWvX4qOPPkK3bt1w6tQpTJkyBXfv3sXmzZvx5ZdfYvPmzYL94+Li8OGHH2LkyJEYOnQo9u/fD39/f+jr68vfk0wmw/Dhw3H+/Hl8+OGHsLe3R0xMDLZu3YqbN28iLCwMIpFIfswLFy7g0KFDGDduHCwsLNCyZcsyvTciIqq62KayTaWqicEkUQVp3LgxTExMcPfuXXnamDFjMHnyZEE+Hx8f9OzZE+Hh4fD19QUADB06FNOnT8fNmzdhb28PAJBKpThw4AB69OiB+vXrl3r+R48e4a+//oJYLAYA2NnZ4YMPPsCJEyfw7rvvluk9PX78WHBMqVSKlStX4uXLlzhz5oy8QX369CnCwsKwevVqwfCYf//9F8HBwRg0aBAA4KOPPoKnpyfmzp2L999/H1paWggNDcXx48cRHh6Ot99+W75vu3btMH78eERERMDb21uefuvWLZw+fRpt2rQp03siIqKqj20q21SqmjjMlagC1alTBxkZGfLXRkZG8v9nZGTg2bNnsLOzQ7169RAVFSXfNmDAAOjr62Pfvn3ytLNnz+LRo0fyxrE0ffv2lTdQAODh4QEg/05mWRU+pqurKwBgyJAhgjuzrq6uyM3NxaNHjwT7m5mZCe4AGxoa4sMPP8TDhw9x/fp1AMDBgwdhZ2cHBwcHJCcny386deoEkUiEs2fPCo7p7u7ORo+IqBZgm8o2laoe9kwSVaCMjAyYmprKX6ekpGD+/Pk4dOgQnj9/LsiblpYm/79YLIaPjw/279+PuXPnQiQSISQkBPXr10fPnj2VOre1tbXgdUGDlZKSUta3U+SYJiYmAPLvGCtKL3yuN954A1pawntYzZs3BwDcv38fbdq0wb///ovY2Fh5emFJSUmC17a2tqq9CSIiqpbYprJNpaqHwSRRBXn06BHS0tLQrFkzedpHH32ES5cuYdKkSWjTpg3q1q0LkUiE0aNHQyqVCvYfOnQofvzxR5w/fx4dOnRAeHg4Bg0aBD09PaXOr62trTBdJpPJ/y8SiQSvC0gkEpWOqcy5lCWVSmFvb4+lS5cq3G5paSl4zVnmiIhqPrapbFOpamIwSVRBCobTFDyLkJKSglOnTiEgIAABAQHyfFlZWQrvbHbv3h1mZmbYt28fkpKSkJaWpvRwHGWJxWKFQ3RenwlPne7evQupVCq4k/rvv/8CAGxsbADk32mNiopCly5dBJMCEBFR7cU2tSi2qVQV8JlJogpw+vRpLF++HE2bNsWQIUMAQP5lX/jO4oYNG4rcQQUAHR0dDB48GIcOHcLOnTvRrFkzuLu7q7Wcb7zxBmJjY/H06VN52rVr13Dp0iW1nqdAUlKSYJrxly9f4vvvv0fjxo3lC1H3798fiYmJCqdqz87ORnp6eoWUjYiIqia2qYqxTaWqgD2TROV04sQJ3LlzB3l5eUhKSsKZM2cQERGBJk2aYM+ePTAwMACQ/8zD22+/jbVr1yI3NxdNmjTBxYsXceHCBTRo0EDhsYcOHYoNGzbg5MmTgjuv6jJ8+HAEBQVhwIABGDFiBJKSkrB9+3bY29tXSAPTvHlzfPbZZ4iOjkajRo0QEhKC2NhYbNmyRX5h4Ovri0OHDmHGjBk4f/48OnbsCJlMhtu3b+PgwYPYsWMHOnfurPayERGR5rFNVR7bVKoKGEwSlVPBcwh6enqoX78+HB0dsWTJEgwbNgx169YV5A0ODkZAQAC2b9+OvLw8eHh44PDhw+jXr5/CY7dp0wZOTk74+++/1T4cBwBatWqFTZs2YfHixZg1axZatWqFzZs3Y//+/Th37pzaz2dra4uVK1di7ty5uHnzJho3boygoCAMHjxYnkdLSws//PADNm7ciD179uDIkSMwMDCAra0txowZI7/bSkRENQ/bVOWxTaWqQJSSkqL607xEVGm8vLygp6eHY8eOabooRERE1RrbVCL14jOTRFVYdHQ0rly5Aj8/P00XhYiIqFpjm0qkfuyZJKqCbty4gaioKGzYsAEJCQm4evWqYHFmIiIiUg7bVKKKw55Joiro0KFDmDRpErKysrB161Y2ekRERGXENpWo4rBnkoiIiIiIiFTGnkkiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlIZg0kiIiIiIiJSGYNJIiIiIiIiUhmDSSIiIiIiIlKZ0sHkxYsXsWXLFkHagQMH0KFDB7Ro0QIBAQGQSqVqLyARERERERFVPUoHk19//TUuXLggf3379m34+/tDS0sLLi4u+Pbbb7Fp06YKKSQRERERERFVLUoHkzdv3oSrq6v89d69e2FgYIDjx49j//798PX1xQ8//FAhhSQiIiIiIqKqRelgMj09HWKxWP76xIkT8PLygomJCQDgrbfewv3799VfQiIiIiIiIqpylA4mLS0tERMTAwCIj49HdHQ0vL295dvT0tKgo6Oj/hISERERERFRlaN09NenTx9s2bIF2dnZuHz5MgwMDNCrVy/59uvXr6Np06YVUkgiIiIiIiKqWpTumQwMDETfvn0REhKCpKQkbNiwAWZmZgDyeyXDw8Ph5eVVYQWtKWJjYzVdhBqB9ag+rEv1YV2qD+uSlMHPiXqwHtWHdak+rEv1qci6VLpn0tjYGN9++63CbXXq1MGNGzdgZGSktoIRERERERFR1aV0z+T27duRmpqq+CBaWqhXrx50dXXVVjAiIiIiIiKqupTumZw+fToCAwPRs2dP+Pr6okePHpxwh4iIiIiISAP+evIXLidchlQmLTZPG7M2MIVphZVB6Wjw7NmzCAkJQVhYGA4fPowGDRpgwIAB8PX1RYcOHSqsgEREVDtE3PoLKdKH6G/fV9NFISIiqtLCYsIw+pfRpeab7DoZH1p+WGHlUDqYbN26NVq3bo2vvvoKZ86cQUhICPbt24etW7eiWbNm8PX1xeDBg2Fra1thhSUiItUdj4rFrsgTaGTzEg0byDRdHIVkkGH5mW+RpZOA3ccWILBzr9J3olohLy8PmZmZCrcZGBgU+wgOKY/1qDpjY2OO0CON2nVjl6aLAECFYLKASCRCly5d0KVLF6xcuRK//PILdu7ciSVLlmDJkiVwd3eHn58fhgwZAgMDg4ooMxERKem3qFsY/FsXQPclcFPTpSnFfy3Sb7K5iAj6F6c/WwMnR80WiTQrLy8P6enpEIvFEIlERbbr6+vzWkMNWI+qkclkSElJQd26dRlQksY8TH+o6SIAKEMw+brLly8jIiICf/75J2QyGZycnJCdnY1PP/0UixcvxtatW9GpUyd1lZWIiFS08EA40PClpouhMkNZQ9i30nQpSNMyMzOLDSSJNEUkEkEsFiMtLQ316tXTdHGolnqS+UTwemTrkdDT1iuSz93KHajAQUlKz+Za4NatW1i4cCHatGmD9957D7/88guGDx+OM2fO4Ny5czh58iROnz4NMzMzTJ8+vSLKTERESop+eFfTRVDdjYHwb/EJtLU1XZCq4/z58xg6dCgcHBwgFouxa5dweJNMJsOSJUtgb28PS0tL9O7dG//8848gT0pKCsaPHw8bGxvY2Nhg/PjxSElJEeT5+++/0atXL1haWsLBwQHLli2DTCa8Cjl06BDc3d1hbm4Od3d3hIeHq1wWVTCQpKqIn0vSpJd5L5Ga/WpourZIG6u6rcJyr+VFft6ze69Cy6J0z+SGDRsQEhKC6Oho6OnpwcfHB8uXL0f37t2hXajFb9OmDfz9/TF58mS1F5iIiFRQv1AwGT0M00aba6YsxXgcL8K+vf/dTX1qD/GDD9Dn0DUADTRarqokMzMTjo6O8PPzw4QJE4psX7NmDYKCghAUFIQWLVrgm2++Qf/+/fHHH3+gbt26AICxY8fi4cOHCA0NBQBMmTIFH3/8Mfbt2wcASEtLQ//+/eHh4YGTJ08iNjYWkyZNgpGRkbw9j4yMxOjRoxEYGIg+ffogPDwcH330EY4dOyafjE+ZshARUdklZCYIXpsbmUNLpHIfoVooHUzOmjULbm5uWLFiBfr37w+xWFxi/nbt2mHmzJnlLiAREZWDuFAweWoe5m2rWsHk+vV62HfCUP6628AcGBgUP815dZKWlobLly8jKSkJXbt2hbl52eq+R48e6NGjBwBg4sSJgm0ymQwbN27E1KlT0a9fPwDAxo0b0aJFC4SGhmLUqFGIiYnB8ePHcfToUbi5uQEAVq1aBR8fH8TGxqJFixbYv38/Xr58iY0bN8LQ0BCOjo64desWNmzYgE8++QQikQgbN25E586dMWPGDABAq1atcPbsWWzcuBFbt25VqixERFQ+hYNJC2MLDZVEhWGuly9fxrFjxzBq1KhSA0kAcHBwQEBAQLkKR0REZZeVlwWYPH6VIBMBqTaaK1Ax/vxTOLqlQweJhkqiXitWrICDgwMGDBiACRMmyId6Jicnw8rKCtu2bVPLee7du4eEhAR4e3vL0wwNDeHh4YFLly4ByO9RrFOnDtzd3eV5OnbsCGNjY0Get956C4aGrwX23bohPj4e9+7dAwD88ccfgvMU5Ck4hjJloYrRu3fvSrmJX1nnKY979+5BLBbjypUrmi4KUYUo/LykJoNJpXsmmzVrVpHlICIiNYt58kCYkNYYkOgjIyMLdeqo91wSCfDggQgNGshgYqLavn/8IWyK3NyqfzC5bds2LFq0CB9++CG8vLwEPXINGzZEr1698OOPP2L06NLXCCtNQkL+HWozMzNBupmZGeLj4wEAiYmJaNiwoeA5L5FIBFNTUyQmJsrzNGrUqMgxCrbZ2toiISFB4XkKjqFMWYoTGxtbJM3AwAD6+vol7peVlVXi9spmaWlZ4vYhQ4Zg7dq1Je6/ZcsW9OnTR6XzSqVS5OXllVgfu3btwvbt23Hnzh1oa2vD2toaPXv2REBAgNL1qMx5yurnn3/GuHHjEBkZCWtr6yLbfXx8YGtri40bN5Z4nOzsbPm/Ffn5SEtLk3/2X6fos0xlw7pU7HrcdcFrwzzDUuuqrHXZokWLErerNJtrVlYWwsPDERUVhbS0NEilwmFIIpEI69evV72URESkdlH37gsTUt4AAMTHa6FFC/UNI717Vws+PsZ48iR/sEvr1hKsWPES7u6lB4WPHonw6NGrQTL6+jI4O0vwX0dYtbV582a8//77WLNmDZ49e1Zke5s2bUq9IK5tFF2wpKamlrhkRVZWVpVb0iImJkb+/2PHjmHKlCmCNAMDg1LLrKenp/L70tLSgo6OTrH77dy5E7Nnz8bixYvRpUsX5Obm4p9//kFkZKS8XOo4T3n07dsXAQEBCA0NLTK67caNG7hy5Qrmz59f6rkLbkBU9JInJiYmaNKkiSCtYNg4lR/rsnjSRGEb3rJRyxLrqiLrUulhrg8fPsRbb72F8ePHY9euXdi1axeOHj2KvXv3Yvfu3Th27BjOnj1bIYUkIiLV/RNfKJh8nh9MPn6s3lkIZ840kAeSAHD9ujamTzcsYY9XCg9xdXGRQK/ozObVTlxcHLp06VLsdrFYjOfPn6vlXBYW+cObkpKSBOlJSUnyZzTNzc2RnJwsmJlVJpPh6dOngjyKjlGwreBcJZ1HmbLUdBYWFvKfgmUjXk8LCwtDu3btYGZmhnbt2uG7776T7+vs7AwAGDlyJMRisfz13bt34efnh5YtW6JRo0bw9PTE0aNHVSrXL7/8gj59+mDUqFFo1qwZWrVqhffffx+LFy+W5ynLeXJycjBv3jw4OjrCysoKXl5eOHHihHx7bm4uPv/8c9jb28Pc3BxOTk6YP3++wmPp6upi6NCh2L17d5FZhHfu3AlbW1t4enpi37598PLygrW1Nezs7DBy5Eg8fvxY4TEB4OzZsxCLxUhOTpanKRoKe/PmTQwZMkR+3DFjxsh724mqksLDXC2NSx4RUZGU7pmcN28enj17hl9//RXNmjWDnZ0dtm3bho4dOyIoKAjbt2/HoUOHKrKsRESkgn+fFeree61nElDPUNJbt7Rw/LhukfS//9bGixeAkVHJ+0dGCpuhN9+s/kNcgfxgsXBA9bp//vlHHniVV9OmTWFhYYGIiAi0b98eQH6P3cWLF7FgwQIAgJubGzIyMhAZGSl/bjIyMhKZmZny125ubpg/f76gty8iIgJWVlZo2rQpAODNN99EREQEpkyZIj9/RESE/BjKlKW8xKtLn7dBnVKmppSeSUnh4eGYOXMmFi9eDG9vb5w4cQKfffYZzM3N4ePjg4iICNjZ2WHt2rXo2bOnfLb8jIwMvPPOO5g9ezYMDQ0RFhaGESNG4Pz582jZsqVS57awsMCZM2cQFxcHW1tbhXnKcp5Jkybh7t272LJlCxo3boxff/0VQ4cOxcmTJ+Hs7IxNmzbh559/xtatW2FjY4PHjx+XONxuxIgRWLduHc6cOSO/IZOTk4OQkBD4+/tDJBIhJycHgYGBaNmyJZKTkzFv3jyMGTMGv/zyi1J1ociTJ0/Qq1cvjBgxAgsXLkRubi4WLlyIDz74AL/99hu0tDQzUyaRItVyAp5Tp05hzJgxePPNNwV/UPr6+pg+fTo8PDwQGBhYIYUkIiLVPcwsFEw+fz2YVI///a/459mSkkrvAf39d2HP5Jtv5pW7TFVBjx498N133ynsfbx+/Tq+//579OrVS+njZWRkIDo6GtHR0ZBKpXj48CGio6Px4MEDiEQi+Pv7Y82aNTh8+DBu3LiBiRMnwtjYGIMGDQKQP+tq9+7dMW3aNERGRiIyMhLTpk1Dz5495UOfBg0aBENDQ0ycOBE3btzA4cOHsXr1akycOFH+rOWECRNw5swZrFq1Crdu3cLKlStx9uxZ+Pv7A4BSZanN1q9fD19fX4wfPx52dnb4+OOPMXjwYKxZswYAYGpqCgCoV68eLCws5K+dnZ0xevRoODk5oVmzZpgxYwbatm2r0k38L774Ag0aNICLiwvat2+P8ePHY8+ePcjNzZXnUfU8d+/eRWhoKLZv345OnTrB1tYW48ePxzvvvIMdO3YAAB48eIDmzZvDw8MDTZo0gbu7O4YPH15sOVu2bImOHTti586d8rQjR44gJSUFw4YNA5AfcPbo0QO2trZwdXXFypUrcfHiRTx69Ejp+ihs69ataN26Nb766iu0atUKrVu3xubNm3H58mVO5ENVTuGeSStjKw2VRIVgMjMzU34nS++/MUjp6eny7W+99RbOnz+v3tIREVGZJeUq7plUxzDXx49F8PMzQkhI8WNSk5JKbmKePRPhr7+EwWTHjjWjZ3L27NkA8tvG+fPnQyQSYdeuXRg9ejS6desGCwsLfP7550of78qVK/D09ISnpydevnyJJUuWwNPTUz5E8dNPP4W/vz9mzpwJLy8vPHnyBGFhYYJ1HYODg9G6dWsMHDgQAwcOlF8sF6hXrx4OHjyI+Ph4eHl5YebMmZg0aRI++eQTeR53d3ds27YNu3fvRqdOnbB3715s27ZNvsaksmWprWJiYgQz6gL5n5GbN2+WuF9mZibmzp0Ld3d3NG3aFI0bN8aVK1fw8OFDpc9taWmJ3377DRcuXIC/vz9kMhmmTZsGb29vvHjxokznuXr1KmQyGTp27IjGjRvLf3799VfcvZu/LNEHH3yAa9euwdXVFTNmzMCxY8eKzLlR2IgRI/DTTz8hJSW/V/iHH35A9+7dYWWVf8EcFRUFPz8/tG7dGtbW1vDy8gIAlepD0Xu5cOGC4H04OTkBgPy9EFUVCS+qTs+k0sNcrays8ORJfhRsbGyM+vXr49q1a3jvvfcA5N950tUtOtSJiKq3uIRnmLwrGBlaD9GqpRQ6/DMvVlpqGkziVJzKtAI91y50gZpiC0A9PZNjxxrhwoWSm5DSeiZPndKBTPYqT+vWElhYyErYo/qwsLDAqVOnsHDhQhw+fBgymQz79+9H3bp1MXjwYMyfPx8NGjRQ+nidO3eWX1grIhKJEBgYWOIIIbFYjG+//bbE8zg5OZU6VLBfv37yNSTLWhYSen2WXUXmzJmD48ePY+HChWjevDmMjIwwYcIE5OTkqHwuR0dHODo6Yty4cbh48SJ8fHxw+PBhfPTRRyqfRyqVQiQS4eTJk0WuAQuGSru4uCA6OhonT57E6dOn4e/vj9atW+PHH38sdujo+++/L5+I591338XJkyfx/fffA8gPeAcOHIiuXbti8+bNMDMzQ3JyMnx8fIotZ8F5Xn8OMy9POApCKpWiR48eWLRoUZH9C89OTKRJedI8PH3xVP5aBBHMjTT3TLrSwaSHhwdOnjwpv5Pat29frF+/Hjo6OpBKpdi0aRN69uxZYQUlosqXlJIJ9019kF3vbwDAlVsaLlB1UPYb4+r3eqefRDd/aRAA8fHl65nMzESpgSRQejB58qTwGN7eNWOIawFTU1OsWbMGa9aswdOnTyGVSmFqaspnr8rp9WcYq+JsriVp1aoVLl26hA8//FCedvHiRdjb28tf6+rqQiIR9tD//vvvGDp0qDyIz8rKwt27d9G8efNylafgvJmZmWU6T5s2bSCTyZCQkABPT89iz1O3bl35TYgPPvgA3bt3x507d2BnZ6cwv7GxMQYOHIidO3fi2bNnMDU1xbvvvgsgf1bK5ORkzJkzRz5i7vDhwyW+z4Lhwk+ePJH//9q1a4I8bdu2xcGDB9GkSRN2jlCVlvgiETK8ujHS0LAhdLU195lVOpicOHEiIiIi5F/c8+fPR1xcnHyIzdtvv42lS5dWWEGJqPK9u3aWPJCkau55M0CWH12Wt2fy9u2i+69Z8wKxsdpYv/7VM5SKhrnGx4vw99/akEqBEyeETVC3brlF8tcUBRewVLtNnjwZH330EVxcXODt7Y3jx49j//79gucDbWxscPr0aXTq1An6+voQi8Vo3rw5fvrpJ/Tq1Qu6urpYtmyZfC1FZU2fPh2Wlpbw9PREo0aNkJCQgP/9738wMjJC165dAUDl89jZ2WHIkCGYOHEivv76a7Rt2xbPnz/HuXPn0LRpU3nHg6WlJZydnaGrq4v9+/fDxMSkyJqmhY0YMQI7duzAvXv3MHLkSOjo5H9fWFtbQ19fH1u2bMG4ceMQExMjmJFWkWbNmsHa2hpLly7F/Pnzcf/+fSxfvlyQZ+zYsfjuu+8watQoTJ06FaampoiLi8PBgwexaNEiDtOuJh6lP8JP//6E9Jz00jNXU1Vp8h1AhWDSyclJPnYcyB8u8+OPPyIlJQXa2tr8IyOqYebs+Rn/muzQdDFIXSInyf+bkCCCRAJoa5eQvwS3bwt37NEjFyNH5mLdOmFPZGKi8PXp09oYMsQY2dlFeywNDWVKrUtZXSxbtqzUPCKRSKXnJqn6e++99/DNN99g3bp1CAwMRJMmTbBixQr4+PjI8yxatAizZs2Ck5MTrKyscO3aNXz99deYPHkyevXqBbFYDH9/f5WDya5du2LXrl3Yvn07kpOTUb9+fbi4uODgwYPynseynCcoKAj/+9//MHfuXDx+/Bj169dH+/bt0blzZwD5vZJr167FnTt3IBKJ4OzsjP3798OolKmeXV1d4ejoiBs3bmDEiBHydFNTU2zcuBELFixAcHAwnJyc8PXXX2PgwIHFHktXVxdbt27FZ599hrfffhvOzs6YO3cufH195XmsrKxw7NgxfPXVVxg4cCCys7Plz2MWrFtJVVtadhq893gXeZ6wptPksiAAIEpJSakZD6hUE1yAVT1Yj+qjqC6lUhkaLeiKLPHVV4lPWwHnZwIA9A1kmD8/C8bG/Pp4XWJCIswtKue5hcQELSxeog+pRBiYubvn4dKlQvcJE1sDj4STfvzzTxqsrMr2+1u6VB9Ll74aWjhxYjYWL87C3r26mDDh1QXiwIE52Lr1pfz2gy1NAAAgAElEQVT14MFG+O03xUNx3nknF/v3v5C/ru5/4/Xr1y92m0gkgkwmg0gkwrNnzyqxVNVPamqqfK1GRarbMNeqivVYNoo+n9X9u6sqUbUuw2+HY8RPI0rPWMMMdxqO9e+sLzFPRX4ui+2Z3LNnT5kO6OfnV+bCEFHVEPzbJWEgKdUC9u8DEtoCALIBPP4pGwsXZmmmgFVUrH7lXUQsCdeH9M+iF3+XlJzB/tYtLVhZla0nsPAw1xYt8mdmNDMTBqeJicJ8V68W3xU6fLjqE4lUZYqWBJFKpbh//z6Cg4Nx4cIFhIaGaqBkREQ106P0si8NU12JIEL/Fv01WoZig8mJEycWSSuYbez12bBeTwcYTBLVBGsuBgOvrQtukfoePvmkJebMeZW2bp0+pFJgzpwsFNzQlsmAxYv1ER6uCy+vPCxcmAUdpQfTkyr+/FO5Maru7nlwdJTg8WMtHDv2qlfwyhVtdOlStmAyNlZ4bju7/OOYmQmn+3/69FXbkJQkEgSXuroydO2aBz09oFevXPTrV7Mm31FES0sLtra2WLRoEcaNG4fPP/8cwcHBmi4WEVGNUPhZwret34a7lXsxuas/HS0ddLHpAo/GHpotR3Ebrl69KnidmpoKf39/1K9fH2PHjpXPwHX79m1s2bIFqamp2LhxY8WWlogq3IUb9/HI5KAgbdKb4zDaKwdr1ujj6dNXAUFQkD6SkkT49tv8oYz79+ti+fL8yPLmTW20aSOBn1/NnVRFU6RS4PLl0oPJqVOzMH9+/vNOW7fqFQomdQCo3hsokxXtmWzZUnHP5Ouzud64IdzHwUEqGNZa23h4eGDevHmaLgYRUY0RnxkveD241WCMdB6podLUHsVO6WdjYyP42bhxI8zNzXH48GH069dPPiFPv379EB4eDlNTU2zYsKEyy05EaiaVyjB87wxA61WPlV6qAz7p9TaMjYEFC4oOaz10SBdRUVr46ScdjB8vnFDh8GFOr14R/v1XCykpwq9vExNhIDdgQA4CA19NnNGunbAX8q+/yjb7zuPHImRmvgoSTUxkMDfPP7epqbAMT59q4dgxHWRmAn//LTyfk1PNmWynLK5cucIlQoiI1OhJ5hPBa8s6mp2YprZQegDazz//jDlz5ihcWFckEqF3794KF3oloupjyvZ9eNbgV0HaoEaToaWV/3f/wQe5MDbOxMiRxvLt2dkidO2qeDbnqKgyThdaQ507p43Dh3Vhby/Fhx/mFDsE+PlzESIidGBkJIODgwRNm8rw8iVw8aIOHBwk+OMPYb16eeVi7txsLF+ujzp1ZBg7NgdubsJgzclJAl1dGXJz83+XDx5o4elTUZEAsDSFeyXt7CQoaBZ0dYH69aV4/vxVHl9fYzg6SmBrKxwCW9ODyeLmHUhNTcWFCxcQHh4uWGuQiIjKp/AwV03PclpbKB1MymQyxMTEFLv95s2bRZ6lLM358+exbt06XL16FfHx8QgKCsKwYcMAALm5uVi0aBF+++03xMXFoW7duujcuTPmzZuHJk2ayI+RnZ2N2bNn48CBA8jKyoKnpydWrFiBxo0by/M8ePAAM2bMwNmzZ2FgYIBBgwZh0aJF0NPTk+c5d+4cZs2ahZs3b8LS0hKffvopRo8erdL7IarO4pPTsStxFmD4Kq3esy5YO3moIF+/fnl4771c/PRT6b2OUmmpWWqFjAxg+nRDhIS8+s7Jzgb8/YsOM42N1UKfPsZ48uRVQGZiIkNaWn7EVreuDM2bCwOxDh0kaNdOgt27ix82qq8PODpKBZPgREVpo3t31Z5VLPq8pPCXbGYmQ+G5Z27c0MaNG4V7Jmv2h0PRvAMFGjZsiGnTpnFZECIiNYrPEA5zZTBZOZQOJnv37o3t27fDxsYGo0ePhrFxfs9EZmYmtm3bhh07dmDw4MEqnTwzMxOOjo7w8/PDhAkTBNtevHiBq1evYsaMGXB2dkZaWhpmz56NQYMG4fz58/LFawMDA3HkyBFs3boV9evXx6xZs+Dr64vTp09DW1sbEokEvr6+qF+/Po4cOYLnz5/D398fMplMvmBtXFwchgwZgmHDhuHbb7/F77//js8++wwNGzZEv379VHpPRNXVhB3BkBkmv0rINcTOIWugo110KF7HjnlKBZMJCVrIyADq1FFnSasff38jhIcL62vXLj34++fg5k0t7Nuni+Tk/Ho+cUJHEEgCkAeSAJCeLkJUlPCru0MH5Xr52rXLEwSTV66oHkz+/bfimVwLmJnJcOtW6cep6T2ThecdAPJH8YjFYq7LTESkZll5WUjJTpG/1hZpw9TQVIMlqj2UDiaXLl2Ke/fuYe7cufjqq69gYWEBAEhISIBEIkHHjh2xZMkSlU7eo0cP9OjRA0DRu7j16tXDjz/+KEhbtWoVOnbsiJiYGDg5OSE1NRU7d+5EUFAQvLy8AACbN2+Gs7MzTp06hW7duuHkyZP4559/cO3aNVhbWwMAvvrqK0yZMgVz5syBiYkJtm/fDktLS3lw2apVK/z5559Yv349g0mqFe4lpOB07mrgtc4jD0yDZ2tbhfkLD6Esyd27WnB2rtm9UIr89Zc2goP1kJkpKhJIAvlB2d27Wujd21geSJaV8sGkBDt2CMuoihcvgLAwPUGao6Pw3IVndFXE1FQqf86yprKxsdF0EYiIao3Cz0taGFtAW4uP2lQGpYPJevXq4ciRI/j5559x/PhxPHjwAEB+QPjOO+/Ax8dH4fOU6pSeng4AEIvz1yyIiopCbm4uvL295Xmsra3RqlUrXLp0Cd26dUNkZCRatWolDyQBoFu3bsjOzkZUVBQ8PT0RGRkpOEZBnj179iA3Nxe6upxEhGouiUwC3x+mAwap8jRRVn1sHvNxsfu0bSuBnp4MOTml/83fuVP7gsmUFKBfP2OkpxdfPzKZCF9+aVDuQLJtWwkaNlQuMGvbVhj4FX7+sTShobpITX31nho0kKJbN2HPZuEZXRWp6UNciYiochV+XtLC2EJDJal9VF4Brnfv3ujdu3dFlKVEOTk5mD17Nt59913585CJiYnQ1tZGw4YNBXnNzMyQmJgoz2NmZibY3rBhQ2hrawvydO3atcgx8vLykJycDEtLxWOuY2Njy/ReyrofCbEeVSOTyfD5oR9xNnsnRPrpMDSUQiTKDyYztTMFed3yxiMrJRGxKYnFHs/U1BmPH+sL0kJDr2HfPgvs328uT/vjj+dwdHxSePcaKzY2FhERYqSn1ys17y+/FH+jqlu3Z1i06C5CQsyxb585nj7VRU6OMPjT15dg8uRYxMZmKFW27GwdAC7y10+eyJT6O5JKgaNHG2DevGaC9PfeS8CDB8JForOzGwOwKvF4jo4JiI2NLzEPUPa/8RYtWpRpv/Jo06aNyjdURSIRoqKiKqhEVN3t2rULn3/+OR49qn0LsROpqshMrnxestJUi+XE8/LyMH78eKSmphY7Q54mlOWCJTY2ViMXOjUN61E1zzJTEXhkNU7prgL+i1/Si3tU7qk9tn86BY3MjIvJkO/tt0UICRGmde9ug9hYPezf/yotLc0cLVrUjmfECj6XBw/ql55ZgS++yEKjRlJYW8vg5aUNLS07zJsHzJ2bBSALy5frY/Hi/HU8DQ1l2Lv3Jbp0KTlwe51EAmhpySCV5gc96ek6sLFpAf0SipuXB4wYYVQk8BWJZJg+3Ri2tsK/Q1vboge7dSsN8+YZ4LffdODmJsGsWXVQp07Jf7/V7W+8U6dOFT46h6oPf39/wfVKgwYN8Oabb2LhwoVo2bKlBktGVDMVnnzHylj5tpHKp8oHk3l5eRgzZgxu3LiBn376CQ0aNJBvMzc3h0QiQXJyMkxNXz1km5SUhLfeekue59KlS4JjJicnQyKRwNzcXJ4nKSlJkCcpKQk6OjpFej2JqpP4Z2lwX/MB0uqfU26HVGt4Pfyp1EASAEaOzBHMTrps2UsAQLNmwiGMUVHaePkSMDRErXHjhuLho59+mo01axRHbkZGMsyYkQ1Fo+oLYpSZM7PRpIkUt29rYdiw3CJ1XRpt7fxhqAkJr4KepCQRrK0VD02VyYCpUw0V9qD26JEHW9ui+/n45GLBAgP56xkzsmBuLsPGjS8hk716LzXNxo0bNV0EqmK6du2KzZs3AwDi4+Mxd+5cDB8+HJGRkRouGVHNw2GumlOlV0zOzc3FqFGj8PfffyM8PFw+6U8BFxcX6OrqIiIiQp726NEjxMTEwN3dHQDg5uaGmJgYwTCRiIgI6Ovrw8XFRZ7n9WMU5GnXrh2fl6RqbeimFcoHkkkOwM7fMLyPcnfzOnWSYNGil2jbVoJJk7Ixdmz+MheFA5zoaG04ONTF6dPV/0H4W7e08Pvv2qUuefLPP0Xf66BBOZgzJwvW1op3dnPLUxhIvk4kAvz8cjFnTrbKgWSBws80JiYW3wyEhurihx/0iqS3aCHB0qVZCvdxcJBi6dKXaNVKAj+/HMycmS0oP1Ftoa+vDwsLC1hYWMDFxQUTJ07ErVu38PJl/o23+fPno0OHDrC0tISzszPmzp2LrCzFf1cAcPfuXfj5+aFly5Zo1KgRPD09cfToUUEeZ2dnLF++HFOnTkWTJk3g6OiItWvXCvKkpqZi+vTpaNWqFSwsLODm5iaY8PDSpUvo1asXrKys4ODggOnTpyMtLU2NNUOkfvGZhXom67BnsrJotGcyIyMDd+7cAQBIpVI8fPgQ0dHRqF+/PqysrDBy5EhcuXIFe/bsgUgkQkJC/l0HExMTGBoaol69ehgxYgTmzZsHMzMz+dIgTk5O8mcgvb294eDggAkTJmDRokV4/vw55s6diw8//BAmJiYAgFGjRmHLli0ICAjAqFGjcOnSJezevRvBwcEaqRcidcjJleAa9hbdINEF9hwGHrm9SpNpAVn1YGQEvPuu8hcNn3ySg08+Ea6VaGMjhUgkg0z2KnJISdHCrFmGOHdOuWf7qqIfftDFp58aQiIRoVevXOzc+QLar8WM2dnAqlXW+OuvOoiJEQaTd+6koUGD/CCuU6c87NtXNEDr1KlylsowN5fi9Wl7ExOFEZ5UCsTHi2BkBGzfLiynhYUU3377Ap07S6BVwq3ICRNyMGFC0TU0a6Pc3FzcunULaWlpkCq4C9GpUycNlKp6E4tffx659GeTyyslJbX0TKVIT09HWFgYHB0dYfjfMA0jIyOsX78eVlZWiImJwfTp06Gnp4fZs2crPEZGRgbeeecdzJ49G4aGhggLC8OIESNw/vx5wdDZDRs2IDAwEFOmTMFvv/2GL774Ah07doSbmxtkMhmGDBmClJQUBAUFwc7ODrGxsfIJDv/++28MGDAAAQEBWLduHZ4/f47AwEB88skn+P7778tdD0QVhT2TmqPRYPLKlSvo06eP/PWSJUuwZMkS+Pn5ISAgAEeOHAGAIpPjBAUFYdiwYfJ9tLW1MWrUKGRlZcHT0xObNm2C9n9Xedra2ti3bx9mzJiBd999FwYGBhg8eDAWLlwoP56trS1CQkLw5ZdfYtu2bbC0tMSyZcu4LAhVaxuPnofUSPjlimQ74Jd1wO13Fe7j45MD49JHuJZIXx+wtpbhwQNhkHL9ujYSE0XVckmIhw9FmDkzP5AEgCNHdLFtmx7GjXsVMH35pQF27y56YWttLZUHkgAwcWI2wsJ0kZsrrJ9OnVRb77GsivZM5pcjNxf47DNDHDigi8xMxV2IBw5konVrzsSqDJlMhoULF2LLli3IzMwsNt+zZ88qsVRUmY4fPy6fMDAzMxPW1tYIee1B888//1z+/6ZNm2L69OlYt25dscGks7MznJ2d5a9nzJiBo0eP4tChQ5g5c6Y83dvbG+PHjwcAfPzxx9i8eTNOnz4NNzc3nDp1CpGRkfj999/RqlUrAPnXQAU9omvXrkX//v0xefJk+fFWrFgBT09PJCUlFZnQkKiwpy+e4tMTn+LCowuQysrXXkglUmgdV24QZXpOuuA1J+CpPEoHk5MmTcKoUaPQoUMHhdsvX76Mbdu2ISgoSOmTd+7cGSkpKcVuL2lbAX19fSxfvly+RqQiTZo0wb59+0o8zttvv40zZ86Uej6i6uL7K2GAyWsJUSOBH3cUm9/WVoJZs7KL3a6Kbt1ysWNH0WcDz53TwYABuWo5R2WaN88AL18KA6yZMw1Rv74M77+fi/BwXWzdqvhZyMLrMLZtK8WmTS8xdqyhvPfWxEQGV9fK6pkUBpNJSfkN9aZNevj++6I9pgVcXPIYSKpg9erVWLVqFUaOHAkPDw98/PHH+Oqrr1CvXj18++230NHRwYIFCzRdTKpAHh4eWLNmDYD865ng4GAMGDAAx48fh7W1NQ4dOoSNGzfizp07yMzMhEQigURS/PdAZmYmli1bhmPHjuHJkyfIy8tDVlYWnJycBPkKv7a0tJTPCxEdHQ1LS0t5IFnY1atXcefOHRw8eFCeJpPlf2fcvXuXwSSVas7ZOfj535/Vd8Ay3mflBDyVR+lnJnfv3o27d+8Wu/3evXtVaqZVotosKSUT/+odFKS1zB6CadOycPNmGg4dysD48dno3j0X3bs/w/btLxAZmVHm5/AK+/LLbPj5FR3mePZs9XtuMjJSGwcOKA6yxo41wptv1sG4ccXPLlQ4mASAgQNzERT0EnXqyGBkJMOyZf9n787ja7j6B45/bvaVIBtCgkTEVoJEqH0NpQ0JxVNtVe0PVUuborT1Q1DLU6S0sXShRe2UKkF4ROxbSWIXIRvZZL93fn/kyY2RxQ03uYmc9+vl9eqcOTNz7jS5me+cc74nvdiMqtqUO8w1X17P5G+/FR1IAhXyJYAu/fLLL/Tv359ly5bRvXt3AN544w3ef/99Dh8+jFKp5PhxDeczCxWSmZkZ9evXp379+ri7u/Pdd9+RkpLC+vXrOX36NCNGjKBr16789ttvHDt2jBkzZpCdXfTv2axZs9ixYwdffPEFe/fuJSQkhFatWpGVJf+ufT7Xg0KhUAeEL6JSqRg+fDghISHqf8ePH+fcuXOyXlFBKIxSpdRuIPmSrE2tsTazfnFFQSu0Nsz18ePHGJfV05AgCEVSqSS6L5sGVvnzfBRpNhzb4ImJUW7Po729kk6dcoOcyMhbWl+CIS9758CB2fj65o+bPXas3CeQLmD9+uKDrNu3iw+Q3dwKD9CHDs3Gzy8blYoyCyShYM9kbKyCyEg9rl4t/nO8844IJksiKiqK8ePHA6D3vwmmmZm5v3/GxsYMHjyY1atXM2PGDJ21saJ6dg5jRkYGJiYmxdQuPxQKBXp6eqSnpxMaGkrNmjVlQ13v379f7PGhoaG8++676ik4GRkZ3L59mwYNGmjchubNm/Po0SPCw8ML7Z184403uHbtGvXr1y/kaEEo3sXYiyRn6TZZUxWjKgR0DkBPUa5zjL5Win2yO3HihOzN6e7du9UJc56VmJjItm3baNq0qfZbKAhCiYz7cSN3rTbKyloq3sPEqOwDubZtczAwkMjJye39unlTn+hoBbVqVYx5k+npsHv3q2V0dnMretiaLpJFFwwm9dixo/iGtG6dQ926FeP/WXlhZWWlnodWpUoVjIyMZFnFjY2NxXzJ11xmZqY6cWBiYiI//PADqamp9O7dm9TUVB4+fMjmzZvx8PDg0KFD/PHHH8Wer0GDBuzZs4c+ffpgaGhIQECA+gWFpjp16kTr1q0ZPnw48+bNo0GDBty+fZvExER8fHyYNGkSPXr0YPLkyXzwwQdYWloSERHB/v37WbZs2UvfC6FyOHZfPl2sb4O+rOyh+fS35928dZMG9TV/WQJgaWSJvl7FGwVVkRX7dBkSEkJAQACQ+0Zt9+7d7N69u9C6bm5u6rqCIOhGzJNUfk/4Ep4ZdWmc1ITfJ03RSXssLMDdXUlYWP5XTXCwAcOGVYxerr/+MiAlJX+upK2tikuXUlixwpi5c1/cG+LgoKJRo/I1z9DGRt6euDgF27fLg8mFC9M5eNCAgwcNMTWV+OqropcrEArn5ubG5cuXgdyeSXd3d4KCgujZsycqlYr169drfUSAUL4cOXJE3ftnaWmJi4sL69evp0OHDgBMnDgRf39/MjIy6NKlC1988QVTphT9Xf1///d//Pvf/6ZPnz5YWVkxduzYEgeTenp6bNmyhS+//JJRo0aRmpqKk5MTn376KQBNmzZl3759zJ07l7feegulUomTkxN9+/Z9ybsgVCbPB5Nd63bFysTqpc9XxbDKKx0vlA1FYmJika+b09PTSU9PR5IknJ2dWbp0Kf3795efQKHA1NS0wgwz0bXIyEjxAKEF4j4W7t3lgeyX/PMLsszZ3P0oPd2dizymtO/l3LnGLF6c//3g5qbk+PFU2bIa5dWwYWbs3ZsfaI0Zk8mCBRlIEjRrZklUlHwYjUIhERKSyoIFJiQnK/jiiwzati2bxDqaio1V0LBhlSL36+lJRESkUK2aRESEHnZ2kiwbbVmp6L/jv/76K0FBQezbtw8TExNOnjyJj4+Pen6boaEhGzdupFu3blq5XrNmzQodJtmzZ082b97M/PnzC7zwtbW1JSIiQr0tSRILFixgw4YNJCYm0qpVKxYvXoybm5u6TmJiItOnT1evb9i7d28WLlyIlVX+A9/Vq1eZNm0a586do1q1anzwwQdMnz4dxUssNJqUlETVqkUv/1GRhrmWZ+I+vpzCfj7L+rsrLi2OP8L/IDo1usyuWZQfLv5Aek66evv08NO4VH/5e1HR/w6UJ6V5L4vtmTQ1NVWvh3Tx4kWsra0xMzMrlYYIgvBqImLvyQNJoC3/LjaQLAt+ftl8+62xOnPptWv6bN9uiK9v+e6dTE6GgwflX5GDBuW2WaGAzp1z+OUX+XxKZ+d0mjZV8csvaWXWzpKqUUNCT09CpSr8wb5jxxysrXODx6LmewovNmzYMPUSVgBeXl6Ehoby559/oq+vT7du3Uo01+1FgoODZZlAHz16ROfOnXnnnXfUZS4uLuzZs0e9rf/cG53ly5ezcuVKVq5ciYuLCwsXLsTHx4fTp09jaWkJwMiRI4mKimLr1q1Abu/a6NGj1RnTk5OT8fHxoV27dhw+fJjIyEjGjx+PmZmZbLkJQRBeXbYym4HbB3Ip7pKum1JALYtaOFfT7fOHUDY0nkR19uxZfHx8ityfk5PDggULilwfSRCEF7sZ/YTBP37NTePtYKR5750kSSil53rAssz5btgo7TeyhFxdVfj5ZbN5c37gtWCBMQMGZBe78L2uhYUZkJWVH3DVq6ekZcv8e1xYMOnungKU77f7+vpgbS2ps7g+b+jQ8h3kV2ROTk6MHTu2VM5tbS3PXPjzzz9jaWkp+7ttYGCAnV3hC3lLkkRgYCCffPKJOsFLYGAgLi4ubN26lQ8//JDw8HD+/vtv9u/fj4eHBwBLly7F29tb/dZ7y5YtpKenExgYiKmpKY0bNyYiIoJVq1YxYcKEl+qdFAShcCFRIeUykAR40+FN8fteSWj8KDdixAhGjhxZ6NqPV69epUuXLixdulSrjROEyuS3YxfxWNuBG1XWIRknIilyyFFp9q9AIAm0VI7CpXZ1HXySgj77LBN9/fyhkjdu6HP+fPke5xoaKm9fp045PPt3sVOngotf5QaT5Z+NTeHDVqtWlejXTwST2tCiRQu++eYbrly5UubXliSJn3/+mcGDB6tHFwHcuXOHRo0a0bx5c0aMGMGdO3fU++7evUtMTAxdu3ZVl5mamtKuXTtOnToFQFhYGBYWFnh6eqrrtG3bFnNzc1kdLy8v2XW7devGw4cPuXv3bml9ZEGolP66/Zeum1AoAz0D/t1KjESoLDTumQwICOCrr77ixIkT/Oc//6FHjx5IksSSJUtYuHAhdnZ27NixozTbKgivrYysHMaFvIvK4qFWzqcf05ofJ3yilXNpQ4MGKry9c9izJ3/+4fHj+rRqpdv5hGlpcP68Ps2aKany3DTC0FD51+Pzcx9tbCQ8PXM4dSq3noWFRKtWKUD5X9Tb1lZV6FIggwZlYVr0kplCCTg6OrJ8+XKWLl1Kw4YNGTBgAAMGDMDZufSHfQUHB3P37l2GDx+uLmvdujWrVq3CxcWF+Ph4Fi1aRM+ePQkNDaV69erqrKPPL0pvY2PDw4e530uxsbHUqFFD1tugUCiwtrYmNjZWXadWrVoFzpG3z8nJqch2R0ZGFigzMTF54bJjeVlzhVcj7mPJJScnq3/2n1XYz3Jp2Be5T7btU9cHBzOHMrl2UQz1DGln0w6TRBMiE1/9PpTVvawMXvZevmiupcbB5KhRo+jWrRtjx45l8ODBvPvuu0RERHD27Fnee+895s2bh4WFxUs1UhAqu4MXIlGZayGQjG8Ip8ezavS7NKhdvtZ97dxZHkyGhBgwaVJWMUeUrqQk6NLFglu39KlSRWLt2jS6d8/tbczOhrNn5cFW27YFeyKXLEln/HhT4uP1mD07g6pVy1eynaIU1TP5r3/p7v/H62bnzp3Ex8ezY8cOtm/fTkBAAAsWLKBp06b4+vri4+NDnTp1SuXaGzZswN3dXbbIfI8ePWR1WrduTYsWLdi4cSMTJkwolXaUVGEPLElJScUmhhGJY7RD3MeXU6VKlQK/x2WVNOZW4i3uPb2n3jbUM2R53+VUMS46wVpFIxLwaI/OEvA8r0GDBuzbtw9vb282bdqEQqHg66+/FpPqBeEVhd4MlxfkGEHAEwz0DLh3N7nI9QgzMqBtW0sePPjfiHWVIT4+WQwekF74ATr05pvyYOzkSQOys3Wz1iJAUJAxt27lBozJyQp8fc2xtVXRuXMOAwZkk56e3/tib6/C0bFgANakiYojR54iSblJeSrKC1R7+4KfZejQLN54QyTc0SZra2tGjhzJyJEjefToEdu2bWP79u3Mnj2bOXPm0KZNGw4cOKDVa8bFxbFv3z4WL15cbD0LCwsaNWqkXjs6by5lXFyc7OE4Li4OW3iij0sAACAASURBVFtbIDf7a0JCApIkqXsnJUkiPj5eVicuLq5Am/L2CYLwaiRJYtX5Vcw4NkNW7lXb67UKJIWKo0TpL+7du4ePjw9nzpyhf//+1KpVi3nz5rFixYrSap8gVApXYiLkBafHQ7YZOZlG3L9rjKG+YaH/1q8158F9Y1AZgsoQI6Pyuyagq6sKW9v8YOXpU4VO503u2lXwXVpsrB6bNxvx7rvmsnJPTyXF5RGoaDkGvL3l8yInTszkP/8pfy8gXif29vaMGzeOAwcOsHz5ciwsLDh9+rTWr7Nx40aMjY0ZOHBgsfUyMjKIjIxUB5GOjo7Y2dkRHBwsq3Py5En1HEkPDw9SU1MJCwtT1wkLC+Pp06eyOidPnpQNmQwODqZmzZo4Ojq+1GeSpLJfmkYQXkRXP5fLzywvEEgC9HDqUUhtQSh9GvdMbtiwgVmzZmFkZMRPP/1Ev379SEpKYtq0acyaNYu9e/cSGBhY7HwIQRAKdyc1Ap4dJR6Xv67b9et6uLoW7DE6e1afb76RD0v66KMs6tYtnw9eCkVu7+S2bfkZUENCDPDwKPuhoXfvKrhwQfOBGZ6eBYe4VmReXkq2b3/KkSMGdO+eTYcOFWN4bkV24sQJtm/fzq5du4iPj6dq1ar861//0uo1JEnip59+YsCAAQWmncycOZPevXvj4OCgnjOZlpbGkCFDgNy5j2PHjmXJkiW4uLjg7OzM4sWLMTc3x9fXFwBXV1e6d+/O5MmTWbZsGQCTJ0+mV69e6uFTvr6+BAQEMG7cOKZOncqNGzdYtmzZS68zaW5uTmJiIlZWViIzpFBuSJJEYmKieskcbfnvg/+yPWK7bK3GZ+WocthyfUuh+3rW66nVtgiCpjR+mvrkk0/w9vZm+fLl6sn0VatWZc2aNfTr149PP/2UDh06FLposiAIxYuTrssL4vODyWvX9Hn7bXkwEx+v4L33zGRLV1haSkyZklmq7XxVHToo2bYtf/v4cX2mTCn7duzeXbKxte3avV7BJECXLjl06fL6fa7y5PTp02zbto2dO3fy6NEjLCws6N27NwMHDqRbt24YGJRopskLhYSEcPPmTdasWVNgX3R0NCNHjiQhIQFra2tat27NwYMHqVu3rrrOpEmTSE9PZ9q0aSQmJtKqVSu2bdsme2D+8ccfmT59urrn09vbm4ULF6r3V61ale3btzN16lS6dOmClZUV48ePf+l5mQYGBlhaWpKcnFzo/uTkZKo8nz1LKDFxH0vO0tJSq7/D4Y/D6be1X6HZ2V9kgvsEXKu7aq0tglASGv8WrFy5kqFDhxa6r1+/fnh5eTFFF0+FglDBKVVK0kyfm2z3XM/k8wICjImOlpcvXZquXmy+vPLykgcv4eG6Geb6fDA5dmwmrq5KPvnErEDdbt2yxVxCocSaNm1KdHQ0JiYm9OjRgwEDBtCrV69STXLSsWPHQpfvAli7du0Lj1coFPj7++Pv719kHSsrq0KD1Wc1adKEP//884XX05SBgQFVq1YtdF9sbGypJTKqTMR91L0Dtw6UOJD8vO3nvN/0fWpa1CylVgnCi2kcTBYVSOaxtrZmw4YNr9wgQahsrkbfBYNnehSf2kB6DfXm9evygOvOHQXr1xvJyiZMyMTXt/yvD1i/vgo9PQmVKrdHNTpaj7Q0MCsYw5Wa+HiFejmPPOPGZVKnjkRUVAaLF+c/7DdrpiQoKK3CzYkUdK9JkyZ8+eWX9OnTR2Q6FwThhW4l3ipR/S+8vmC65/RSao0gaK5E/fOPHz9m1apVhISEEBcXx/fff4+HhwePHz/mhx9+4J133sHVVXSzC0JJHL8u75U0TGzMs2HhjRt6REbq8fvvhuzda8i1a/Lg0tFRxZdfls+kO88zMoI6dSTu3s2Pzu7c0aNx47Lr+Tt6VP6198YbSurUye3RnTYtkzt39Nizx5BOnXJYsSIdK6sya5rwGvn999913QRBECqQ20m3ZdtjWoyhsXXjQuu61XCjTc02ZdEsQXghjYPJu3fv4u3tzePHj2ncuDF37twhPT13gnD16tXZtm0bcXFxL0xHLghCrvjkNDYfv8DsS5PhmZFv9gauKGup1MNYc3IUtGlT9CR/f/8MjIyK3F3u1Kun5O7d/CG6t2+XbTAZHCz/2uvaNT90NzaGH39MR5LSRW+kIAiCUGaeDybfb/Y+bjXciqgtCOWHxkuDzJ49G0mSCA0NZcuWLQVSIvfp04djx45pvYGC8DqatHYzzoGufBHRh2yTaNk+J3NXOnXSLDFKo0ZK/PzK//DWZ9WvLw8cb90q0QpFr0SS4MgReTDZuXPBey0CSUEQBKGsZCmziEqJkpU5VXXSTWMEoYQ0foo7cuQIH3/8MU5OToWm53Z0dCQ6OrqQIwVBeNb9uCQ2xH8CximF7m9i2xB//wxsbIrvrXN2VrJ2bRr6uluq8aXUqyf/XLdv538NnTunz8qVRoUmHdKGmzf1iIrKP7eJiYSnp1gWQxAEQdCde8n3UEn5fxtrmtfE1MBUhy0SBM1p/MSWmZmJVTGTh5KSktDTK7seBkGoqL7bHwxGaYXvTK5Nz0ZtqVtX4vff0zAzyx8BUKWKxKJF6YSHJxMdncSZM6llOjxUW4rqmTx2TJ+ePc2ZMcOU7t0tiIjQ/vfJ80NcvbxyKMXkmoIgCILwQrcT5UNc61nV01FLBKHkNH5ac3Nz48SJE0Xu37t3L82bN9dKowThdbbv5t6Chf/9FA5/g/PREDq1z122wt1dyZ49T+nXL5sPPsjk+PEUPv44Czs7qUyzn2pbwWBSH0mCGTNMycnJHfWQmqrg22+NtX7tkBB5MCnWWRQEQRB07VaSPJNrvaoimBQqDo0T8IwdO5bRo0fj5uaGj48PACqVioiICBYuXMiZM2f49ddfS62hgvA6SE7LJMp0v6xsSZO/adDSk/v39XjnnWzZsFV3dyU//1xEL2YF5eQkDyajohRs327I5cvy8bq//27EV19lYG+vvbUzr16Vvz9r314McRXKTmZmJrt37yYpKYlevXrh4OCg6yYJglAOPN8zWd+qvo5aIgglp3Ew6efnR1RUFPPmzWPevHkADBw4EAA9PT2++uorvL29S6eVgvCaWPPXSdlcSb00O4Z3dcdAXwlUjsDG1BRqPZOtVqVSMHZs4XNDfvzRiJkzMwvdV1KZmfL5mQCurpXjngtlb9q0aYSFhXH06FEAlEol3t7eXLhwAUmSmDNnDvv376dJkyY6bqkgCLr2fCZX0TMpVCQlWmdy8uTJ+Pn5sWvXLm7duoVKpaJevXr069cPJyenEl/8xIkTfPfdd1y8eJGHDx+ycuVKhg0bpt6/a9cu1q9fz8WLF0lISGD37t106NBBdo7MzExmzpzJH3/8QUZGBh07duTbb7+ldu3a6jr3799n6tSphISEYGJigq+vL3PnzsXomfUUjh8/zowZM7h+/Tr29vZMmjSJESNGlPgzCUJxfru0C6rkbzfI6YuBfuWba1yvXn4wCZCZWXj61HXrjPj880wMSvRNVbgbN/RQqfKv4+CgQqwlL5SWv//+Wz2KB2D79u2cP3+eb7/9lubNmzNy5EgWLVrE+vXrdddIQajEspRZrL6wmivxV7R63pTkFCxvFb2cV2HCHobJtsWcSaEiKfEjmoODA+PGjdPKxZ8+fUrjxo0ZMmQIY8aMKbA/LS0NDw8PBg0aVOh+AH9/f/bt20dQUBDVqlVjxowZDB48mKNHj6Kvr49SqWTw4MFUq1aNffv28eTJE8aOHYskSSxatAiAO3fuMGjQIIYNG8aaNWsIDQ1lypQp1KhRg7ffflsrn1UQzt94yA2zX2RlAxr30VFrdKt+fRXFTMFWS0jQIyJCO+tQRkTIh9GKXkmhNMXExMhesu7du5emTZuqX1KOGDGC77//XketEwThy5Av+f5CKf0OPni1w0XPpFCRvNT7/tTUVBITEwusNQlQp04djc/Ts2dPevbsCVBogPruu+8CkJCQUOjxSUlJ/Pzzz6xcuZIuXboAsHr1apo1a8aRI0fo1q0bhw8f5tq1a1y+fFk9P+Wrr75i4sSJzJo1iypVqrBu3Trs7e3VwaWrqytnzpxhxYoVIpgUtGbi7yvBPEu9rZ/iyKS3OumwRbrTunUOP/9sVKDcyUlJ9eoS587lfzVdvaqvlWDy+eVGGjaseJlwhYrDyMiI9PR0ACRJ4tixY7z33nvq/VZWVjx+/FhXzROESu/A7QO6bkKhbMxssDIpevUEQShvNB5fl5GRwVdffYWzszN169alefPmvPHGGwX+laULFy6QnZ1N165d1WUODg64urpy6tQpAMLCwnB1dZUlOujWrRuZmZlcuHBBXefZc+TVOX/+PNnZFWtBeKF8+uduHJeNfpSV+dh8ipmJoY5apFuDBmUzcGCWrKxKldzlUDp0kPcY/vOPHoW8tyqx55caadRI9EwKpadx48Zs3ryZxMREfv75Z548eUKPHj3U++/du4e1tbUOWygIlZckSUSnls+10Ue3GK3rJghCiWjcMzllyhQ2bdpE37598fLyKnbNybISGxuLvr4+NWrUkJXb2NgQGxurrmNjYyPbX6NGDfT19WV1OnfuXOAcOTk5JCQkYG9vX3ofQqgUZm3bBIYZ6m291NosGfWuDlukW6amEBSUzsSJmfz2mxHJyQo++SQTFxcVjRvLg7ylS00ICjLGzy+LRYsyeNnlbMPD5cNcRc+kUJo+++wzBg8eTP36uVkZ27ZtS/v27dX7Dxw4gLu7u66aJwiV2uOMx2Qq85O7mRuas7jLYq2cOyYmBjs7u5c61rmaM21qttFKOwShrGgcTO7evZvhw4ezbNmy0mxPhRIZGVmmxwlyFeU+qlQSR5N/hqr5ZW/qjyDmwT1idNcsGV3dSzMzeDbPVWQkWFqaAvIMl8nJCoKCjKlVK5r+/Qsf9l6cnByIjJQ/uBsYRBIZqf3eyYryc1kRvOy9dHFx0XJLSq5Tp04cPXqU4OBgqlSpwoABA9T7njx5wptvvknfvn112EJBqLwepMgnNTpYOjCk8RCtnDvSMLJcfAcJQlnROJhUKBRlPoz1RWxtbVEqlSQkJMiGC8XFxeHl5aWukzfkNU9CQgJKpRJbW1t1nbi4OFmduLg4DAwMCvR6PutlviwiI8WXjDZUpPu49u/T5FR95qFYaci3wz/EpXZ13TXqGeXtXjo6goGBRE5OwQyv33/vyOTJ1UvcO3nzph7Z2fkH2dioaNNG++t4lbd7WZG9DvfS1dUVV1fXAuXVqlVj/vz5OmiRIAhAgSGutSxq6aglglDxafxI1qdPH44cOVKKTSm5Fi1aYGhoSHBwsLrswYMHhIeH4+npCYCHhwfh4eE8eJD/Fio4OBhjY2NatGihrvPsOfLqtGzZEkPDyjmnTdCeVf/dJNu2T+5bbgLJ8sjICFxcCh+CGhenx8GDJcsblpYGc+aYyMrEEFehrBw5coRvvvmGiRMnEhERAeQmsTtx4gSJiYk6bp0gVE4imBQE7dE4mJwyZQq3b99m4sSJnDlzhkePHhEXF1fgX0mkpqZy6dIlLl26hEqlIioqikuXLnH//n0gdyjQpUuX+OeffwC4ffs2ly5dIiYmd3Bg1apVee+995g9ezZHjhzh4sWLjB49miZNmqjnQHbt2hU3NzfGjBnDxYsXOXLkCF9++SXDhw+nSpXcBf8+/PBDHj58yOeff054eDg//fQTGzduZMKECSX6PILwvNuPnnDDZLOs7L1mw4qoLeRp0qTo4afffWdconONGmXG7t3yl0JNm4rkO0LpSk9PZ+DAgQwYMIClS5fyyy+/8PDhQyA30+v777/P6tWrddxKQaicCgSTliKYFISXpfEr/jZtcicEX758mV9++aXIeiVJdX7+/Hn69eun3p4/fz7z589nyJAhBAYGsm/fPsaPH6/eP3HiRCA3sYG/v7/6GH19fT788EMyMjLo2LEj33//Pfr6uck29PX1+f3335k6dSq9e/fGxMQEPz8/vvnmG/V5nZyc2Lx5M1988QVr167F3t6egIAAsSyI8Mr+/XMQGKWqt/We2jOlfxcdtqhicHYuuufw+HEDEhIU1Kjx4hSvKSmwd6/8a87aWsXo0VlFHCEI2vHNN99w/Phx1qxZg5eXF02bNlXvMzIy4p133mH//v189tlnOmylIFROz8+ZrG1RW0ctEYSKT+Ngcvr06SgUBecwvYoOHToUO8xn2LBhDBtWfC+OsbExixYtUq8RWZg6derw+++/F3ueN998k2PHjhXfYEEogQfxyZxQroRnOsW6mo/HxOillnetVFq2LL7n8MoVPTp1enHv4t27ekhS/veWtbWKQ4dScXTUwlojglCMHTt2MHLkSHx9fQt9yeri4sIff/yhg5YJgvAgVR5MimGugvDyNH6qzesJFARBM5/++guSyRP1tiKjGstHvq/DFlUcnTvnULeuinv3Ch+Jf+mSvsbB5LOaNVOKQFIoEwkJCYUm38mjUCjIyMgocr8gCKVHzJkUBO0RXSSCUEr++3gvVMvfbq8/ntrWVXTXoArE2BgOHEhl82ZD6tdXERWlh7+/qXr/5cv6xRyd7/lg0tFRJN4RyoaDgwPh4eFF7g8NDVWvQSkIQtmRJInoFHkwWdtSDHMVhJf1kst/C4JQnNT0LFIsz8jKvuz/ro5aUzHVrCkxaVIW/frl0Ly5vBfy0qWXDSZFr6RQNvz8/NiwYQMnT55Ul+VNFQkKCmLHjh0MGaKdde0EQdBcUmYSaTlp6m1TA1OsjK102CJBqNhEMCkIpWDbyctgkKne1k91wMPVQYctqtiez74aEaFHevqLj3t+mKzomRTKyqeffoqXlxdvvfUW3t7eKBQKPv/8cxo1asTUqVPp1asX48aN09r15s+fj5WVlexfw4YN1fslSWL+/Pk0atQIe3t7+vbty7Vr12TnSExMZNSoUdStW5e6desyatSoAnkNrl69Sp8+fbC3t8fNzY2AgAAkSf6SZufOnXh6emJra4unpye7d+/W2ucUhFdV2HxJbecEEYTKRAxzFYRS8OeV07Jt+5y2OmrJ66FqVXByUnLnTm6PpEql4J9/9GnVqvh5k8/3TNatK4JJoWwYGRmxZcsWtmzZwo4dO1AoFOTk5PDGG2/g4+PD4MGDtf4A6+Liwp49e9TbeVnNAZYvX87KlStZuXIlLi4uLFy4EB8fH06fPo2lpSUAI0eOJCoqiq1btwK5GdRHjx6tTmCXnJyMj48P7dq14/Dhw0RGRjJ+/HjMzMz497//DUBYWBgjRozA39+ffv36sXv3bj744AMOHDhA69attfp5BaEwj9Mfc/DOQW4l3kKi4GiUu8l3ZdtivqQgvBoRTApCKbiQcEo2X9LdWgSTr6p5c5U6mATo1s2C6dMz+PzzTPQKGWMhSaJnUtA9Pz8//Pz8yuRaBgYG2NnZFSiXJInAwEA++eQT9ZJXgYGBuLi4sHXrVj788EPCw8P5+++/2b9/Px4eHgAsXboUb29vIiMjcXFxYcuWLaSnpxMYGIipqSmNGzcmIiKCVatWMWHCBBQKBYGBgXTo0IGpU6cC4OrqSkhICIGBgQQFBZXJfRDKv7DoMCYcnMDNxJtaPa+EhEoq2fe8WGNSEF6NGOYqCFqmUkk8MjssK+vbvI2OWvP6aNasYC/kwoUmfP21caH1Hz9WkJqa3/NjZiZhbS3mTAqvrzt37tCoUSOaN2/OiBEjuHPnDgB3794lJiaGrl27quuamprSrl07Tp06BeT2KFpYWODp6amu07ZtW8zNzWV1vLy8MDXNT4bVrVs3Hj58yN27ub09p0+fll0nr07eOQQB4NPDnxLxJAKlpNTqv5IGkgB1LOuUwicUhMqjxD2TKSkp3L9/n8TExALzJADat2+vlYYJBYWF3+dO8m3s7MUD8YP4B0SbRL+44kvIzMki+EwMT9JSsKuVydErt4hKvUcNayXW1i/+Q3UtPgLJOCm/INuU/h6NS6WtlUmLFoUPaV22zIT69VUMH54tKy8sk6uYFiOUlvHjx5f4GIVCwYoVK7Ry/datW7Nq1SpcXFyIj49n0aJF9OzZk9DQUGJiYgCwsbGRHWNjY8PDhw8BiI2NpUaNGrKhtwqFAmtra2JjY9V1atWqVeAcefucnJyIiYkp9Dp55xCEbGU2V+Ov6roZABjpG9Hfub+umyEIFZrGweTjx4+ZNm0au3btQqks+FAnSRIKhaLQxZmFV/fxmvVsSftE180oX8riRfcTQAFYQnwmhD940QEFVUlpg5mJobZbVul06ZJD06ZKrlwpmMl15kxT/PyyychQUK1a7suWe/fkkaOYLymUpmPHjpV4DqQ250z26NFDtt26dWtatGjBxo0badOmYoyMiIyMLNPjBLmyuo8P0x4WOpdRWxQoqG9ZHw9rDywNLIusZ6hnSDvbdpgmmRKZpN3PLn4mtUfcS+152Xvp4uJS7H6Ng8mJEyeyf/9+Ro8ejZeXF1ZWIo1yWfojdiFY6LoVwstoYTRA1014LRgYwMGDqRw/bsA//+gxe3b+ULvkZAU1a1YFYMCALH78MV0k3xHK1OXLl3XdBBkLCwsaNWrErVu3eOuttwCIi4ujTp38IX1xcXHY2toCYGtrS0JCgvrFMOS+JI6Pj5fViYuLk10nbzuvjp2dXaF18vYX50UPLIXJm88pvJqyvI/xD+Jl2y1sW/D3u39r7fwKFOjrabZ8VGkQP5PaI+6l9pTmvdQ4mAwODmbcuHF8/fXXpdIQoWh3YxJRWZTOkE6hFD2pB6fHMnDIh1CKb2ErE1NT6NEjhx49ICpKjx9+KDhfcts2I/r2zSl0mKsgVBYZGRlERkbSoUMHHB0dsbOzIzg4GHd3d/X+kydPqv+me3h4kJqaSlhYmHreZFhYGE+fPlVve3h4MGfOHDIyMjAxMQFynw1q1qyJo6MjAG3atCE4OJiJEyeq2xIcHCybiylUbg9S5EN8HCwdMNAT+SAFoaLS+LfX1NSUunXrlmZbhCKcDJensSbTkqbVW6qH81VG6enpsiQQ2nLjhh4Pow0gpSakWQMKSKkFsU0hx6TYYw0MJJYsSWfhQmOirteC+EaAgiZfpwLFL2EhlNzbb2cXGkwCrFtnVGB+pAgmhdfZzJkz6d27Nw4ODuo5k2lpaQwZMgSFQsHYsWNZsmQJLi4uODs7s3jxYszNzfH19QVys652796dyZMns2zZMgAmT55Mr1691G+zfX19CQgIYNy4cUydOpUbN26wbNkypk+fru7NHDNmDH369GHp0qX07duXPXv2EBISwv79+3VzY4Ry5/l1Hmtb1tZRSwRB0AaNg8lBgwaxZ88eRo4cWZrtEQpx7u5tecGdzoyq+RvDfbMLP6ASKI3uekmCZs0sISq/R+v//i+d4cOzUCrh99+NOH9eH5UK9b+//jJUZwzNAXYuz+bhSQNQ5kcyrq4ikCwNXl65CZHi4wsmpT5+vOBXW5MmIpgUytahQ4dYsWIFFy5cIDk5udCkddrKMxAdHc3IkSNJSEjA2tqa1q1bc/DgQfVL4EmTJpGens60adNITEykVatWbNu2Tb3GJMCPP/7I9OnTGThwIADe3t4sXLhQvb9q1aps376dqVOn0qVLF6ysrBg/fjwTJkxQ1/H09GTt2rXMnTuXefPmUa9ePdauXSvWmBTUolKiZNsOlg46aokgCNpQZDB59uxZ2fZbb73F8ePHGTBgAP/6179wcHCQLYicp1WrVtpvZSUXHnsbnr3Vj5158ECs6qINOTmwZYshjx8rMDLKHTqZx9RUYsSILPI6QEePzipw/NatOYwcaabePnRInmjHwUGFZdHz/4VXoK8P/fpls25d4b2Tz6pSRaJePRFMCmVn7969vPfeezRq1IiBAwcSFBSEn58fkiSxd+9eXFxc8Pb21tr11q5dW+x+hUKBv78//v7+RdaxsrJizZo1xZ6nSZMm/Pnnn8XWefvtt9XrWQrC854f5lrbQvRMCkJFVmQw2b179wKZ5vLeqh45cqRAfZHNtfTcS70FVZ8peOxMdLQIJrVhxgwTVq8uPBjp1CmHF42kHTgwm+XLlVy+XPhkf9ErWboGDdIsmGzeXCmWBRHK1JIlS2jRogV//fUXSUlJBAUFMWzYMDp16sSdO3fo3r07DRo00HUzBaHMPd8zKYa5CkLFVmQwuXLlyrJsh1CMeNVNecFjZx48EE/Gmjh82ICAAGOsrCTGjcukU6f84C4yUo8ffjAq8th33nnxMGKFAj76KJNPPjErdH+jRqI3rDR5eSnx989gzRoj6tVTceZM4V9pb7whgnqhbP3zzz/MmjULAwMD9SievGW1nJycGDFiBEuXLsXPz0+XzRSEMifmTArC66XIYHLo0KFl2Q6hGE+NCwaT0YaiZxIgONiAzZvz5y3mMTSUcHVVsXSpMRkZufsOHDCkffsc/PyysLeXWLfOCJWq8KC8f/9sBg/WbE6qr282s2ZJpKQUPJfomSx9n32WyWefZQIwapQpmzcXfEHQooX4/yCULWNjY3XGU3NzcxQKhWzJjNq1a3P79u2iDheE11J6TjoJ6QnqbT2FHvbm9jpskSAIr0rjBDz9+vVj6tSpdOrUqdD9x44dY9GiRezevVtrjRPgYUIKKrOY/AKlASTV5UGOCCbPn9fHz8+MnBzNe2lPnDDgxImif+wtLCRmzsxg1KgsjYdFWljA0KFZhQ6XdXMTPZNlaeDA7EKDSdEzKZS1+vXrc+PGDQAMDQ1xdXVl165dDB48GIB9+/Zhby8eooXKJTpFvsxZLYtaYlkQQajgNI5Ijh8/TmxsbJH74+PjOXHihFYaJeT77/XnlgVJrAcqA1JSFCQn66ZN5cXy5UYlCiSL07ixkqtXkwkPT2bMmCz0Shir+/tnFNoL2bChCGLKUpcuOYWWN2gggnqhbHXv3p1t27aRnZ07EZQIOAAAIABJREFUwmHs2LHs27cPd3d33N3d+euvvxgxYoSOWykIZSsq9bn5kiL5jiBUeFp7HfTgwQPMzc21dToBCI9KYMqBL6H6M4WPndX/+eCBHlWqVM6H5Lg4Q/bsMXxxRQ3Nnp1B7dovv26nlRVs2/aUXr0s1BlhO3TIoWrVFxwoaJWREXTsmMOxY/lfbTVrqigk8bQglKpp06YxZswYDAxyfxaHDx+OiYkJO3fuRF9fn2nTpjFkyBAdt1IQXs2p6FNcjb+qcf1zMedk22K+pCBUfMUGk3v37mXfvn3q7fXr1xeayTUxMZGjR4+KZUG06HFKOh2C+pFV/Z/ndsiDyco6jHLHDmtZr6Szs5JZszIASE1V8PnnprI5jLt2pVK9usTGjUbcvJnf7WhiAj4+WfTqVXiPVknUri1x6FAqy5cbo1DAJ59kvvI5hZKbOTODnj0t1Nsff1xwSRdBKG2GhoZUr15dVjZo0CAGDRqkoxYJgnb9cPEHpgVPe6VziGBSECq+YoPJ8PBwdu7cCeSuUXX27FkuXrwoq6NQKDAzM6N9+/bMnz+/9FpaySzc8RdZVf8puCPBRf2f0dGVM6OrSgXbt9vIykaNyuLtt/MDQhcXFRMmmBIdrcf06Rl07Jg73HTevIxSbZudnVTq1xCK5+Gh5Ntv09mwwQh39xzGjhVBvaB72dnZnDlzhkePHuHi4kLTpk113SRBeCXrLq975XM4WDpooSWCIOhSscHkp59+yqeffgpAtWrV+O6770Qa8zKy9+Y+sHquMNMCwvMXgs4bTlnZ3LqlR1xcfpIVc3OJd9+V9z55eCgJDU1FoUCsL1gJffRRFh99JHokhbJ16NAhtm3bxldffYW1tbW6/MaNGwwZMoSbN/Mzc7/99tv8+OOP6mVDBKEikSSJu0l3X1yxGEb6RvRw6qGlFgmCoCsazZnMyMhg5cqV1K9fv7TbIwBpGdncN9kvK7N98hZdc+bxW1JddVl0dOUMJq9ckX/uVq2UVKlSsF5Jk+gIgiC8il9//ZXIyEhZIAkwevRobty4weDBg2nVqhUHDx5k586deHh4MHbsWB21VhBeXmJmIk+zn6q3jfSNGNZ4mMbHmxma4ePiQ30r8VwpCBWdRsGkiYkJkydPJiAgQMyLLAM/HDwJJknqbUWaDVdmbuDwIWN+W5lf78aNyhktXbkif5PftKnImCoIgu6dP3+efv36ycquXr3KuXPnGDhwIN9//z0AH3/8Md7e3mzZskUEk0KFdD/5vmzbsYojS7st1VFrBEHQJY2jkQYNGhATE/PiiiVw4sQJ3n33Xdzc3LCysuLXX3+V7Zckifnz59OoUSPs7e3p27cv165dk9VJTExk1KhR1K1bl7p16zJq1CgSExNlda5evUqfPn2wt7fHzc2NgIAAJEmeuXPnzp14enpia2uLp6enztbLlCSJXy9tkZXVz+mLkaE+TZrIg6bz5/XJrITTwS5fFsGkIAjlT2xsbIERPIcOHUKhUDB06FBZed++fdXrUApCRROVIl/iQ8x9FITKS+Ngctq0afzwww9cvap5CugXefr0KY0bN2bBggWYmpoW2L98+XJWrlxJQEAAhw8fxsbGBh8fH1JSUtR1Ro4cyaVLl9i6dStbt27l0qVLjB49Wr0/OTkZHx8fbG1tOXz4MAsWLOC7775jxYoV6jphYWGMGDECPz8/QkJC8PPz44MPPuDMmTNa+6zPu3Inlq4LvsLh6664Lm+P18/taPdzO2quqEmE+c+yuu808gagTh0JB4f87K2ZmQouXqx8822uXhXBpCAI5Y+JiQkZGfIEXKGhoSgUClq3bi0rr1atGllZYl6vUDGJYFIQhDwarzN5/PhxrK2t6dixIx4eHtSrV69AAKhQKFi8eLHGF+/Zsyc9e/YEYNy4cbJ9kiQRGBjIJ598wttv5yadCQwMxMXFha1bt/Lhhx8SHh7O33//zf79+/Hw8ABg6dKleHt7ExkZiYuLC1u2bCE9PZ3AwEBMTU1p3LgxERERrFq1igkTJqBQKAgMDKRDhw5MnToVAFdXV0JCQggMDCQoKEjjz6OpmrO7kl7tHJgAJpAqQUxCEZVT7RjTs4N609Mzh6io/OQzp07p4+FReYKpJ08UssRDBgYSrq6Vc3kUQRDKF2dnZ44cOcKYMWMASEtL48SJEzRp0oQqz03sfvToETY2NoWdRhDKPRFMCoKQR+OeybVr1xIeHo5KpSI0NJRNmzaxdu3aAv+05e7du8TExNC1a1d1mampKe3atePUqVNAbo+ihYUFnp6e6jpt27bF3NxcVsfLy0sW+Hbr1o2HDx9y925uJrLTp0/LrpNXJ+8c2qbU03DpiExLrI/+go2VmbrI01MeOJ48qfH7gNfC88l3GjZUYWyso8YIgiA8Y+TIkRw4cIAJEybwyy+/8MEHH5CSksK//vWvAnWPHj2Km5ubDlopCK/uQcoD2bYIJgWh8tI4Enny5ElptqOAvPmZz7+5tbGx4eHDh0Du/JQaNWqgeGbtB4VCgbW1NbGxseo6tWrVKnCOvH1OTk7ExMQUep28cxQlMjLyJT4ZBeZrFupBG9j+E46OtYiMjFAX16plCjRRb588CRERkZVm+YudO2sB+QvSOzomEhl5W3cNek287M+yUJC4l9rzsvfSxcXlxZVKgZ+fH6dPnyYoKEidA2Do0KGMHDlSVu/atWscP36cgIAAXTRTEF6Z6JkUBCFP5erW0rKXeWCJjIzMD34THeH0OLjVDTsbA7Zte8rvvxnyn8U2kOgEQJM+WbLr1KsHFhYSqam553jyxBA9PVecnV//oZ4bNhjy449msrJ27cx09uD4usgbEi68OnEvtaei3suFCxcybdo07t69S506dbCzsytQp0aNGhw+fBhnZ2cdtFAQXp0IJgVByFPiYPL69ev89ddf3Lt3D4C6devSs2dPGjVqpNWG5f0BjouLo06dOuryuLg4bG1tAbC1tSUhIQFJktQBmiRJxMfHy+rExcXJzp23nVfHzs6u0Dp5+7Xtn8l/k5kBzRrZo1LltjvmITiaJJF+1wQS88dtNmggH9ZqYABt2uQQHGyoLjt1Sv+1DybPnNFn0iSzAuXNm1ee+aIViVKpJLMSpho2MjIiLS1N1814LbzoXhobG6OvXz4TkNnY2BQ7H9LW1rbU/r4IQmnLVmbz8OlDWVkty1pF1BYE4XWncTApSRJTp05l3bp1SJKE3v9WhFepVMyZM4cRI0awaNEi2ZDTV+Ho6IidnR3BwcG4u7sDkJGRwcmTJ/n6668B8PDwIDU1lbCwMPW8ybCwMJ4+fare9vDwYM6cOWRkZGBiYgJAcHAwNWvWxNHREYA2bdoQHBzMxIkT1dcPDg6WzcXUJusqZlAFnJxU3LqV/zB044Yet27J5wTWq1cwSGzdWikLJs+f12fYsOxSaWt5sXOnYYGyNm1yePNNEUyWN0qlkoyMDMzMzFAoFCiVSnJycnTdrDJRngOciqa4eylJEikpKVhYWGBgIAbYCEJZevj0ISop/9nExswGU4OCGfkFQagcNP4rvHz5ctauXcvQoUOZMGGCevhRZGQkK1euZO3atdSpU4dJkyZpfPHU1FRu3boF5AalUVFRXLp0iWrVqlGnTh3Gjh3LkiVLcHFxwdnZmcWLF2Nubo6vry+Qm3W1e/fuTJ48mWXLlgEwefJkevXqpW6fr68vAQEBjBs3jqlTp3Ljxg2WLVvG9OnT1YHvmDFj6NOnD0uXLqVv377s2bOHkJAQ9u/fr/FneRkNG8qDyfBwfW7elAeT9esXDCbd3QuuN5lHkngt508eOiT/UW3bNokdO0A8t5c/mZmZmJnl9iLfvn2blJQUzeYJvwYyMjK0vh5vZfWie6lQKHjy5AktW7YskClVEISXo5JU3Hxyk7ScokcFXIm7ItsWQ1wFoXJTJCYmavSU16pVK5o2bcqGDRsK3T98+HCuXr3K2bNnNb54SEgI/fr1K1A+ZMgQAgMDkSSJBQsWsH79ehITE2nVqhWLFy+mcePG6rqJiYlMnz6dP//8EwBvb28WLlyIlZWVus7Vq1eZOnUq586dw8rKig8//JDPPvtM1ou6c+dO5s6dy507d6hXrx4zZ86kf//+Gn8WTT07D2j2bBOWLy8+FemDB0mYm8vLYmIUuLrmPzwZGUlcuJDCmDFmXLyozwcfZDFnTsZrE1RGRyto3Dj/8+rpSRw8eIFWreoXc5SgKW3PTUtLS8PMzIx79+6RlJSkHsVQGaSnpxe6Zq5Qcprcy8TERB48eICPjw+GhgVHLwivv4o6t7a8iYyMpKZjTfr/0Z9zMedKdOxbDd7il36/lFLLKh7xM6k94l5qT2neS417JqOiohg/fnyR+zt16sSBAwdKdPEOHTqQmJhY5H6FQoG/vz/+/v5F1rGysmLNmjXFXqdJkybqYLMob7/9tno9y7LSsGHxQzRr1lQVCCQB7OwkatdW8eBB7kN6VpaCsWPNOHYs93/n8uXGuLvn4OiowtFRolq1it0rdPiw/Me0VSslVaqI4a3lXVpaWqUKJIWyp6enR3p6OikpKVSvXl3XzRGECm3frX0lDiRB9EwKQmWncTBpY2PDxYsXi9x/8eJFsQBzCbm6Fp80p7D5knlatlSqg0mAo0fl/yvffz83Cq1aVWLv3lSaNq24CXqeDya7dq0c8+8qOpWq4v7MCRWHJEmVMtmTIGhbxOOIF1cqhFdtLy23RBCEikTjYNLHx4eVK1fi4ODA6NGj1XNUUlJSWL16Nb/++muxPZdCQS4uxfeuOToW/TDu7q5kz54XD+tKSlLwn/8Ys2ZNeonbVx5IEgQHy39Mu3cXwaQgaMvAgQMZOHAgQ4cO1XVTBEHQofsp92XbDpYOVDOpVmR9Y31j+jboS39n7U8JEgSh4tA4mPziiy+4cuUK8+bNIyAgQJ3WPDY2FqVSSZcuXYodjioUVLUqsuGqz2vYsLhgUvOA6vjxipvtMCFBwZMn+ffH3FzC3V3J//I2CYLWzJ07lz///JORI0fy4YcfqsvPnTvHv//9b/bu3Subi/2icyUlJbFo0aIi65w6dYpPP/2U7du3y5aJGDRoEElJSezbt0+dzTQjI4PevXszZcqUQueZa2Lv3r0sXbqUv//++6WOF4r3Mi9TFQoFK1as0Mr1lyxZwu7du7lx4wZGRka0bt2a2bNny3IMjB07lk2bNsmOa926texnIjMzk5kzZ/LHH3+QkZFBx44d+fbbb6ldu7a6zv3795k6dSohISGYmJjg6+vL3LlzMTIyUtc5fvw4M2bM4Pr169jb2zNp0iRGjBihlc8qlI4HKQ9k28u6LaO7U3cdtUYQhIpC4yjD1NSU7du3s2/fPv766y+ionIXrO3Vqxe9evWid+/epdbI19nw4VnMn29SoNzQUGLAgKwij2vRQvM5g9HResTGKrC1rXhzJ2Nj5VmEatZUiQyuQqkxMjJi48aNvPPOO1SrVvQbeW1o3rw5BgYGnD9/nl69egEQExNDbGwslpaWRERE4ObmBsClS5fIzs6mVatWL3WtyrI0iy4dO3asxEtjaWspLcgN3j766CPc3d2RJIl58+bxzjvvcOrUKdnPcufOnVm9erV6+9kAEMDf3599+/YRFBREtWrVmDFjBoMHD+bo0aPo6+ujVCoZPHgw1apVY9++fTx58oSxY8ciSZL65cmdO3cYNGgQw4YNY82aNYSGhjJlyhRq1KhR5rkJBM1FpUTJtsVcSEEQNFFsMDlq1CjatWuHp6en+qGmT58+9OnTp0waVxlMn55Js2ZKHj9WMGBANpcv63PypD69e+fg6Fh08GdllZvAJyJCs8jqzBl9+vSpeA+UcXHyhy0bm4oXEAsVh7u7O3Fxcaxfv57JkycXWe/27dusXLmSCxcuYGxsTOvWrZk4cSJmZmYEBQWpE361b98egO+++069Xm4eU1NT3NzcOHfunDqYPHfuHG5ubtjZ2an/O6+8Zs2a1KpVC5VKxYYNG9i1axdPnjyhTp06jBo1ig4dOgDw8OFDfH19mTNnDrt27eLKlSuMHz+epUuXyto0YsQIPvroIwCysrJYuHAhBw8exNzcHD8/P4YNG6at21opXL58WafX37Ztm2x79erV1K1bl9DQULy9vdXlxsbG2NnZFXqOpKQkfv75Z1auXEmXLl3U52nWrBlHjhyhW7duHD58mGvXrnH58mUcHHKDja+++oqJEycya9YsqlSpwrp167C3t1cHl66urpw5c4YVK1aIYLKcUkmqAj2TtS1rF1FbEAQhX7HB5LZt29iyZQsKhQIrKys8PDzw8vKiXbt2tGjRQqRi1wKFAlmQ5+mpxNNTs17Hdu1yNA4mz56tmMFkTIx8CHBF7F0VcrVv365Mr3fixH9LfIyenh5jxozB398fPz8/9cPys+Lj4xk/fjxvvfUWEyZMICcnhzVr1vD555+zbNkyhgwZwp07d0hOTubLL78EKHIdRHd3dw4ePKjePnfuHC1btsTe3p6jR4+qA7pz586pg9HNmzezceNGpk2bRqNGjThw4ABffPEFQUFBNGzYUH2u77//ngkTJuDv74+enh4qlYrVq1ezefNmANmyG7///jsfffQR69at4+TJkyxbtow33niDpk2blvgeCuVDamoqKpWqwNDskydP4uzsTNWqVWnfvj2zZs1SJ8+7cOEC2dnZdO3aVV3fwcEBV1dXTp06Rbdu3QgLC8PV1VX2u9GtWzcyMzO5cOECHTt2JCwsTHaOvDqbNm0iOztbPDuUQ48zH5OtylZvVzWuiqWRpQ5bJAhCRVFsMHn//n1Onz5NaGgop06d4r///S8HDhxAoVBgbGxMy5YtadeuHW3btsXDw0MsHF3G2rdXsn59wfJ169LIzoZRo8zUZadPGwAVL+Ph88NcbW1FhlChdLVr145mzZqxZs0avv766wL7t2/fjrOzM+PGjVOXzZw5E29vbyIiImjZsiXGxsYYGRlRo0aNYq/l7u7Ohg0bePToEfb29pw7d47PP/8cOzs7/vOf/5CTk0NWVhbXr19n4MCBAGzatIkhQ4bQs2dPAD7++GMuXrzIpk2bmD17tvrcvr6+6t4lAHNzcxQKRaFt8vDwwNfXFwA/Pz+2bt3KmTNnRDBZgX3++ec0a9YMDw8PdVn37t3p168fjo6O3Lt3j7lz59K/f3+OHDmCsbExsbGx6OvrF/gZsbGxITY2FsjNk/B85vYaNWqgr68vq9O5c+cC58jJySEhIQF7e/tC2xwZGflSn/VljxPyxWTEyLZtjGzEfX0F4t5pj7iX2vOy9/JF61MWG0yamprSsWNHOnbsCOSm+r9y5QonT57k1KlThIWFcfLkSRQKBXp6eri5uRESEvJSDRVKzsur8J5GFxclZmbysnPn9FEqqXDzDePiRM+kUPbGjRvH6NGjuX79eoF94eHhXLhwge7dCyamiI6OpmXLlhpfp1mzZhgZGXH27FlatmxJQkICzZo1w8TEBDMzM65fv05KSgpKpRJ3d3eePn1KfHw8zZs3l52nefPmnDx5UlbWqFEjjdvRoEED2ba1tTVPnjzR+HihcIcOHWLFihVcuHCB5ORkJKng99fjx4+1ft0vvviC0NBQ9u/fr07iBKhfSEDu+sstWrSgWbNmHDhwgP79dZ+R82UW1BaLmmvHoWOHZNsNrBuI+/qSxM+k9oh7qT2leS9LlOZTT0+P5s2b07x5c0aPHo0kSezfv5/ly5dz6tQprl69WiqNFArn4CBhb6/i0SN5wNWggQoTE6heXcXjx7n7UlMVXL6sR4sWFatnT/RMCrrQuHFjOnfuzKpVq/jggw9k+yRJol27dkyYMKHAcc8OHdWEsbExTZo04fz58wC4ublhYpKbkKtly5acP3+elJQU6tSpg42NDU+fPi3yXM8nc8k7jyYMDOR/ChQKhVgn9BXt3buX9957j0aNGjFw4ECCgoLw8/NDkiT27t2Li4uLbC6jtvj7+7Nt2zZ2796Nk5NTsXXz5uHe+l96bFtbW5RKJQkJCVhbW6vrxcXF4eXlpa5z6tQp2XkSEhJQKpXqrMS2trbExcXJ6sTFxWFgYPDC3npBN2LS5T2TIvmOIAiaKlEwmZWVxdmzZwkNDSU0NJSwsDCSkpKwtLSkW7dueHp6llY7hSLY2ko8eiQvy3uebdNGyYED+YHmO++YM3hwNh4eSgYOzKYiEAl4Xh8vM4dRl0aPHs2wYcMKPDg3bNiQw4cPY29vXyAIS0/PXc/V0NBQ42DM3d2dPXv2IEmSrFezZcuWHD16lJSUFHUWV3Nzc6ytrbl06RKtW7dW17106dILAwdDQ0OUSs2zQAuvZsmSJbRo0YK//vqLpKQkgoKCGDZsGJ06deLOnTt07969QI/wq/rss8/Yvn07u3fvls2fLUpCQgIPHz5UJ+TJy4UQHByMn58fAA8ePCA8PFz9993Dw4PFixfz4MED9XIhwcHBGBsb06JFC3WdPXv2yK4VHBxMy5YtxXzJcupRhvxBoraFSL4jCIJmCl/g8H8SExPZv38/c+bMoXfv3tStW5c+ffrw008/Ua1aNb788kuOHz/OnTt32Lp1K9OmTSurdgv/M3SofPmQTp1yityXmKjH6tXGfPSRGatXy9PBl1ciAY+gKw4ODvTv31+dsCbPgAEDSE1NZdasWVy9epUHDx5w+vRpAgICSEtLA8De3p5bt25x9+5dEhMTi12aw93dnZiYGI4dO1YgmLx48SIRERGyTLBDhw5l06ZNHDx4kHv37vHDDz9w8eJFhgwZUuznqVmzJllZWYSFhZGYmEhGRsbL3BZBQ//88w++vr4YGBioh5rmBfNOTk6MGDFCnWFXG6ZOncrGjRv54YcfsPr/9u47vqb7f+D4696b3JvEShBJJARJJKQIaSOirRotOhQV1KjaRZdN+VZLa9aomt+Gqn5VzRYdtLWrajRGUREEERKRyN733t8f+eVyk5sp44b38/HIg3vm+57c3M95n8+ytSUqKoqoqCiSkpKA7AF5pk+fzvHjx7l+/TqHDx+mb9++2Nvb8/LLLwNQo0YNBg4cyIwZMzhw4ABnzpxh5MiReHt7G/pAdujQgSZNmvDWW29x5swZDhw4wIcffsgbb7xhGDdh8ODB3L59mylTphASEsL69ev59ttvTdbmC/MQmWqcTLpUl5pJIUTRFFgz6ebmhkqlomXLlvj5+TFmzBj8/f3zdL4XFWfAgAxWrNBw40Z20jVmzP1Bdrp1y8LHJ4vTp/P+mmfMsKJjxyzc3c27KVvumklp5irK05AhQwzTfOSwt7dn1apVrFq1ivHjx5Oeno6DgwN+fn6GWpdu3bpx6tQphg4dSmpqqsmpQXJ4e3uj0WjIzMykWbNmhuWurq5UrVqV2NhYo30DAwNJSUlhxYoVxMbGUr9+fT799NNC+0I0a9aM7t2789FHHxEfH280NYgofRqNxtDUOGfwowebfjo7OxMWFlZq5wsKCgLIM/XG5MmTmTp1KiqVigsXLvDdd98RHx+Pg4MDzzzzDF999RXVqt0ftXPOnDmoVCoGDx5MWloazz77LKtWrTIkxCqVik2bNjFhwgS6dOmClZUVgYGBzJo1y3CMBg0asHnzZj744APWrl2Lo6Mj8+bNk2lBzJg0cxVClJQiLi4u36qeOnXqkJmZScOGDWndujVt2rTB39+/SM1nhGll0QH29m0Fv/xiiY+PlpYttTzYdWr/fgt69Khicr/WrbPYvTuZUpw3u1TpdGBvXx2t9n6AkZHxWFlJp+zSVNrXMiUlBRsbGy5cuGBywJFHWWpqarH7TQrTinItExISCAkJoXPnzjg5OZVTZEXToUMH/P39mT17NgBt2rShUaNGbNiwAYA+ffpw6dIlQ39ZUTKVpSwITwjnvd/f45+7/5jl92Jsaiw67j+sPTP4DK41XCswosqrsnwmKwO5lqWnwgbgCQ8PN/SRPHbsGB9++CEJCQnY2try1FNP4e/vj7+/P61atUKj0ZRJgKJwTk56hgzJMLnuueey6Nkzg+3b8zZrPXbMguBgFb6+5tOP6vBhFRMnWqNQwLRpaUaJZPXqeooxpogQQlSYTp06sX79ej7++GMsLS0ZNWoU7733nqGWOSwszOTUM+LRNOOPGey7sa+iwygSBQrqVq1b0WEIISqJApNJjUZDQEAAAQH3Jxs/f/68Ibn86quvmDlzJmq1mhYtWuDv7y+Fo5lRKGDVqlReey0TS0tYvFjD0aP3f+3nzinLPZnMyIBJk6w4ccKC/v0zGD06OxHW6WDMGBtDk90BA4xrVO3tpYmrEKJymDhxIm+99ZZhkKY33ngDKysrduzYgUqlYuLEiYX2cxWPjqMRRwvfyEx41vTEUiUDJQkhiqZYo7lCdv8eb29vhg4dik6n49dff2XJkiUcO3aMkydPSjJphtRqeOml7AFAgoNVRsnkpUsqoHxHdl24UMO6ddk12R98YE2LFlrattUSFqY0JJKmyOA7QojKwtLSkpo1axot6927N717966giERFiUuL43by7YoOo0hsNbZ88uwnFR2GEKISKVYymZaWxokTJwxTg5w4cYKkpCT0ej1WVlbFmqxbVIzGjY1r9y5dKnBA31Kn1cK8ecZtVYOC1LRtm8qpU6p89som04IIISqLFi1aMGfOHF588UWT63fv3s3kyZM5c+ZMOUcmyltIbIjR68Z2jfkp8KcKisa0q1ev0qhRI2pa1USlLLgsFkKIBxWYTMbExHD06FFD8nj27FmysrLQ6/XUrFmTtm3bGgblkfmjKgcPD+Mmrdk1k+Xn8OG85/vpJ0syMgpPJh0cpJmrEKJyuHHjBsnJyfmuT05OJjw8vBwjEhUldzLZtHZT7G3Ma1T8OE2c2cUkhKgcCkwm3d3dUSgU6PV6XF1d6dmzpyF59PT0LK8YRSlyc9OhUOjR67MHtrk6lEUlAAAgAElEQVRxQ0FqKpTXAJTr1+cdCCgjQ8HBgxZSMymEeKQoChgq+/Lly0ZTcohH18WYi0avPWvK/ZMQ4tFRYDI5fPhwAgIC8Pf3x9HRsbxiEmXIxgbq1dNz40b2TY5er+DKFSVPPFG2tX5xcfDxx1YmR5UF2LLFkjNnCk4mZY5JIYQ5+/bbb9m4caPh9WeffcbXX3+dZ7u4uDguXLhAly5dyjM8UUFyJ5NNajWpoEiEEKL0FZhMzp8/v7ziEOWocWOt0UA3oaGqMk8mR460Yc+e/JtBb95snGRaWurJzDR+qu/sLDWTQgjzlZqaSkxMjOF1UlISSmXefulVqlRhyJAhTJ48uTzDExUkdzNXz1pSMymEeHQUezRXUfl5eOj4/ff7r8t6EJ6wMGWBiaQpzz6bxRtvZDBkiA1arYIGDbS0a5dVRhEKIcTDGzp0KEOHDgWgefPmzJ07N98BeMTjISE9gYikCMNrC6UFbrZuFRiREEKULkkmH0O5R3QNDS3bZPLbb/MmkrNmpdK4sY6BA23IyMjbr6hlSy2vvprF/v1JXLqkokuXTNSmW8gKISq5n376icWLF/P7g0+5KrmzZ89WdAiijBy/dZxph6YRnlj4AEpZOuOHoG62bqhVUpgJUVaCg1Vs2GBJYqICOzs9r7+egY/Po9lN6uZNBcOH2/D33yp0uuxxUVasSC33+eMlmXwM5R7R9dw5FXo9PDhWhE4HmzdbcuFC9rqiyMqCy5eVJCQo8PXVMmJEOvXr69m40bjgXLgwlaFDMwD46adkBgywISrKOKFt0yY7xubNdTRv/mh+CQjz8sknn/DLL7/w8ssvM3XqVKN1K1asYMOGDQQEBLBgwYIyi+H27dv06tWL6tWrs2XLFqpWrWpY9/bbb9OwYUPGjx9frGMFBQXRpEn+fbRGjBhBw4YNjd7znj17mDlzJmPGjKFfv36G5f/973/ZvXs327dvL8G7y/baa6/x2muvGR33Ufbrr7/y66+/cuPGDQDq169Ply5d6NSpUwVHJkoiJjWG3jt6E5ceV6L9ZfAd85GRAV9+qebOHSXDh6fj4iJdaSq7O3cUvPhiFdLS7t/Qbtyo5uTJxEdyrvL5862M5o4PCVExfrwVBw7kP5J4WTD7ZDIxMZFPP/2UH3/8kbt37xqaDrVq1QoAvV7P3Llz+frrr4mLi8PX15fPPvvM6OYpLi6OSZMmsXv3bgC6dOnC/PnzsbW1NWxz/vx5Jk6cSHBwMHZ2drz55ptMmjSpwNH4KitPT+Pk7OJFFd99Z8nrr2cCkJkJI0ZY8/33JX96euyYBatWqenYMYubN+8nitbWegIDMwyvn3pKy9GjSaxYoWbNGjX37inp0SODDh2kSasofw4ODuzdu5f3338f6/8f4jgrK4vdu3fj4OBQbnGkpaXxzTffMGrUqDI/V6tWrdi3b5/RsuDgYBwcHDh16pRR0hccHFzi+YQzMzMfq+mj0tLSGDRoEL/99htKpdIwiN2+fftYu3Ytzz//POvXr0ej0VRwpI+XO/dSyVQklbily6w/Z5U4kQRo6WDe83Gnp8OZMypSU+8vc3PT5ZtoRUcrCAnJLuM9PXVFHnU9PR2+/lrN1atKlErw88vi1VezKM9brpkzrVi2LPvv74cfLAkOTkQlU2xWaocPWxglkgAJCQp27rRk2LCMfPaqvP75J2/LwvPn81YQlTWzTybfffddzp8/z8qVK3F2dmbTpk10796dv/76i7p16/L555+zfPlyli9fjoeHB/Pnz6dHjx6cOHHCMOz6sGHDuHnzJlu3bjUcc+TIkWzatAmAhIQEevToQUBAAPv27SM0NJQxY8ZgY2PDO++8U2HvvazY2+vp0CGTffvu39hNnWpFmzZZ7NplSVCQhuvXH77pq06n4LffjG8eu3XLpHp14+1q1tQzfXo6H3yQTnx8drMEISqCm5sbd+/eZd++fbz00ksAHD16FLVaTYsWLUhISDBs+++//7J69WouXbpEZmYm7u7uDBs2DF9fXwBOnTrFe++9x5IlSwwPv3744QeWL1/OunXrcHZ2zjeOwMBAtmzZQq9evbC3Nz33m16v59tvv+WHH37g7t27uLi4MGDAADp37gxAr169gOzvP4CWLVuybNmyPMdp1aoV33zzDVFRUYaEOTg4mIEDB7Jy5Uq0Wi0qlYrU1FT+/fdfunXrBsCVK1dYunQpZ8+eRaPR8PTTT/P+++8balM/+eQT4uPjadGiBVu3biUzM5OGDRsSGRlp+M4GOHLkiCGWkydPsmTJEm7fvo2npyfTp0+nbt26Bf7OzNWcOXP49ddfmTx5MqNHj6b6/3/xJSYmsnLlSubOncvcuXOZMWNGBUf6+HhmwCn++fE5qB4Or78CTmfK9fy+Dr4MemJQuZ6zOOLioHPnqoSEGGdUCoWeZctS6d8/02j5rl0WDB5sQ1ZW9l2rhYWe9etTePHFwh8Gv/eeNd99dz+jX7FCY9RqqTzkJJIA168rOXpUxdNPl2/zQFG6bt82nUGdOKHi/4vCR0ruVn0AmZkKYmMV1KpVfvfSRU4ma9asyerVqwkMDDS5fvv27QwbNozY2NhSCy41NZWdO3eyfv16nnnmGQCmTp3K7t27Wbt2LdOmTWPlypW8//77vPrqqwCsXLkSDw8Ptm7dyuDBgwkJCeH3339n9+7d+Pn5AbB48WK6du1KaGgoHh4ebNmyhdTUVFauXIm1tTVNmzbl0qVLrFixgrfffvuRrJ1csCCNtm3vP8GJi1Pi41O9kL0eXr9++RcUSiWSSD7C2v7WtlzPd+T5I4VvZMLLL7/Mjz/+aEgmf/zxR1588UVu3bpltF1KSgpdunTh/fffR6FQsHXrVj744AM2b95MjRo1aNmyJf369WPWrFl8/fXX3Lt3jy+++ILx48cXmEgCtG/fnlOnThEUFJSnyW2O//73v+zfv5/x48dTv359zp07x7x586hWrRoBAQEEBQUxbNgwFi1ahLu7e761gs2bN8fS0pLg4GC6du1KZGQk0dHRdO3alXXr1hESEkLTpk05e/YsWVlZtGrVitTUVMaOHUvTpk0JCgoiISGBefPmMXv2bGbPnm049qlTp6hSpQqLFi1Cr9djb2/PoEGDeOmll+jRo4dRHBkZGXzzzTd88MEHqNVqZs6cyYIFC1i8eHGhvzNztG3bNgYMGMCUKVOMllerVo1JkyYRHh7Oli1bJJksJZmZ2U/iLfK5q/n1eHh2IgmQUA8OfwC9+zzUOd3t3Pmh5w+oFIVXZ1ko1Oze7sicD4tW9VW7tp4338zAwaH8ysSffrLMk0hC9hRiixdr8iSTS5ZoDIkkQFaWgiVLNIUmk3o9/Phj3u+jHTssyy2ZNNV959YtJSDJZGUWGWm6IuTvvx+9KmedLrtZrymRkWaaTOoL6Tin0+lKPenKyspCq9ViZWVltNza2pqjR49y/fp1oqKi6NChg9G6gIAAjh07xuDBgzl+/DhVq1aldevWhm38/f2pUqUKx44dw8PDg+PHj9OmTRtDszaAjh078umnn3L9+nUaNGhQqu/LHLi56Rg/Pp1PP7UqcLsGDbQMHpxR5OpyOzs9X36p4ezZvH+49evreOYZ+aIW5u35559n2bJlhIeHY2Njw7Fjxxg7dixBQUFG2+XUQOYYN24cBw4c4K+//jLUDg4bNowTJ04wZ84cIiMjCQgIKPLonqNHj+a9996jT58+NGrUyGhdamoq3333HYsXL8bHxweAunXrcuHCBbZt20ZAQIChGX/16tWpVatWvuexsrKiSZMmhmTy77//pkmTJlhZWdGyZUuCg4Np2rQpwcHBODs74+joyM6dO0lLS+M///kPVapUAWDSpEm888473Lx5ExcXFwA0Go0hOcyhVCqxsbHJE5NWq2XcuHG4uroC0Lt3bz777DP0en2lfKAXHR1dYJNgHx8fNm/eXI4RPXo++UTDb795cvduFW7fVvDLL8m0bm26jPl2S67lF3oDD5dMznl2Di7VXIq27RwN8+YVXN7m9v33lvz5ZxImZpcpE1eu5H+isDAlWi1GzUBDQ/OW85cvFx5sXJyCpKS8f9M3b5bf33lMTN5zmYqpPGm1MHasNT/8YEmbNln8978p1KhRoSFVOlFRpn+Hly+riI1VULPmo1NhEROjMHqY86CoKCXe3uU33kixmrkWVKCfPHnSqA9iaahWrRp+fn6GPpAODg5s3bqV48eP06hRI6KiogDyNAOzt7fn9u3bANy5c4datWoZxa5QKKhduzZ37twxbJO7KVXOMe/cufNIJpMA/v4FPz1s0yaLoKCUYs/v+MormXh6Vic93fjz8vrrGeVWKApRUtWrV6ddu3b8+OOPVKtWjZYtWxr6uz3o3r17fPnllwQHBxMbG4tOpyM9PZ3IyEjDNhYWFnz00UcMGDAAOzs7li5dWuQ4WrZsiZ+fH6tWrcoz5++1a9fIyMhg/PjxRt9tWVlZJmMtjK+vL7/88gtg3C+yZcuWHDx4kAEDBhAcHGxornvt2jXc3NwMiSRAs2bNUCqVhIWFGZLJhg0bGiWSBVGr1YZEEqBWrVpkZmaSmJhoaCJamTg7O3Po0CGGDBlicv2hQ4cKraEWBbtwQcWZM/cTtJs3lfkmk/dSkvIsq2VVu0T9impoajDSZyTPN3y+yPt8/33x+wtfvKjiwgVlmc8DnePB8Q1y02oVREYqDPcDCQnZfdFyi41VkpICNjb5nyc83PRFj4hQlltfr4iIvCe5datik8nffrNg/frs78s9eyz5+ms177776PXzK0u3b+f/Gf77bxXPP//ojMcRGZn/57WgdWWhwGRy5cqVrFq1yvB66tSpzJo1K8928fHxJCQk0Ldv31IPcPXq1YwZM4amTZuiUqlo0aIFvXr14vTp06V+ruIKDQ0t1/1KW1aWGmhucp2HRwqff36BlBQoSbjPPNOI33+vabSsTZtLhIaW3hejuVzHR0FpXku1Wk3VqlVJSUkptWOWROqDI0gUgVarRavVkpqaSqdOnZg/fz7W1tYMGjSI1NRUo/UAH3/8Mffu3WPkyJE4OjpiaWnJxIkTSU1NNTr3qVOn0Ol0JCYmEhkZiUV+7fDIHrQFID09ndTUVAYPHsxbb73F8ePHjc6fc/xZs2ZRp04do2NYWFiQmpqa51gF8fb25quvviIsLIzg4GDGjx9PamoqTZo04YsvviAqKoqQkBC6d+9OamoqWVlZ6HQ6o+NmZmYa/s25Xmq1Os+59Xq9YZsH91UqlSbjTElJybeJbnJyMvHx8Vy7do2kpPvJgoeHR4Hvt6xs3LiRgIAAXF1d6devH59++invvPMOo0ePxt3dHYDLly+zcuVKdu3axfTp0yskzkdFvXrGSVZ+SQpAzL28Zc+J3lfKpaYiI6PgWr+CnDunMotkMme9s3N2sh4Rkf+2ERFKPDzyjzm/fdPTFdy9qyjyID4PI7tJq7GCEpHysHix8WBcH35oLclkMeVXMwlw5MijlUya6i9ZlHVlocBk0t7eHi8vLwBu3LiBk5MTTk5ORtsoFAqqVKmCj4+PYaCH0tSwYUN+/vlnkpOTSUxMxNHRkcGDB9OgQQPDYBHR0dHUq1fPsE90dLThBqtOnTrExMQYNZXS6/XcvXvXaJvo6Gij8+a8zn2j9qCS3LDk9NM0B/XrZ3es1+vz/vF17aqiceOSxzl2rIoHp4x75pksnnvONf8dismcrmNlV9rXMiUlBRsbG6Kiooyax5e0D2N5UalUqFQqrK2tadu2LWq1moSEBDp16oRarTZaD3Du3DnGjh1L+/btAYiNjSU2NhZLS0vDNrdu3WLZsmWMHz+eY8eOMX/+fFauXJlvQpnTpF+j0WBtbY23tzddunQhKCjIEIO1tTWenp6o1Wru3btHQECAyWPlDITzYDz58fX1Ra1W89tvv3Hv3j2efPJJrKysaNy4MTY2NuzYsQOtVou/vz/W1ta4u7uzZ88edDqdoXby33//RafT0bhxY6ytrfNcrxwPvo8clpaWKBQKk3FaWVnlG39mZiY1atSgQYMGecqmijBmzBhWr16Nq6sr48aN4/r16/zvf/9jw4YNRuWPXq9n4MCBjB07toIjrtxcXIwTloKSoejIvJ+h8PDyafZ25YoSrfZ+OVu7to5Jk9JNbrt3rwV79tx/eHL+vArINLltact9/Tw8tEZNWbOTQK3JbXMfpyTJZM6+9vZl3x3GdDJZsTWTFZ3MPgry6zMJsGSJFSdPWvC//yVTyg0pK0SlqZns1auXYUTAl19+mYkTJ9KuXbtyCSy3KlWqUKVKFeLi4ti7dy8zZ87E1dUVBwcH9u/fb2h+lZaWxtGjR5k5cyYAfn5+JCUlcfz4cUO/yePHj5OcnGx47efnx0cffURaWprhZm7//v04OTkZNbt61Gg04OCgN/mhy/3Et7jattXy9tvprFqlxtFRz6JFxaslEqIiKRQKvv76a4B8m2nWr1+fPXv20LRpU9LS0li+fLlRkqjVapk1axY+Pj50796d9u3bM3DgQNauXcuIESOKHMuwYcMMrT4aNmwIZH8fvv766yxbtgy9Xo+Pjw8pKSmcP38epVLJq6++ip2dHRqNhmPHjuHk5GSoMTZFrVbzxBNPsGXLFkN/yRw+Pj5s2bKFBg0aGPo5vvDCCwQFBfHJJ58wbNgwEhMTmT9/Pu3atTM0cc2Po6MjZ86coXPnzlhaWpZ694iK9ODDE6VSyRdffMFbb73Fr7/+Snh49gT39erV44UXXsDb27uiwnxk1K+fu2Yy/xvJhOi8nc9u3lTSokXZ1/pdvGjct7BFCy0jRpiucapVS2+UTJ47Vz4Jhlabt5ln69bGyeSDfRoLTiYLvpE11cT0wX1LOPtQsZhq0lrRyZyp5r06HdI9qIiSkiAxseDP3h9/WDB7thXz56eVU1Rlp+CaSTNKJh/0448/lmUc+dq7dy86nQ4PDw/CwsL4z3/+Q+PGjenfvz8KhYJRo0axaNEiPDw8cHd357PPPqNKlSqGJNjT05NOnToxduxYlixZAsDYsWPp3LmzoTamV69ezJs3j9GjRzNhwgQuX77MkiVLHtl5Jh/k4qIz+SQndyFdEp98ksbUqWnY2JTvfDdClIYH+wOaMnXqVObPn8+QIUOoXbs2Q4cO5d69e4b169ev5+bNm6xfvx6AGjVqMH36dCZMmEDr1q1p0aJFkeJwcHAgMDCQDRs2GC0fPnw4dnZ2bNy40fC95+7uTv/+/YHs5q7vv/8+69at46uvvqJFixYmpwbJ0apVK5PzSLZs2ZK9e/caHthBdm3h4sWL+fzzzxk2bJjR1CCFGTZsGAsWLKB3795kZGQYTQ3yKPL29pbEsYzUq2dcq1hQMpkWk7eVUWHNOkvLxYvG58k91/ODnnjCuFbu3LnyGYUyNtbSaDAPOzsdjRsbx/Lg9S0oYSzsuhZWM1keTMVQUFzlIT4+77LbtxWFjlsRF5c9Ou6tW0qsrfV06ZJVYM1weTh2TMX160pefDGTfJ5hlrrcyZWLiw5LSz1hYcZ/Q7t2WTJvXlqlvy8tqPbRrJq55hYbG8uKFSs4fPgw0dHRrFq1Cj8/P2JjY/nyyy/p3r07np6epRpgQkICH3/8Mbdu3cLOzo5u3boxffp0Qx+a9957j9TUVCZOnEhcXBy+vr5s377dMMckQFBQEJMmTeK1114DoGvXrkYDWtSoUYPvv/+eCRMm0L59e2xtbRkzZgxvv/12qb4Xc+TiouPkybzLH7ZmMkch9+NCmI3C+q/lXu/h4cGXX35ptKxdu3aGJpmDBw9m8ODBRuv9/Pw4dOhQvudwcnIymVyNHj2a0aNHGy1TKBQEBgbmO10TQLdu3QzzQhbGVLwAPXr0yDONB2TPyVnQgEL5Xc8nnnjCUOub46WXXjJMxZLDx8enUiaaj/oDSHNS1GauMfFp6FMc8iwvr8QlJMT4PF5e+TfjdHPTodHoDQPYRUcruXNHQZ06ZdscNzLSuAWGi4seFxfjcz54vQpK3IvS9zI/5ZXQmWrmmpCgIDm5Yu5b4uKyp2jL7cqV+/1UTcnKgsDAKpw4cf92ft48Pb//noSXV8UklBs2WDJmTPYITA0aaPn552Tq1i375uS5myk7OemYNSuN8eOt/7+5eM52Ss6cUeLjU7EJ98OqlDWT169fp2vXrsTGxtK0aVOuXbtmGCyhZs2abN++nbt377JgwYJSDTC/G5kcCoWCqVOn5jsXG4CtrS3//e9/CzyPt7e3YTTDx0nuwiJHaSWTQgjxOBkzZgzvvPNOkbZVKBR55i8VRWdvr0ej0ZGenn1TlZCgIC6OPP2hToVGA6aSyfK54crdzLWgm3wLC2jSRMvp0/dvz86dU9GhQ9kOHJI7mXR21hWYrD9M7WLB+5bP7yS/kVtv31bi7l7+9z+5a89yXL2q5Nln808mjxxRGSWSkD3FyYIFGtasqZjuRcuX3x9I6No1Fb16VeHnn5PKvJ9i7uTK0VGPv7+WI0eS6N/fhp9+ut98fPduS3x8TPdbriwKShjNtmZyxowZ6PV6/vrrL6pVq2YYmS7Hiy++yE8//VTqAYqylbuwgOzmLQ9U7AohhCgiX1/fR3Y6KXOjUICDQwY3bhhPD2Jra1yu/RNqov0g5VMzmZmZd+7F3M1Hc3viCR0PDlh//rySB6bTLhNRUcbJZL16eZPJB/s6Fly7mP9Nrk5X8BQc5VEzqdebrpmE7Nhy3d6Wi6tXTcdz5UrBAzD99pvpka537LBk1qy0cqkRfFB0tIILF4wT4wsXVEycaM2XX5Ztcpu7ZtLR8f7nt0uXTKNk8rvvLNGa+DP08tLRrVsm+QwgblYKGmwoOVlBYiLldi9f5GTywIEDvPvuuzRo0IDY2Ng8611dXeUJayVkKpnM3Q9FCCFE0QwePLjApseidDk5pRslk+HheedlDLlmerCNkiaT588rmTnTiogIJRYWeiwssmsUVarsf9VqPXZ22T8pKcYTizs56QqtofH2Nr7L/eefsu83mbeZqw4HBz0WFnpD/LGxSpKTwcqq4ITw5s3854u8c0dBZmbJ+1uWhvh4SEnJv2YyZ8Ta8hQWll8yaXr5jz9a8Ntv2XNRmpKVpaBp0+p89FEqo0dnUMTpfh/aH3+YTiu2b7dk9uy0Mp32xVTNZI4XXjCu2b92TcWCBab/roYOTWfhQvMeoEevz1szaWurM2oqHRWlpFq18qllL3IymZ6eXuCoe/Hx8ShlyKlKx3QyKU1chRBCPJygoCCWLl1KVFQUXl5ezJkzJ9+pbErK0dF4VFRTffmu3zB9AxsZqSAjg2LdaMfHQ8+eVUrcjKyg/pI5cg/Cs3u3JQkJqVSvXqJTFknumklnZz0qFTg56Y3m74yIUFKtmt4oIbS11aHVKgwjaaalKYiJUVC7dt7rnrvm0d1dy+XL92/qS/I7Ka6Caj8ranqQ4iST69db8u67NkU67kcfWfPPPyqCglLLZcCZgwdNJ2harYJt2yx5662ymzcz94A0D9ZMOjjo8fXN4u+/C0971q5V89ZbGRU+iFFB4uOz/85yWFvr8fDQceLE/c9LZGT51bIXOZls0qQJR44cYciQISbX//TTTzRv3rzUAhPlw1QtpCSTQgghHsb27duZMmUKCxcuxN/fn6CgIAIDA/nrr7+M5oV+WLmTyevXlWTkul+9HWE8GXwOvV7BzZtKXF2LXubNm2f1UP2RijIoylNPaalZU0ds7P2+oGvXqnn//bK7Ec+dTOY8aHZx0Rkl6FevZieTxtvq0Wr1/Puv8TQippLJ3H0iGzXSkZioMFxTvV7BL79Y5NlXowE7Oz2WlgXXbFlbZ9fa7N1rwdWr2TWkud24kf/vLzxcSUpKgacoVFpa8Y+RXzPXsDAlH35ohaWlHoUiewqXlStNf55btNASFqYkIcH4Gm/bpubyZRVvvplBzZo6NKZ3R6UCBwcdtWrpsbQES0tQqfTFSkIPHco/rdi4UU2PHsWbM/XuXQuqVy9aALlrtZ2cjH/5Y8emM2BA4WmPXq9g0SKNWU9pl/sz7OCQ3ZLgQdl/U+VTy17kZHLUqFGMHDmSJk2aGAbE0el0XLp0ifnz53Py5Mk8Q9cL82dqwubcBYUQQpgjvak7RWEWli9fTr9+/Rg0aBAACxYsYO/evaxdu5YZM2aU2nmcnIwTrOXLNUYDgGR7Jt/9W7UqvwECNBo9ffsWnhBaWcFbb2Uwe/b95rsffWTNrFlWBez1cLRa4xv2nGSyXj0dR4/eX963b96hTl1cdGi1GCWT7dtXzWfeROOFzs467t5VGCXogwZV3DDwa9ZoWLMmn2yryFoVvkkRZWQoWLq0aPH06JFBzZp6k7WWZ86oGDvWutTiKokzZ1R4eha3et2nxOdzcDB+cPPyy1n8+msShw5Z5OkvGRGhZP36+w9UNm5Us3FjObUNLgWOjnqjmliAoUNtGD48u4x8++0MBg4su/MXOZkMDAzk5s2bzJ49m9mzZwMYptpQKpV8/PHHdO3atWyiFGXG1Je9JJPiYahUKtLT01EoFHKzL8pMVlYWiYmJKJVKVKrymYuvMA/OM/o4y8jI4PTp03lGte3QoQPHjh0r1XPlrpksTy4uOtatS0Gvz56iIftHQXo63L2rID7+fgGrVsOzz2YVOMfkg0aMSGfpUg1JSfePkTvhKytKpd7Q38xUV5jc6tXTkZmrwkmvV5isFczNxUVPTIye4OCSRCoe9PzzWXh76wgISOT77y359NOye/hQFG3aZKFWw8GDxZqFsNTkrpkE8PPT4ueXt7ZOq80eGTd7wKPKx8FBn6dmEu4/vCnrW7Fi/YbHjh1LYGAgO3fu5OrVq+h0Oho2bMgrr7wio9dVYr17Z7B5c/YTGIVCT/fuxWuGIMSDNBoNmZmZJCQkkJWV9Vj1pU5OTiYz912VKJH8rqVer0er1RIbG1SHD7wAABTRSURBVEvG/7dnrFpes2KLIomJiUGr1WJvb2+03N7enjt37uS7X2hoaLHP5eamAlUGaMu/FuHtt8OoUcP0AwQ3N9P7FOct9ujhwjffOJYgsofj5pZKWFh2oA4OtkDBHa8cHW+RmakEXIt9rlq1wvH0VJdo38edtbWWrCwFmZlKXnnlLmr1NcPnq3t3UKtrMmNGowqLr3Xr29Spk8HBg/n8MZShunXTuXv3EjExRd+nX79azJrVsOyCKkPOzlHY2aUCHibX5zzoLMl3LGTPq12QYj8ucHFxyTN5tqjcJk9OJyRExbVrSsaNS6N+falNEg/H0tISLy8vw9yt5lJzVNZiY2OpWbNmRYfxSCjsWur1etLT02nZsiU2NkUbjEKYt8JuWEwJDQ3FMXAWkT+9Bcn2BW6rrB5J35FXuHf2afbts8hTm1ZUNWroGTkyg5Eja6NQ1C7ZQYpg/nyIjc1kzx4LoxFhy5Krq47PP9cZfhcNGkBISDrbtqlJyzXApUYDL76YyZgxNdFq4fz5TH75pWixWlnp6d07kwED6qBQgEaTxsGDFmTlM51mSoqCuDiFyekcHpSUlD3Poqenjueey6JGjfzvZzQaPR07ZnH6tIq5c62IiSmda6zT6Ur8ELVRIx1ffJGKTgfBwSoyMrJrvDMyjGPz8tLyyitZREYqSEhQ4OlpSe5E4r33oHnzZH7/3YKMDEhNVRAbm/81TEtTcOuWgqQkBZmZkJlZ+PU2Ra3W8+KLWUybVh0LC4iPT2PzZkuSk4t/fbOytFhYFO/+wcVFx+zZmTRuXLzvk3Hj4N69dDZuLFmsFcHSEp57LovJk6thY1ONM2cy2LLF0mhwHgA7OzvgZom+Y4tCERcXJ5lDOQoNDS2zX+bjRK5j6SnLa5mQkEBISAgpKSkoymMouQoWGRmJo2P51yQ8igq7liqVChcXF1xdpUbD3GRkZODk5MSaNWvo3r27YfmECRO4cOECP//8c6mdKzQ0FNdGrkVqUm+pskSpqHwtJbTasm+mllMOWDxki0SdrmixKhRQVo1W8puWpLzI/UnpkWtZfKb+BhUKuHKl7K5lgV8bLVq0KNbBFAoFpx+caVcI8VirXr06Tz31VEWHUW6k4Cs9ci0rL7VajY+PD/v37zdKJvfv30+3bt1K/3yqyjNQRkmUR8OOnLkyH5Y59Gp4DJ5bCpGvivgbLPCrw8vLq0gHCQ8P599//30sah6EEEIIUbAxY8YwcuRIfH19ad26NWvXriUyMpLBgwdXdGhCCCFKUYHJ5KZNmwrcOTw8nM8++4z9+/ej0WgYWJbjzgohhBCiUujZsyexsbEsWLCAqKgomjRpwubNm6lfv35FhyaEEKIUlajP5M2bN1m4cCHffvstAG+88QZjx46lbt26pR6gEEIIIYQQQgjzU6wW8hERESxcuJANGzYAMHDgQMaNGydJpBBCCCGEEEI8ZoqUTOZOIgcMGMC4ceNwdnYu0+CEEEIIIYQQQpinApPJiIgIFi1axIYNG9Dr9ZJECiGEEEIIIYQACukz6eDgQGZmJs2aNWPcuHG4uLgUekBfX99SDVAIIYQQQgghhPkpMJm0s7O7v2Eh037o9XoUCgWxsbGlF50QQgghhBBCCLNU4NSWy5cvN/wsW7aswJ+cbYRpQUFBNG/eHAcHB9q1a8eff/5Z0SGZvTlz5mBra2v007hxY8N6vV7PnDlz8PLywtHRkZdeeol///23AiM2D0eOHKFv3740adIEW1tbQ1/nHEW5bnFxcYwYMYL69etTv359RowYQVxcXHm+DbNQ2LUcNWpUns9op06djLZJT09n4sSJNGrUiLp169K3b18iIiLK821UuEWLFtG+fXvq1auHm5sbffr04cKFC0bbyOdSFIeUqcUnZWrJSJlaeqRMLR3mVqYWmEz269ev2D8ir+3btzNlyhTGjx/PoUOH8PPzIzAwkPDw8IoOzex5eHgQEhJi+HnwhuHzzz9n+fLlzJs3j3379mFvb0+PHj1ITEyswIgrXnJyMk2bNmXu3LlYW1vnWV+U6zZs2DDOnj3L1q1b2bp1K2fPnmXkyJHl+TbMQmHXEuC5554z+oxu2bLFaP3UqVPZtWsXa9as4eeffyYxMZE+ffqg1WrL4y2YhT/++IOhQ4eyZ88edu7ciYWFBd27d+fevXuGbeRzKYpKytSSkzK1+KRMLT1SppYOcytTSzTPpCiejh074u3tzdKlSw3LWrVqxauvvsqMGTMqMDLzNmfOHHbu3MnRo0fzrNPr9Xh5eTF8+HAmTJgAQGpqKh4eHsyaNYvBgweXd7hmydnZmfnz59O/f3+gaNctJCSE1q1bs3v3bvz9/QE4evQoXbt25cSJE3h4eFTY+6lIua8lZD9FjY2NZdOmTSb3iY+Px93dneXLl9O7d28ge57eZs2asXXrVjp27FgusZubpKQk6tevz4YNG+jatat8LkWxSJlaMlKmPjwpU0uPlKmlp6LL1AJrJsXDy8jI4PTp03To0MFoeYcOHTh27FgFRVV5XLt2DS8vL5o3b86QIUO4du0aANevXycqKsroulpbWxMQECDXtQBFuW7Hjx+natWqtG7d2rCNv78/VapUkWtrwtGjR3F3d8fX15d3332X6Ohow7rTp0+TmZlpdL1dXFzw9PR8rK9lUlISOp0OW1tbQD6XouikTH04UqaWLvnuKn1SphZfRZepRZpnUpRcTEwMWq0We3t7o+X29vbcuXOngqKqHJ588klWrFiBh4cHd+/eZcGCBbzwwgv89ddfREVFAZi8rrdv366IcCuFoly3O3fuUKtWLaNBtxQKBbVr15bPbC6dOnXilVdewdXVlRs3bvDJJ5/QrVs3Dhw4gEaj4c6dO6hUKmrVqmW03+P+9z9lyhSaNWuGn58fIJ9LUXRSppaclKmlT767SpeUqSVT0WWqJJPCbD3//PNGr5988kl8fHz49ttveeqppyooKiHue+211wz/9/b2xsfHh2bNmrFnzx66detWgZGZrw8++IC//vqL3bt3o1KpKjocIR4bUqYKcydlavGZQ5kqzVzLWK1atVCpVEbV9ADR0dHUqVOngqKqnKpWrYqXlxdXr17FwcEBQK5rMRXlutWpU4eYmBj0+vvdqfV6PXfv3pVrWwgnJyfq1q3L1atXgexrqdVqiYmJMdrucf2cTp06lW3btrFz504aNGhgWC6fS1FUUqaWHilTH558d5UtKVMLZi5lqiSTZUytVuPj48P+/fuNlu/fv9+onbIoXFpaGqGhoTg4OODq6oqDg4PRdU1LS+Po0aNyXQtQlOvm5+dHUlISx48fN2xz/PhxkpOT5doWIiYmhtu3bxu+yH18fLC0tDS63hEREYaO74+TyZMnGwq9B6cjAPlciqKTMrX0SJn68OS7q2xJmZo/cypTVVOmTPmo5G9FFEW1atWYM2cOjo6OWFlZsWDBAv7880+WLVtGjRo1Kjo8szV9+nTUajU6nY7Lly8zceJErl69yuLFi7G1tUWr1bJkyRLc3NzQarVMmzaNqKgolixZgkajqejwK0xSUhIXL14kKiqKb775hqZNm1K9enUyMjKoUaNGodetdu3anDx5kq1bt9KsWTMiIiIYO3YsrVq1euyGMi/oWqpUKmbOnEnVqlXJysrin3/+4Z133kGr1bJgwQI0Gg1WVlZERkYSFBSEt7c38fHxjB07lurVq/Pxxx+jVD4ez/MmTJjAd999x7p163BxcSE5OZnk5GQgOzlQKBTyuRRFJmVqyUiZWjJSppYeKVNLh7mVqTI1SDkJCgri888/JyoqiiZNmjB79mzatm1b0WGZtSFDhvDnn38SExND7dq1efLJJ5k2bRpeXl5AdnX83LlzWbduHXFxcfj6+vLZZ5/RtGnTCo68Yh0+fJhXXnklz/LXX3+dlStXFum6xcXFMWnSJH755RcAunbtyvz58w0jhT0uCrqWixYton///pw9e5b4+HgcHBx45plnmDZtGi4uLoZt09PTmT59Olu3biUtLY1nn32WhQsXGm3zqMvvczN58mSmTp0KFO3vWT6XIoeUqcUnZWrJSJlaeqRMLR3mVqZKMimEEEIIIYQQotgej/pgIYQQQgghhBClSpJJIYQQQgghhBDFJsmkEEIIIYQQQohik2RSCCGEEEIIIUSxSTIphBBCCCGEEKLYJJkUQgghhBBCCFFskkwKIcpFs2bNeO211yo6DCGEEKLSkzJVmAtJJoUooQ0bNmBra2v4cXBwwMvLi549e7Jq1SoSExMrOkQhhBCiUpAyVYjKyaKiAxCispsyZQoNGzYkMzOTO3fu8McffzB16lSWL1/Oxo0beeKJJyo6RCGEEKJSkDJViMpFkkkhHlLHjh156qmnDK/HjRvHwYMH6du3L6+//jrHjx/H2tq6AiN8fOj1etLS0uR6CyFEJSVlqvmQMlUUhTRzFaIMtGvXjokTJxIeHs7mzZsNy8+dO8fo0aPx8fHBwcGBRo0aMWTIEMLDww3bXLlyBVtbW5YtW5bnuOfOncPW1pY1a9bke+7r169ja2vL4sWL+frrr/Hx8aFOnTq0b9+e4OBgo21feuklXnrppTzHGDVqFM2aNTN5zKCgIFq0aIGTkxOvvvoqN27cQK/Xs3DhQry9vXF0dKRv377ExMSYjO/gwYO0a9cOBwcHfH192bhxY55t0tPTmTt3Lq1ataJOnTo0adKEqVOnkpKSYrSdra0tY8eOZfv27QQEBFCnTh22b9+e77URQghR+UiZKmWqMF9SMylEGenTpw8zZ85k3759DBo0CID9+/dz+fJl+vbti5OTE2FhYaxdu5a///6bo0ePYmNjg5ubG35+fmzevJm3337b6JibN29GrVbTs2fPQs+/fft2kpOTGTx4MAqFgs8//5yBAwdy+vRpLC0tS/Setm3bRkZGBsOHDycuLo6lS5fy5ptv0rFjRw4cOMC7775LWFgYq1ev5oMPPmD16tVG+1+7do033niDQYMG0bdvX7Zs2cKoUaPQaDSG96TX6xkwYABHjhzhjTfewMvLi5CQENasWcPFixfZvn07CoXCcMw///yTHTt2MHz4cBwcHGjcuHGJ3psQQgjzJWWqlKnCPEkyKUQZcXZ2pnr16oSFhRmWDR06lHfeecdou65du9K5c2d27dpFnz59AOjbty/jxo3j4sWLeHl5AaDT6di2bRsvvPACdnZ2hZ4/IiKC4OBgbG1tAXB3d6dfv37s3buXLl26lOg93bp1y+iYOp2ORYsWkZqayqFDhwwF6t27d9m+fTtLliwxah5z5coVgoKC6NWrFwBvvvkmzz77LB9++CHdu3dHqVSydetWfv/9d3bt2sXTTz9t2Ldly5aMGDGC/fv306FDB8PyS5cucfDgQZo3b16i9ySEEML8SZkqZaowT9LMVYgyVLVqVZKSkgyvbWxsDP9PSkoiNjYWd3d3atSowenTpw3revbsiUajYdOmTYZlhw8fJiIiwlA4FqZbt26GAgogICAAyH6SWVK5j+nr6wtA7969jZ7M+vr6kpmZSUREhNH+9vb2Rk+Ara2teeONN7h58ybnzp0D4Pvvv8fd3Z0mTZoQExNj+Gnbti0KhYLDhw8bHbN169ZS6AkhxGNAylQpU4X5kZpJIcpQUlIStWvXNryOi4vjo48+YseOHdy7d89o24SEBMP/bW1t6dq1K1u2bOHDDz9EoVCwefNm7Ozs6Ny5c5HO7eLiYvQ6p8CKi4sr6dvJc8zq1asD2U+MTS3Pfa6GDRuiVBo/w3JzcwPgxo0bNG/enCtXrhAaGmpYnlt0dLTR6wYNGhTvTQghhKiUpEyVMlWYH0kmhSgjERERJCQk0KhRI8OyN998k2PHjjFmzBiaN29OtWrVUCgUDBkyBJ1OZ7R/3759+eGHHzhy5AhPPvkku3btolevXqjV6iKdX6VSmVyu1+sN/1coFEavc2i12mIdsyjnKiqdToeXlxdz5841ud7R0dHotYwyJ4QQjz4pU6VMFeZJkkkhykhOc5qcvghxcXEcOHCAKVOmMGXKFMN2aWlpJp9sdurUCXt7ezZt2kR0dDQJCQlFbo5TVLa2tiab6Dw4El5pCgsLQ6fTGT1JvXLlCgD169cHsp+0nj59mnbt2hkNCiCEEOLxJWVqXlKmCnMgfSaFKAMHDx5kwYIFuLq60rt3bwDDl33uJ4srVqzI8wQVwMLCgsDAQHbs2ME333xDo0aNaN26danG2bBhQ0JDQ7l7965h2T///MOxY8dK9Tw5oqOjjYYZT01NZf369Tg7Oxsmou7Rowd37twxOVR7eno6iYmJZRKbEEII8yRlqmlSpgpzIDWTQjykvXv3cvXqVbKysoiOjubQoUPs37+fevXqsXHjRqysrIDsPg9PP/00S5cuJTMzk3r16nH06FH+/PNPatasafLYffv2ZcWKFezbt8/oyWtpGTBgAMuXL6dnz54MHDiQ6OhovvrqK7y8vMqkgHFzc2P8+PGcPXuWunXrsnnzZkJDQ/nyyy8NNwZ9+vRhx44dTJgwgSNHjuDv749er+fy5ct8//33rFu3jmeeeabUYxNCCFHxpEwtOilThTmQZFKIh5TTD0GtVmNnZ0fTpk2ZM2cO/fv3p1q1akbbBgUFMWXKFL766iuysrIICAhg586dvPrqqyaP3bx5c7y9vTl//nypN8cB8PT0ZNWqVcyePZtp06bh6enJ6tWr2bJlC3/88Uepn69BgwYsWrSIDz/8kIsXL+Ls7Mzy5csJDAw0bKNUKvnf//7HypUr2bhxIz///DNWVlY0aNCAoUOHGp62CiGEePRImVp0UqYKc6CIi4srfm9eIUS5ad++PWq1mj179lR0KEIIIUSlJmWqEKVL+kwKYcbOnj3LqVOneP311ys6FCGEEKJSkzJViNInNZNCmKELFy5w+vRpVqxYQVRUFGfOnDGanFkIIYQQRSNlqhBlR2omhTBDO3bsYMyYMaSlpbFmzRop9IQQQogSkjJViLIjNZNCCCGEEEIIIYpNaiaFEEIIIYQQQhSbJJNCCCGEEEIIIYpNkkkhhBBCCCGEEMUmyaQQQgghhBBCiGKTZFIIIYQQQgghRLFJMimEEEIIIYQQotj+D1yQ3wOvIG0cAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mean_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/fps</td><td>▁▄▄▄▅▆▆▇▇████████▆▆▇▇▇▇▇▇▇▇▇▇█████▇▇▇▇▇▇</td></tr><tr><td>train/approx_kl</td><td>▄▂▂▃▆▄▆▃▂▂▇▃▂▂▄▂▃▄▁▄█▅▅▃▁▃▃▂▁▁▂▂▃▃▃▂▁▂▁▂</td></tr><tr><td>train/clip_fraction</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/clip_range</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/entropy_loss</td><td>█████▇▇▆▆▆▅▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/explained_variance</td><td>▁█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▅▂▂▆▄▁▂▄▃▁▂▄▇▃▄▂▂▃▄▃▄▃▂▅▂▄▃█▄▃▅▄▂▄▃▃▇▅▂▅</td></tr><tr><td>train/policy_gradient_loss</td><td>▄▆▆▅▂▄▃▅▆▇▁▅▆▆▄▆▅▄█▄▂▂▅▄█▃▅▆▇▇▇▇▄▅▄▆▇▄█▆</td></tr><tr><td>train/std</td><td>▁▁▁▁▁▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>train/value_loss</td><td>▇▂▃▄▂▃▂▂▃▁▂▄▅▄▄▃▃▃▃▂▃▂▁▅▂▃▄█▃▅▄▃▁▄▄▃▄▅▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>1000.0</td></tr><tr><td>eval/mean_reward</td><td>3003000.0</td></tr><tr><td>global_step</td><td>100352</td></tr><tr><td>time/fps</td><td>116.0</td></tr><tr><td>train/approx_kl</td><td>0.0</td></tr><tr><td>train/clip_fraction</td><td>0.0</td></tr><tr><td>train/clip_range</td><td>0.2</td></tr><tr><td>train/entropy_loss</td><td>-2.87217</td></tr><tr><td>train/explained_variance</td><td>-0.0</td></tr><tr><td>train/learning_rate</td><td>0.0003</td></tr><tr><td>train/loss</td><td>774743552.0</td></tr><tr><td>train/policy_gradient_loss</td><td>-5e-05</td></tr><tr><td>train/std</td><td>1.01779</td></tr><tr><td>train/value_loss</td><td>1801531008.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">laced-sun-1</strong>: <a href=\"https://wandb.ai/nishamdev/StockTrading/runs/3ay74for\" target=\"_blank\">https://wandb.ai/nishamdev/StockTrading/runs/3ay74for</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 3 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221124_212202-3ay74for/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from stocktrade import Stocktrade\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    timesteps = 100000\n",
        "    algo = \"PPO\"\n",
        "    Stocktrade.stocktrade(algo,timesteps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment#2 - PPO over 100000 steps (Hyper Parameter Tuning)\n",
        "\n",
        "**Experiment PPO algo with gamma=0.8 , learning_rate=0.0007 , ent_coef=0.3**"
      ],
      "metadata": {
        "id": "qoiN2JoSLt_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 6.2  Running PPO algorithm over 100 time steps\n",
        "from stocktrade1 import Stocktrade\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    timesteps = 100000\n",
        "    algo = \"PPO\"\n",
        "    hyper_param = True\n",
        "    Stocktrade.stocktrade(algo,timesteps,hyper_param)"
      ],
      "metadata": {
        "id": "5X5aqZqoLEet",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5656d34a-1128-457e-d096-4e3324eba7a2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnishamdev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Reinforcement-learning-Live-Trading/wandb/run-20221124_213700-gzr2j7r6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/nishamdev/StockTrading/runs/gzr2j7r6\" target=\"_blank\">fiery-cloud-2</a></strong> to <a href=\"https://wandb.ai/nishamdev/StockTrading\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
            "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Logging to runs/gzr2j7r6/PPO_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Reinforcement-learning-Live-Trading/env/stock_trading_env.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  prev_cost + additional_cost) / (self.shares_held + shares_bought)\n",
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=1000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 129  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 15   |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=3000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 3000         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.924253e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.84        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.5e+08      |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.000189    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.03e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 124  |\n",
            "|    iterations      | 2    |\n",
            "|    time_elapsed    | 32   |\n",
            "|    total_timesteps | 4096 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=5000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 5000          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7029524e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.84         |\n",
            "|    explained_variance   | 1.39e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.91e+08      |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -3.59e-05     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.11e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 122  |\n",
            "|    iterations      | 3    |\n",
            "|    time_elapsed    | 50   |\n",
            "|    total_timesteps | 6144 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=7000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 7000         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.901705e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.84        |\n",
            "|    explained_variance   | 3.7e-06      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.79e+08     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -4.49e-05    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.77e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 120  |\n",
            "|    iterations      | 4    |\n",
            "|    time_elapsed    | 67   |\n",
            "|    total_timesteps | 8192 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=9000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 9000         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.012277e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.84        |\n",
            "|    explained_variance   | 1.91e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.22e+08     |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -3e-05       |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.64e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 5     |\n",
            "|    time_elapsed    | 85    |\n",
            "|    total_timesteps | 10240 |\n",
            "------------------------------\n",
            "Eval num_timesteps=11000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 11000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.237606e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | 1.49e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.71e+08     |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.000223    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 8.35e+08     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 6     |\n",
            "|    time_elapsed    | 102   |\n",
            "|    total_timesteps | 12288 |\n",
            "------------------------------\n",
            "Eval num_timesteps=13000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 13000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.916881e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | 1.07e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.25e+08     |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.000105    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 9.67e+08     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 7     |\n",
            "|    time_elapsed    | 119   |\n",
            "|    total_timesteps | 14336 |\n",
            "------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 15000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.3318294e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 6.56e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.47e+08      |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -2.47e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.51e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 16000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 8     |\n",
            "|    time_elapsed    | 136   |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "Eval num_timesteps=17000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 17000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.1420495e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 4.17e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.26e+08      |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -1.03e-05     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.02e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 9     |\n",
            "|    time_elapsed    | 153   |\n",
            "|    total_timesteps | 18432 |\n",
            "------------------------------\n",
            "Eval num_timesteps=19000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 19000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1089433e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.95e+08      |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -4.31e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.1e+09       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 10    |\n",
            "|    time_elapsed    | 170   |\n",
            "|    total_timesteps | 20480 |\n",
            "------------------------------\n",
            "Eval num_timesteps=21000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 21000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.4110715e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.8e+08       |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.000141     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.01e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 22000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 11    |\n",
            "|    time_elapsed    | 187   |\n",
            "|    total_timesteps | 22528 |\n",
            "------------------------------\n",
            "Eval num_timesteps=23000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 23000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.1458406e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 3.58e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.82e+08      |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -1.12e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 7.16e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 24000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 12    |\n",
            "|    time_elapsed    | 204   |\n",
            "|    total_timesteps | 24576 |\n",
            "------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 25000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.221278e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.89e+08     |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.000127    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 8.97e+08     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 26000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 13    |\n",
            "|    time_elapsed    | 221   |\n",
            "|    total_timesteps | 26624 |\n",
            "------------------------------\n",
            "Eval num_timesteps=27000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 27000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1548109e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.01e+08      |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -3.9e-05      |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 8.6e+08       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 28000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 14    |\n",
            "|    time_elapsed    | 238   |\n",
            "|    total_timesteps | 28672 |\n",
            "------------------------------\n",
            "Eval num_timesteps=29000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 29000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.9606377e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.11e+08      |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -6.07e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 9.35e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 30000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 15    |\n",
            "|    time_elapsed    | 255   |\n",
            "|    total_timesteps | 30720 |\n",
            "------------------------------\n",
            "Eval num_timesteps=31000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 31000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.7034548e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.04e+08      |\n",
            "|    n_updates            | 150           |\n",
            "|    policy_gradient_loss | -7.35e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.28e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 32000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 16    |\n",
            "|    time_elapsed    | 272   |\n",
            "|    total_timesteps | 32768 |\n",
            "------------------------------\n",
            "Eval num_timesteps=33000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 33000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3627869e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.57e+08      |\n",
            "|    n_updates            | 160           |\n",
            "|    policy_gradient_loss | -3.65e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 9.68e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 34000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 17    |\n",
            "|    time_elapsed    | 289   |\n",
            "|    total_timesteps | 34816 |\n",
            "------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 35000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.936846e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.67e+08     |\n",
            "|    n_updates            | 170          |\n",
            "|    policy_gradient_loss | -0.000132    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 1.61e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 36000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 18    |\n",
            "|    time_elapsed    | 306   |\n",
            "|    total_timesteps | 36864 |\n",
            "------------------------------\n",
            "Eval num_timesteps=37000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 37000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.1484233e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.85         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.94e+08      |\n",
            "|    n_updates            | 180           |\n",
            "|    policy_gradient_loss | -0.00012      |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 6.25e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 38000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 19    |\n",
            "|    time_elapsed    | 323   |\n",
            "|    total_timesteps | 38912 |\n",
            "------------------------------\n",
            "Eval num_timesteps=39000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 39000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.477289e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.94e+08     |\n",
            "|    n_updates            | 190          |\n",
            "|    policy_gradient_loss | -6.32e-05    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 7e+08        |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 40000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 120   |\n",
            "|    iterations      | 20    |\n",
            "|    time_elapsed    | 340   |\n",
            "|    total_timesteps | 40960 |\n",
            "------------------------------\n",
            "Eval num_timesteps=41000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 41000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.0056032e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 2.48e+08      |\n",
            "|    n_updates            | 200           |\n",
            "|    policy_gradient_loss | -4.88e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 5.74e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 42000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=43000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 43000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 21    |\n",
            "|    time_elapsed    | 363   |\n",
            "|    total_timesteps | 43008 |\n",
            "------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 44000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.1579617e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.41e+08      |\n",
            "|    n_updates            | 210           |\n",
            "|    policy_gradient_loss | -6.54e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1e+09         |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 45000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 22    |\n",
            "|    time_elapsed    | 380   |\n",
            "|    total_timesteps | 45056 |\n",
            "------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 46000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7366256e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.78e+08      |\n",
            "|    n_updates            | 220           |\n",
            "|    policy_gradient_loss | -8.2e-05      |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.2e+09       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=47000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 47000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 23    |\n",
            "|    time_elapsed    | 397   |\n",
            "|    total_timesteps | 47104 |\n",
            "------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 48000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.823684e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.86        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.01e+08     |\n",
            "|    n_updates            | 230          |\n",
            "|    policy_gradient_loss | -2.29e-05    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 1.01e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=49000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 49000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 24    |\n",
            "|    time_elapsed    | 414   |\n",
            "|    total_timesteps | 49152 |\n",
            "------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 50000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.7896021e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.6e+08       |\n",
            "|    n_updates            | 240           |\n",
            "|    policy_gradient_loss | -0.000125     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 9.13e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=51000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 51000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 25    |\n",
            "|    time_elapsed    | 431   |\n",
            "|    total_timesteps | 51200 |\n",
            "------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 52000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.7808204e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.77e+08      |\n",
            "|    n_updates            | 250           |\n",
            "|    policy_gradient_loss | -0.000112     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.06e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=53000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 53000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 26    |\n",
            "|    time_elapsed    | 448   |\n",
            "|    total_timesteps | 53248 |\n",
            "------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 54000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.601723e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.86        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.02e+08     |\n",
            "|    n_updates            | 260          |\n",
            "|    policy_gradient_loss | -0.00012     |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 1.06e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=55000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 55000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 27    |\n",
            "|    time_elapsed    | 465   |\n",
            "|    total_timesteps | 55296 |\n",
            "------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 56000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.031755e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.86        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.34e+08     |\n",
            "|    n_updates            | 270          |\n",
            "|    policy_gradient_loss | -0.000163    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 6.96e+08     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=57000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 57000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 28    |\n",
            "|    time_elapsed    | 482   |\n",
            "|    total_timesteps | 57344 |\n",
            "------------------------------\n",
            "Eval num_timesteps=58000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 58000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.598399e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.86        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.7e+08      |\n",
            "|    n_updates            | 280          |\n",
            "|    policy_gradient_loss | -0.0002      |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 1.51e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=59000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 59000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 29    |\n",
            "|    time_elapsed    | 499   |\n",
            "|    total_timesteps | 59392 |\n",
            "------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 60000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.6676723e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.88e+08      |\n",
            "|    n_updates            | 290           |\n",
            "|    policy_gradient_loss | -1.35e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.28e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=61000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 61000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 30    |\n",
            "|    time_elapsed    | 516   |\n",
            "|    total_timesteps | 61440 |\n",
            "------------------------------\n",
            "Eval num_timesteps=62000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 62000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.2304303e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.22e+08      |\n",
            "|    n_updates            | 300           |\n",
            "|    policy_gradient_loss | -7.94e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.06e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=63000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 63000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 31    |\n",
            "|    time_elapsed    | 533   |\n",
            "|    total_timesteps | 63488 |\n",
            "------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 64000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0953809e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.67e+08      |\n",
            "|    n_updates            | 310           |\n",
            "|    policy_gradient_loss | -4.25e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.41e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=65000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 65000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 32    |\n",
            "|    time_elapsed    | 550   |\n",
            "|    total_timesteps | 65536 |\n",
            "------------------------------\n",
            "Eval num_timesteps=66000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 66000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.3624743e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.86         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.18e+08      |\n",
            "|    n_updates            | 320           |\n",
            "|    policy_gradient_loss | -6.77e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 9.68e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=67000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 67000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 119   |\n",
            "|    iterations      | 33    |\n",
            "|    time_elapsed    | 567   |\n",
            "|    total_timesteps | 67584 |\n",
            "------------------------------\n",
            "Eval num_timesteps=68000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 68000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.9362272e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.97e+08      |\n",
            "|    n_updates            | 330           |\n",
            "|    policy_gradient_loss | -8.59e-05     |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 1.12e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=69000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 69000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 119   |\n",
            "|    iterations      | 34    |\n",
            "|    time_elapsed    | 584   |\n",
            "|    total_timesteps | 69632 |\n",
            "------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 70000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.0065054e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.11e+08      |\n",
            "|    n_updates            | 340           |\n",
            "|    policy_gradient_loss | -8.29e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 9.28e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=71000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 71000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 119   |\n",
            "|    iterations      | 35    |\n",
            "|    time_elapsed    | 601   |\n",
            "|    total_timesteps | 71680 |\n",
            "------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 72000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0952936e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.94e+08      |\n",
            "|    n_updates            | 350           |\n",
            "|    policy_gradient_loss | -4.43e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.06e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=73000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 73000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 119   |\n",
            "|    iterations      | 36    |\n",
            "|    time_elapsed    | 618   |\n",
            "|    total_timesteps | 73728 |\n",
            "------------------------------\n",
            "Eval num_timesteps=74000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 74000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.6975467e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.67e+08      |\n",
            "|    n_updates            | 360           |\n",
            "|    policy_gradient_loss | -6.61e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 9.24e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=75000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 75000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 119   |\n",
            "|    iterations      | 37    |\n",
            "|    time_elapsed    | 636   |\n",
            "|    total_timesteps | 75776 |\n",
            "------------------------------\n",
            "Eval num_timesteps=76000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 76000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.4274354e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.11e+08      |\n",
            "|    n_updates            | 370           |\n",
            "|    policy_gradient_loss | -0.00013      |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 6.4e+08       |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=77000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 77000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 119   |\n",
            "|    iterations      | 38    |\n",
            "|    time_elapsed    | 653   |\n",
            "|    total_timesteps | 77824 |\n",
            "------------------------------\n",
            "Eval num_timesteps=78000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 78000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.567955e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.87        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.91e+08     |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | -0.000139    |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 1.24e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=79000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 79000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 119   |\n",
            "|    iterations      | 39    |\n",
            "|    time_elapsed    | 670   |\n",
            "|    total_timesteps | 79872 |\n",
            "------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 80000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.2132008e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.87         |\n",
            "|    explained_variance   | -2.38e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.59e+08      |\n",
            "|    n_updates            | 390           |\n",
            "|    policy_gradient_loss | -4.94e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 8.56e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=81000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 81000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 119   |\n",
            "|    iterations      | 40    |\n",
            "|    time_elapsed    | 687   |\n",
            "|    total_timesteps | 81920 |\n",
            "------------------------------\n",
            "Eval num_timesteps=82000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 82000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.433503e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.87        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.67e+08     |\n",
            "|    n_updates            | 400          |\n",
            "|    policy_gradient_loss | -0.000102    |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 1.37e+09     |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=83000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 83000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 119   |\n",
            "|    iterations      | 41    |\n",
            "|    time_elapsed    | 704   |\n",
            "|    total_timesteps | 83968 |\n",
            "------------------------------\n",
            "Eval num_timesteps=84000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 84000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2741366e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.88         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.03e+08      |\n",
            "|    n_updates            | 410           |\n",
            "|    policy_gradient_loss | -3.38e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 9.87e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=85000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 85000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=86000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 86000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 42    |\n",
            "|    time_elapsed    | 728   |\n",
            "|    total_timesteps | 86016 |\n",
            "------------------------------\n",
            "Eval num_timesteps=87000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 87000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5736441e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.88         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.94e+08      |\n",
            "|    n_updates            | 420           |\n",
            "|    policy_gradient_loss | -4.95e-06     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.27e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 88000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 43    |\n",
            "|    time_elapsed    | 745   |\n",
            "|    total_timesteps | 88064 |\n",
            "------------------------------\n",
            "Eval num_timesteps=89000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 89000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.0506635e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.88         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.1e+08       |\n",
            "|    n_updates            | 430           |\n",
            "|    policy_gradient_loss | -9.16e-06     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.46e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 90000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 44    |\n",
            "|    time_elapsed    | 762   |\n",
            "|    total_timesteps | 90112 |\n",
            "------------------------------\n",
            "Eval num_timesteps=91000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 91000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 8.4526255e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.88         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.74e+08      |\n",
            "|    n_updates            | 440           |\n",
            "|    policy_gradient_loss | -3.76e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.54e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=92000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 45    |\n",
            "|    time_elapsed    | 779   |\n",
            "|    total_timesteps | 92160 |\n",
            "------------------------------\n",
            "Eval num_timesteps=93000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | 3e+06        |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 93000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.309041e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.88        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.23e+08     |\n",
            "|    n_updates            | 450          |\n",
            "|    policy_gradient_loss | -0.000173    |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 1.1e+09      |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=94000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 46    |\n",
            "|    time_elapsed    | 796   |\n",
            "|    total_timesteps | 94208 |\n",
            "------------------------------\n",
            "Eval num_timesteps=95000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 95000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3450044e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.88         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.48e+08      |\n",
            "|    n_updates            | 460           |\n",
            "|    policy_gradient_loss | -4.65e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.16e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 47    |\n",
            "|    time_elapsed    | 813   |\n",
            "|    total_timesteps | 96256 |\n",
            "------------------------------\n",
            "Eval num_timesteps=97000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 97000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.2505777e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.88         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.86e+08      |\n",
            "|    n_updates            | 470           |\n",
            "|    policy_gradient_loss | -9.88e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 8.62e+08      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=98000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 118   |\n",
            "|    iterations      | 48    |\n",
            "|    time_elapsed    | 830   |\n",
            "|    total_timesteps | 98304 |\n",
            "------------------------------\n",
            "Eval num_timesteps=99000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 1e+03         |\n",
            "|    mean_reward          | 3e+06         |\n",
            "| time/                   |               |\n",
            "|    total_timesteps      | 99000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0876684e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.88         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.54e+08      |\n",
            "|    n_updates            | 480           |\n",
            "|    policy_gradient_loss | -3.25e-05     |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 1.23e+09      |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 118    |\n",
            "|    iterations      | 49     |\n",
            "|    time_elapsed    | 846    |\n",
            "|    total_timesteps | 100352 |\n",
            "-------------------------------\n",
            "mean_reward:3003000.00 +/- 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x576 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mean_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/fps</td><td>█▅▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/approx_kl</td><td>█▂▂▁▇▁▁▂▅▁▂▂▃▂▄▃▂▃▂▁▄▄▇▇▁▂▃▃▂▂▆▄▃▄▂▁▁▆▂▂</td></tr><tr><td>train/clip_fraction</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/clip_range</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/entropy_loss</td><td>██▇▇▇▇▇▇▇▆▇▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/explained_variance</td><td>▁█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▂▁█▇▃█▃▅▆▃▄▃▆▃▇▂▂▅▄▂▄▄▁▅▅▆▃▄▄▄▃▅▃▄▄▅▅▃▄▂</td></tr><tr><td>train/policy_gradient_loss</td><td>▂▇▇▇▁▇█▇▄█▇▆▆▇▄▆▇▆▆█▅▄▃▂█▇▆▅▆▇▄▄▇▅▇█▇▃▇▇</td></tr><tr><td>train/std</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/value_loss</td><td>▄▂█▇▃▆▄▄▄▂▃▃▅▃▇▂▁▄▅▄▄▄▂▆▅▆▃▄▃▄▁▅▃▆▃▆▇▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>1000.0</td></tr><tr><td>eval/mean_reward</td><td>3003000.0</td></tr><tr><td>global_step</td><td>100352</td></tr><tr><td>time/fps</td><td>118.0</td></tr><tr><td>train/approx_kl</td><td>0.0</td></tr><tr><td>train/clip_fraction</td><td>0.0</td></tr><tr><td>train/clip_range</td><td>0.2</td></tr><tr><td>train/entropy_loss</td><td>-2.88094</td></tr><tr><td>train/explained_variance</td><td>0.0</td></tr><tr><td>train/learning_rate</td><td>0.0003</td></tr><tr><td>train/loss</td><td>353603904.0</td></tr><tr><td>train/policy_gradient_loss</td><td>-3e-05</td></tr><tr><td>train/std</td><td>1.02212</td></tr><tr><td>train/value_loss</td><td>1225394944.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">fiery-cloud-2</strong>: <a href=\"https://wandb.ai/nishamdev/StockTrading/runs/gzr2j7r6\" target=\"_blank\">https://wandb.ai/nishamdev/StockTrading/runs/gzr2j7r6</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 3 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221124_213700-gzr2j7r6/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9hAAlYOijs-"
      },
      "source": [
        "# B - A2C Algorithm - Advantage Actor Critic (A2C)\n",
        "\n",
        "A2C is a policy gradient algorithm and it is part of the on-policy family. That means that we are learning the value function for one policy while following it, or in other words, we can’t learn the value function by following another policy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment#3 - Running A2C algorithm over 100 time steps\n",
        "\n",
        "Experiment A2C algo with with default params - gamma=0.99 , learning_rate=0.0007 , ent_coef=0.4"
      ],
      "metadata": {
        "id": "ilSaJremhBt4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M4TOfiFWijs-",
        "outputId": "1f69ec2c-9df8-42c9-db37-eadc558e8f17"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Reinforcement-learning-Live-Trading/wandb/run-20221124_215606-16thhgmp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/nishamdev/StockTrading/runs/16thhgmp\" target=\"_blank\">stellar-haze-7</a></strong> to <a href=\"https://wandb.ai/nishamdev/StockTrading\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
            "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Logging to runs/16thhgmp/A2C_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Reinforcement-learning-Live-Trading/env/stock_trading_env.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  prev_cost + additional_cost) / (self.shares_held + shares_bought)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 385       |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.81     |\n",
            "|    explained_variance | -4.89e-06 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | 1.61e+04  |\n",
            "|    std                | 0.986     |\n",
            "|    value_loss         | 4.5e+07   |\n",
            "-------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=1000, episode_reward=1989844.57 +/- 4205.26\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 1.99e+06  |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.8      |\n",
            "|    explained_variance | -9.54e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 6.15e+03  |\n",
            "|    std                | 0.98      |\n",
            "|    value_loss         | 1.14e+07  |\n",
            "-------------------------------------\n",
            "New best mean reward!\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 108  |\n",
            "|    iterations      | 200  |\n",
            "|    time_elapsed    | 9    |\n",
            "|    total_timesteps | 1000 |\n",
            "-----------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 143       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.79     |\n",
            "|    explained_variance | -7.15e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 5.32e+03  |\n",
            "|    std                | 0.977     |\n",
            "|    value_loss         | 4.52e+06  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=601023.59 +/- 3288.73\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 6.01e+05  |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.79     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 7.29e+03  |\n",
            "|    std                | 0.975     |\n",
            "|    value_loss         | 1.13e+07  |\n",
            "-------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 112  |\n",
            "|    iterations      | 400  |\n",
            "|    time_elapsed    | 17   |\n",
            "|    total_timesteps | 2000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 131      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.77    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 3.44e+04 |\n",
            "|    std                | 0.968    |\n",
            "|    value_loss         | 2.46e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=3000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.74     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 1.89e+04  |\n",
            "|    std                | 0.954     |\n",
            "|    value_loss         | 1.28e+08  |\n",
            "-------------------------------------\n",
            "New best mean reward!\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 113  |\n",
            "|    iterations      | 600  |\n",
            "|    time_elapsed    | 26   |\n",
            "|    total_timesteps | 3000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 126      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.74    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 5.34e+03 |\n",
            "|    std                | 0.953    |\n",
            "|    value_loss         | 4.15e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=559036.19 +/- 1991.39\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 5.59e+05  |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.72     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 9.96e+04  |\n",
            "|    std                | 0.945     |\n",
            "|    value_loss         | 8.22e+08  |\n",
            "-------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 113  |\n",
            "|    iterations      | 800  |\n",
            "|    time_elapsed    | 35   |\n",
            "|    total_timesteps | 4000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 123      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 36       |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.7     |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | 3.43e+04 |\n",
            "|    std                | 0.935    |\n",
            "|    value_loss         | 2.03e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=5000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.69    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 6.22e+04 |\n",
            "|    std                | 0.93     |\n",
            "|    value_loss         | 9.68e+08 |\n",
            "------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 114  |\n",
            "|    iterations      | 1000 |\n",
            "|    time_elapsed    | 43   |\n",
            "|    total_timesteps | 5000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 122      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 45       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.67    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 2.59e+04 |\n",
            "|    std                | 0.919    |\n",
            "|    value_loss         | 1.09e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.66    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 5.41e+04 |\n",
            "|    std                | 0.916    |\n",
            "|    value_loss         | 5.12e+08 |\n",
            "------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 114  |\n",
            "|    iterations      | 1200 |\n",
            "|    time_elapsed    | 52   |\n",
            "|    total_timesteps | 6000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 120      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 53       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.65    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 8.04e+03 |\n",
            "|    std                | 0.911    |\n",
            "|    value_loss         | 2.69e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=7000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.65     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 8.55e+04  |\n",
            "|    std                | 0.913     |\n",
            "|    value_loss         | 1.09e+09  |\n",
            "-------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 114  |\n",
            "|    iterations      | 1400 |\n",
            "|    time_elapsed    | 61   |\n",
            "|    total_timesteps | 7000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 119      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 62       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.64    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 3.13e+04 |\n",
            "|    std                | 0.908    |\n",
            "|    value_loss         | 9.9e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.62    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 5.91e+04 |\n",
            "|    std                | 0.897    |\n",
            "|    value_loss         | 4.85e+08 |\n",
            "------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 114  |\n",
            "|    iterations      | 1600 |\n",
            "|    time_elapsed    | 69   |\n",
            "|    total_timesteps | 8000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 119      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 71       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.61    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 2.28e+04 |\n",
            "|    std                | 0.892    |\n",
            "|    value_loss         | 1.25e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=9000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.6     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 1.2e+04  |\n",
            "|    std                | 0.889    |\n",
            "|    value_loss         | 3.33e+07 |\n",
            "------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 114  |\n",
            "|    iterations      | 1800 |\n",
            "|    time_elapsed    | 78   |\n",
            "|    total_timesteps | 9000 |\n",
            "-----------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 79        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.58     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 2.76e+04  |\n",
            "|    std                | 0.878     |\n",
            "|    value_loss         | 1.58e+08  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.57    |\n",
            "|    explained_variance | 3.58e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 3.71e+04 |\n",
            "|    std                | 0.874    |\n",
            "|    value_loss         | 2.65e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 114   |\n",
            "|    iterations      | 2000  |\n",
            "|    time_elapsed    | 87    |\n",
            "|    total_timesteps | 10000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 88        |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.57     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 2.05e+04  |\n",
            "|    std                | 0.877     |\n",
            "|    value_loss         | 7.6e+07   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=11000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.57     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 4.69e+04  |\n",
            "|    std                | 0.874     |\n",
            "|    value_loss         | 3.31e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 114   |\n",
            "|    iterations      | 2200  |\n",
            "|    time_elapsed    | 95    |\n",
            "|    total_timesteps | 11000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 97       |\n",
            "|    total_timesteps    | 11500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.56    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | 1.8e+04  |\n",
            "|    std                | 0.872    |\n",
            "|    value_loss         | 7.95e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.57    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | 5.86e+04 |\n",
            "|    std                | 0.875    |\n",
            "|    value_loss         | 5.71e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 114   |\n",
            "|    iterations      | 2400  |\n",
            "|    time_elapsed    | 104   |\n",
            "|    total_timesteps | 12000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 105       |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.56     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 3.38e+04  |\n",
            "|    std                | 0.872     |\n",
            "|    value_loss         | 1.16e+08  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=13000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.54    |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | 5.05e+04 |\n",
            "|    std                | 0.863    |\n",
            "|    value_loss         | 4.11e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 114   |\n",
            "|    iterations      | 2600  |\n",
            "|    time_elapsed    | 113   |\n",
            "|    total_timesteps | 13000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 2700     |\n",
            "|    time_elapsed       | 114      |\n",
            "|    total_timesteps    | 13500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.54    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2699     |\n",
            "|    policy_loss        | 1.76e+04 |\n",
            "|    std                | 0.86     |\n",
            "|    value_loss         | 9.34e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.52    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | 4.61e+04 |\n",
            "|    std                | 0.852    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 114   |\n",
            "|    iterations      | 2800  |\n",
            "|    time_elapsed    | 121   |\n",
            "|    total_timesteps | 14000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 123      |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.5     |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | 2.83e+04 |\n",
            "|    std                | 0.847    |\n",
            "|    value_loss         | 1.44e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.49     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 4.52e+04  |\n",
            "|    std                | 0.84      |\n",
            "|    value_loss         | 6.03e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 114   |\n",
            "|    iterations      | 3000  |\n",
            "|    time_elapsed    | 130   |\n",
            "|    total_timesteps | 15000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 131      |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.47    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | 2.02e+04 |\n",
            "|    std                | 0.833    |\n",
            "|    value_loss         | 1.08e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.45     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 6.23e+04  |\n",
            "|    std                | 0.824     |\n",
            "|    value_loss         | 5.85e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 114   |\n",
            "|    iterations      | 3200  |\n",
            "|    time_elapsed    | 139   |\n",
            "|    total_timesteps | 16000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 140      |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.44    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 3.34e+04 |\n",
            "|    std                | 0.823    |\n",
            "|    value_loss         | 1.06e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=17000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.43     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 3.28e+04  |\n",
            "|    std                | 0.816     |\n",
            "|    value_loss         | 4.37e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 3400  |\n",
            "|    time_elapsed    | 147   |\n",
            "|    total_timesteps | 17000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 148      |\n",
            "|    total_timesteps    | 17500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.41    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | 2.42e+04 |\n",
            "|    std                | 0.808    |\n",
            "|    value_loss         | 1.02e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.39    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | 9.09e+04 |\n",
            "|    std                | 0.801    |\n",
            "|    value_loss         | 8.28e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 3600  |\n",
            "|    time_elapsed    | 156   |\n",
            "|    total_timesteps | 18000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 157       |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.37     |\n",
            "|    explained_variance | -3.58e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 2.34e+04  |\n",
            "|    std                | 0.793     |\n",
            "|    value_loss         | 1.96e+08  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=19000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.36    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 4.84e+04 |\n",
            "|    std                | 0.788    |\n",
            "|    value_loss         | 8.07e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 3800  |\n",
            "|    time_elapsed    | 164   |\n",
            "|    total_timesteps | 19000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 166       |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.35     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | 1.46e+04  |\n",
            "|    std                | 0.786     |\n",
            "|    value_loss         | 9.34e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.35    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | 4.21e+04 |\n",
            "|    std                | 0.784    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 4000  |\n",
            "|    time_elapsed    | 173   |\n",
            "|    total_timesteps | 20000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 4100      |\n",
            "|    time_elapsed       | 174       |\n",
            "|    total_timesteps    | 20500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.33     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4099      |\n",
            "|    policy_loss        | 1.59e+04  |\n",
            "|    std                | 0.78      |\n",
            "|    value_loss         | 9.34e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=21000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.32    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 3.72e+04 |\n",
            "|    std                | 0.773    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 4200  |\n",
            "|    time_elapsed    | 182   |\n",
            "|    total_timesteps | 21000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 183      |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.32    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | 1.88e+04 |\n",
            "|    std                | 0.773    |\n",
            "|    value_loss         | 9.34e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 22000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.32    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | 5.52e+04 |\n",
            "|    std                | 0.774    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 4400  |\n",
            "|    time_elapsed    | 190   |\n",
            "|    total_timesteps | 22000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 192      |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.3     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 1.95e+04 |\n",
            "|    std                | 0.768    |\n",
            "|    value_loss         | 9.34e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=23000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.29    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | 3.1e+04  |\n",
            "|    std                | 0.763    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 4600  |\n",
            "|    time_elapsed    | 199   |\n",
            "|    total_timesteps | 23000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 200      |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.26    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | 2.16e+04 |\n",
            "|    std                | 0.753    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 24000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.25     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4799      |\n",
            "|    policy_loss        | 5.77e+04  |\n",
            "|    std                | 0.748     |\n",
            "|    value_loss         | 3.79e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 4800  |\n",
            "|    time_elapsed    | 208   |\n",
            "|    total_timesteps | 24000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 209      |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.23    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 1.95e+04 |\n",
            "|    std                | 0.74     |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.23    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | 3.85e+04 |\n",
            "|    std                | 0.742    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 5000  |\n",
            "|    time_elapsed    | 216   |\n",
            "|    total_timesteps | 25000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 218       |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.22     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | 3.18e+04  |\n",
            "|    std                | 0.74      |\n",
            "|    value_loss         | 9.33e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.22    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | 5.34e+04 |\n",
            "|    std                | 0.741    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 5200  |\n",
            "|    time_elapsed    | 225   |\n",
            "|    total_timesteps | 26000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 227      |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.2     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | 1.95e+04 |\n",
            "|    std                | 0.733    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=27000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.18    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | 6.37e+04 |\n",
            "|    std                | 0.725    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 5400  |\n",
            "|    time_elapsed    | 234   |\n",
            "|    total_timesteps | 27000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 235      |\n",
            "|    total_timesteps    | 27500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.18    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | 1.63e+04 |\n",
            "|    std                | 0.725    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.18     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | 4.79e+04  |\n",
            "|    std                | 0.724     |\n",
            "|    value_loss         | 3.79e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 5600  |\n",
            "|    time_elapsed    | 243   |\n",
            "|    total_timesteps | 28000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 244      |\n",
            "|    total_timesteps    | 28500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.16    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | 1.8e+04  |\n",
            "|    std                | 0.717    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=29000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.15     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 3.31e+04  |\n",
            "|    std                | 0.715     |\n",
            "|    value_loss         | 3.79e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 5800  |\n",
            "|    time_elapsed    | 251   |\n",
            "|    total_timesteps | 29000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 253       |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.13     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 2.84e+04  |\n",
            "|    std                | 0.706     |\n",
            "|    value_loss         | 9.33e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.12    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | 3.96e+04 |\n",
            "|    std                | 0.701    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 6000  |\n",
            "|    time_elapsed    | 260   |\n",
            "|    total_timesteps | 30000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 261       |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.1      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | 2.54e+04  |\n",
            "|    std                | 0.697     |\n",
            "|    value_loss         | 9.33e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=31000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 31000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.11    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6199     |\n",
            "|    policy_loss        | 3.73e+04 |\n",
            "|    std                | 0.699    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 6200  |\n",
            "|    time_elapsed    | 268   |\n",
            "|    total_timesteps | 31000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 6300     |\n",
            "|    time_elapsed       | 270      |\n",
            "|    total_timesteps    | 31500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.12    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6299     |\n",
            "|    policy_loss        | 1.61e+04 |\n",
            "|    std                | 0.7      |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 32000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.11    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6399     |\n",
            "|    policy_loss        | 3.01e+04 |\n",
            "|    std                | 0.697    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 6400  |\n",
            "|    time_elapsed    | 277   |\n",
            "|    total_timesteps | 32000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 6500     |\n",
            "|    time_elapsed       | 278      |\n",
            "|    total_timesteps    | 32500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.09    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6499     |\n",
            "|    policy_loss        | 1.8e+04  |\n",
            "|    std                | 0.692    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=33000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 33000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.08    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6599     |\n",
            "|    policy_loss        | 3.07e+04 |\n",
            "|    std                | 0.689    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 6600  |\n",
            "|    time_elapsed    | 286   |\n",
            "|    total_timesteps | 33000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 6700     |\n",
            "|    time_elapsed       | 287      |\n",
            "|    total_timesteps    | 33500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.06    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6699     |\n",
            "|    policy_loss        | 1.82e+04 |\n",
            "|    std                | 0.683    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.05     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | 3.06e+04  |\n",
            "|    std                | 0.679     |\n",
            "|    value_loss         | 3.79e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 6800  |\n",
            "|    time_elapsed    | 294   |\n",
            "|    total_timesteps | 34000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 6900     |\n",
            "|    time_elapsed       | 296      |\n",
            "|    total_timesteps    | 34500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.04    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6899     |\n",
            "|    policy_loss        | 2.12e+04 |\n",
            "|    std                | 0.676    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 35000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.02     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6999      |\n",
            "|    policy_loss        | 3.49e+04  |\n",
            "|    std                | 0.667     |\n",
            "|    value_loss         | 3.79e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 7000  |\n",
            "|    time_elapsed    | 303   |\n",
            "|    total_timesteps | 35000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 7100     |\n",
            "|    time_elapsed       | 304      |\n",
            "|    total_timesteps    | 35500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2       |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7099     |\n",
            "|    policy_loss        | 1.27e+04 |\n",
            "|    std                | 0.662    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 36000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.98    |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7199     |\n",
            "|    policy_loss        | 3.11e+04 |\n",
            "|    std                | 0.653    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 7200  |\n",
            "|    time_elapsed    | 312   |\n",
            "|    total_timesteps | 36000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 7300     |\n",
            "|    time_elapsed       | 313      |\n",
            "|    total_timesteps    | 36500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.96    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7299     |\n",
            "|    policy_loss        | 2.05e+04 |\n",
            "|    std                | 0.648    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=37000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 37000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.95    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7399     |\n",
            "|    policy_loss        | 2.93e+04 |\n",
            "|    std                | 0.645    |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 7400  |\n",
            "|    time_elapsed    | 320   |\n",
            "|    total_timesteps | 37000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 7500     |\n",
            "|    time_elapsed       | 321      |\n",
            "|    total_timesteps    | 37500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.92    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7499     |\n",
            "|    policy_loss        | 1.89e+04 |\n",
            "|    std                | 0.636    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 38000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.91    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7599     |\n",
            "|    policy_loss        | 2.42e+04 |\n",
            "|    std                | 0.632    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 7600  |\n",
            "|    time_elapsed    | 329   |\n",
            "|    total_timesteps | 38000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 330       |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.91     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | 2.16e+04  |\n",
            "|    std                | 0.633     |\n",
            "|    value_loss         | 9.33e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=39000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 39000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.89     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7799      |\n",
            "|    policy_loss        | 2.26e+04  |\n",
            "|    std                | 0.626     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 7800  |\n",
            "|    time_elapsed    | 337   |\n",
            "|    total_timesteps | 39000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 7900     |\n",
            "|    time_elapsed       | 339      |\n",
            "|    total_timesteps    | 39500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.89    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7899     |\n",
            "|    policy_loss        | 1.69e+04 |\n",
            "|    std                | 0.628    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 40000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.88    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7999     |\n",
            "|    policy_loss        | 2e+04    |\n",
            "|    std                | 0.624    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 8000  |\n",
            "|    time_elapsed    | 346   |\n",
            "|    total_timesteps | 40000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 347       |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.9      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | 1.67e+04  |\n",
            "|    std                | 0.63      |\n",
            "|    value_loss         | 9.33e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=41000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 41000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.89    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8199     |\n",
            "|    policy_loss        | 3.93e+04 |\n",
            "|    std                | 0.629    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 8200  |\n",
            "|    time_elapsed    | 355   |\n",
            "|    total_timesteps | 41000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 8300     |\n",
            "|    time_elapsed       | 356      |\n",
            "|    total_timesteps    | 41500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.9     |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8299     |\n",
            "|    policy_loss        | 1.26e+04 |\n",
            "|    std                | 0.63     |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 42000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.89    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8399     |\n",
            "|    policy_loss        | 2.99e+04 |\n",
            "|    std                | 0.628    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 8400  |\n",
            "|    time_elapsed    | 363   |\n",
            "|    total_timesteps | 42000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 8500     |\n",
            "|    time_elapsed       | 365      |\n",
            "|    total_timesteps    | 42500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.88    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8499     |\n",
            "|    policy_loss        | 2.23e+04 |\n",
            "|    std                | 0.626    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=43000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 43000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.9     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8599     |\n",
            "|    policy_loss        | 3.39e+04 |\n",
            "|    std                | 0.629    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 8600  |\n",
            "|    time_elapsed    | 372   |\n",
            "|    total_timesteps | 43000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 8700     |\n",
            "|    time_elapsed       | 373      |\n",
            "|    total_timesteps    | 43500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.88    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8699     |\n",
            "|    policy_loss        | 1.39e+04 |\n",
            "|    std                | 0.625    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 44000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.88    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8799     |\n",
            "|    policy_loss        | 3.51e+04 |\n",
            "|    std                | 0.625    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 8800  |\n",
            "|    time_elapsed    | 381   |\n",
            "|    total_timesteps | 44000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 8900     |\n",
            "|    time_elapsed       | 382      |\n",
            "|    total_timesteps    | 44500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.89    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8899     |\n",
            "|    policy_loss        | 1.75e+04 |\n",
            "|    std                | 0.63     |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 45000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.88     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8999      |\n",
            "|    policy_loss        | 2.55e+04  |\n",
            "|    std                | 0.627     |\n",
            "|    value_loss         | 3.75e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 9000  |\n",
            "|    time_elapsed    | 389   |\n",
            "|    total_timesteps | 45000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 9100     |\n",
            "|    time_elapsed       | 390      |\n",
            "|    total_timesteps    | 45500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.89    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9099     |\n",
            "|    policy_loss        | 2.23e+04 |\n",
            "|    std                | 0.628    |\n",
            "|    value_loss         | 8.56e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 46000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.9     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9199     |\n",
            "|    policy_loss        | 4.45e+04 |\n",
            "|    std                | 0.632    |\n",
            "|    value_loss         | 3.23e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 9200  |\n",
            "|    time_elapsed    | 398   |\n",
            "|    total_timesteps | 46000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 9300     |\n",
            "|    time_elapsed       | 399      |\n",
            "|    total_timesteps    | 46500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.89    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9299     |\n",
            "|    policy_loss        | 1.57e+04 |\n",
            "|    std                | 0.628    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=47000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 47000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.87    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9399     |\n",
            "|    policy_loss        | 3.01e+04 |\n",
            "|    std                | 0.622    |\n",
            "|    value_loss         | 4.51e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 9400  |\n",
            "|    time_elapsed    | 406   |\n",
            "|    total_timesteps | 47000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 9500     |\n",
            "|    time_elapsed       | 408      |\n",
            "|    total_timesteps    | 47500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.87    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9499     |\n",
            "|    policy_loss        | 1.49e+04 |\n",
            "|    std                | 0.622    |\n",
            "|    value_loss         | 1.07e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 48000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.87    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9599     |\n",
            "|    policy_loss        | 3.75e+04 |\n",
            "|    std                | 0.622    |\n",
            "|    value_loss         | 4.32e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 9600  |\n",
            "|    time_elapsed    | 415   |\n",
            "|    total_timesteps | 48000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 9700     |\n",
            "|    time_elapsed       | 416      |\n",
            "|    total_timesteps    | 48500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.85    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9699     |\n",
            "|    policy_loss        | 1.25e+04 |\n",
            "|    std                | 0.617    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=49000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 49000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.82    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9799     |\n",
            "|    policy_loss        | 2.15e+04 |\n",
            "|    std                | 0.607    |\n",
            "|    value_loss         | 3.15e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 9800  |\n",
            "|    time_elapsed    | 424   |\n",
            "|    total_timesteps | 49000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 9900     |\n",
            "|    time_elapsed       | 425      |\n",
            "|    total_timesteps    | 49500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.81    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9899     |\n",
            "|    policy_loss        | 2.28e+04 |\n",
            "|    std                | 0.605    |\n",
            "|    value_loss         | 1.03e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 50000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.82     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9999      |\n",
            "|    policy_loss        | 5.17e+04  |\n",
            "|    std                | 0.608     |\n",
            "|    value_loss         | 4.17e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 10000 |\n",
            "|    time_elapsed    | 433   |\n",
            "|    total_timesteps | 50000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 10100    |\n",
            "|    time_elapsed       | 434      |\n",
            "|    total_timesteps    | 50500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.81    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10099    |\n",
            "|    policy_loss        | 1.37e+04 |\n",
            "|    std                | 0.603    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=51000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 51000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.8     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10199    |\n",
            "|    policy_loss        | 3.9e+04  |\n",
            "|    std                | 0.602    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 10200 |\n",
            "|    time_elapsed    | 442   |\n",
            "|    total_timesteps | 51000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 10300    |\n",
            "|    time_elapsed       | 443      |\n",
            "|    total_timesteps    | 51500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.8     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10299    |\n",
            "|    policy_loss        | 1.39e+04 |\n",
            "|    std                | 0.602    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 52000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.8     |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10399    |\n",
            "|    policy_loss        | 2.63e+04 |\n",
            "|    std                | 0.603    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 10400 |\n",
            "|    time_elapsed    | 450   |\n",
            "|    total_timesteps | 52000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 10500     |\n",
            "|    time_elapsed       | 452       |\n",
            "|    total_timesteps    | 52500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.79     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10499     |\n",
            "|    policy_loss        | 1.46e+04  |\n",
            "|    std                | 0.6       |\n",
            "|    value_loss         | 9.32e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=53000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 53000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.77     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10599     |\n",
            "|    policy_loss        | 2.96e+04  |\n",
            "|    std                | 0.593     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 10600 |\n",
            "|    time_elapsed    | 459   |\n",
            "|    total_timesteps | 53000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 10700    |\n",
            "|    time_elapsed       | 460      |\n",
            "|    total_timesteps    | 53500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.77    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10699    |\n",
            "|    policy_loss        | 1.7e+04  |\n",
            "|    std                | 0.594    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 54000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.75    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10799    |\n",
            "|    policy_loss        | 5.02e+04 |\n",
            "|    std                | 0.589    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 10800 |\n",
            "|    time_elapsed    | 468   |\n",
            "|    total_timesteps | 54000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 10900     |\n",
            "|    time_elapsed       | 469       |\n",
            "|    total_timesteps    | 54500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.74     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10899     |\n",
            "|    policy_loss        | 1.18e+04  |\n",
            "|    std                | 0.587     |\n",
            "|    value_loss         | 9.32e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=55000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 55000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.73    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10999    |\n",
            "|    policy_loss        | 2.39e+04 |\n",
            "|    std                | 0.583    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 11000 |\n",
            "|    time_elapsed    | 476   |\n",
            "|    total_timesteps | 55000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 11100     |\n",
            "|    time_elapsed       | 477       |\n",
            "|    total_timesteps    | 55500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.73     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11099     |\n",
            "|    policy_loss        | 2.16e+04  |\n",
            "|    std                | 0.586     |\n",
            "|    value_loss         | 9.32e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 56000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.72     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11199     |\n",
            "|    policy_loss        | 3.54e+04  |\n",
            "|    std                | 0.583     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 11200 |\n",
            "|    time_elapsed    | 485   |\n",
            "|    total_timesteps | 56000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 11300    |\n",
            "|    time_elapsed       | 486      |\n",
            "|    total_timesteps    | 56500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.71    |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11299    |\n",
            "|    policy_loss        | 1.35e+04 |\n",
            "|    std                | 0.581    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=57000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 57000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.72    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11399    |\n",
            "|    policy_loss        | 2.07e+04 |\n",
            "|    std                | 0.582    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 11400 |\n",
            "|    time_elapsed    | 493   |\n",
            "|    total_timesteps | 57000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 11500    |\n",
            "|    time_elapsed       | 495      |\n",
            "|    total_timesteps    | 57500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.7     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11499    |\n",
            "|    policy_loss        | 1.88e+04 |\n",
            "|    std                | 0.577    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=58000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 58000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.7     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11599    |\n",
            "|    policy_loss        | 2.83e+04 |\n",
            "|    std                | 0.575    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 11600 |\n",
            "|    time_elapsed    | 502   |\n",
            "|    total_timesteps | 58000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 11700    |\n",
            "|    time_elapsed       | 503      |\n",
            "|    total_timesteps    | 58500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.7     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11699    |\n",
            "|    policy_loss        | 2.4e+04  |\n",
            "|    std                | 0.576    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=59000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 59000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.68     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11799     |\n",
            "|    policy_loss        | 1.72e+04  |\n",
            "|    std                | 0.57      |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 11800 |\n",
            "|    time_elapsed    | 510   |\n",
            "|    total_timesteps | 59000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 11900     |\n",
            "|    time_elapsed       | 512       |\n",
            "|    total_timesteps    | 59500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.66     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11899     |\n",
            "|    policy_loss        | 1.51e+04  |\n",
            "|    std                | 0.564     |\n",
            "|    value_loss         | 9.31e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 60000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.65    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11999    |\n",
            "|    policy_loss        | 3.84e+04 |\n",
            "|    std                | 0.561    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 12000 |\n",
            "|    time_elapsed    | 519   |\n",
            "|    total_timesteps | 60000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 12100    |\n",
            "|    time_elapsed       | 520      |\n",
            "|    total_timesteps    | 60500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.63    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12099    |\n",
            "|    policy_loss        | 1.43e+04 |\n",
            "|    std                | 0.556    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=61000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 61000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.62    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12199    |\n",
            "|    policy_loss        | 1.61e+04 |\n",
            "|    std                | 0.551    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 12200 |\n",
            "|    time_elapsed    | 528   |\n",
            "|    total_timesteps | 61000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 12300    |\n",
            "|    time_elapsed       | 529      |\n",
            "|    total_timesteps    | 61500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.61    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12299    |\n",
            "|    policy_loss        | 1.77e+04 |\n",
            "|    std                | 0.551    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=62000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 62000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.6     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12399    |\n",
            "|    policy_loss        | 3.08e+04 |\n",
            "|    std                | 0.549    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 12400 |\n",
            "|    time_elapsed    | 536   |\n",
            "|    total_timesteps | 62000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 12500     |\n",
            "|    time_elapsed       | 538       |\n",
            "|    total_timesteps    | 62500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.6      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12499     |\n",
            "|    policy_loss        | 1.11e+04  |\n",
            "|    std                | 0.549     |\n",
            "|    value_loss         | 9.31e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=63000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 63000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.6      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12599     |\n",
            "|    policy_loss        | 2.32e+04  |\n",
            "|    std                | 0.549     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 12600 |\n",
            "|    time_elapsed    | 545   |\n",
            "|    total_timesteps | 63000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 12700     |\n",
            "|    time_elapsed       | 546       |\n",
            "|    total_timesteps    | 63500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.59     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12699     |\n",
            "|    policy_loss        | 1.34e+04  |\n",
            "|    std                | 0.545     |\n",
            "|    value_loss         | 9.31e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 64000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.57     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12799     |\n",
            "|    policy_loss        | 3.15e+04  |\n",
            "|    std                | 0.54      |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 12800 |\n",
            "|    time_elapsed    | 554   |\n",
            "|    total_timesteps | 64000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 12900    |\n",
            "|    time_elapsed       | 555      |\n",
            "|    total_timesteps    | 64500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.56    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12899    |\n",
            "|    policy_loss        | 1.21e+04 |\n",
            "|    std                | 0.536    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=65000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 65000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.55    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12999    |\n",
            "|    policy_loss        | 2.28e+04 |\n",
            "|    std                | 0.534    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 13000 |\n",
            "|    time_elapsed    | 562   |\n",
            "|    total_timesteps | 65000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 13100    |\n",
            "|    time_elapsed       | 563      |\n",
            "|    total_timesteps    | 65500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.54    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13099    |\n",
            "|    policy_loss        | 1.02e+04 |\n",
            "|    std                | 0.53     |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=66000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 66000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.52    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13199    |\n",
            "|    policy_loss        | 2.18e+04 |\n",
            "|    std                | 0.528    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 13200 |\n",
            "|    time_elapsed    | 571   |\n",
            "|    total_timesteps | 66000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 13300     |\n",
            "|    time_elapsed       | 572       |\n",
            "|    total_timesteps    | 66500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.51     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 13299     |\n",
            "|    policy_loss        | 1.26e+04  |\n",
            "|    std                | 0.526     |\n",
            "|    value_loss         | 9.31e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=67000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 67000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.5     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13399    |\n",
            "|    policy_loss        | 2.71e+04 |\n",
            "|    std                | 0.522    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 13400 |\n",
            "|    time_elapsed    | 579   |\n",
            "|    total_timesteps | 67000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 13500     |\n",
            "|    time_elapsed       | 580       |\n",
            "|    total_timesteps    | 67500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.49     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 13499     |\n",
            "|    policy_loss        | 1.38e+04  |\n",
            "|    std                | 0.519     |\n",
            "|    value_loss         | 9.31e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=68000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 68000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.47     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 13599     |\n",
            "|    policy_loss        | 1.95e+04  |\n",
            "|    std                | 0.514     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 13600 |\n",
            "|    time_elapsed    | 588   |\n",
            "|    total_timesteps | 68000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 13700     |\n",
            "|    time_elapsed       | 589       |\n",
            "|    total_timesteps    | 68500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.47     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 13699     |\n",
            "|    policy_loss        | 7.41e+03  |\n",
            "|    std                | 0.514     |\n",
            "|    value_loss         | 9.31e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=69000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 69000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.45    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13799    |\n",
            "|    policy_loss        | 1.34e+04 |\n",
            "|    std                | 0.508    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 13800 |\n",
            "|    time_elapsed    | 596   |\n",
            "|    total_timesteps | 69000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 13900    |\n",
            "|    time_elapsed       | 598      |\n",
            "|    total_timesteps    | 69500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.43    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13899    |\n",
            "|    policy_loss        | 9.85e+03 |\n",
            "|    std                | 0.505    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 70000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.42    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13999    |\n",
            "|    policy_loss        | 4.71e+04 |\n",
            "|    std                | 0.502    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 14000 |\n",
            "|    time_elapsed    | 605   |\n",
            "|    total_timesteps | 70000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 14100    |\n",
            "|    time_elapsed       | 606      |\n",
            "|    total_timesteps    | 70500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.39    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14099    |\n",
            "|    policy_loss        | 1.09e+04 |\n",
            "|    std                | 0.495    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=71000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 71000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.39     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 14199     |\n",
            "|    policy_loss        | 2.21e+04  |\n",
            "|    std                | 0.493     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 14200 |\n",
            "|    time_elapsed    | 614   |\n",
            "|    total_timesteps | 71000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 14300    |\n",
            "|    time_elapsed       | 615      |\n",
            "|    total_timesteps    | 71500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.37    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14299    |\n",
            "|    policy_loss        | 8.29e+03 |\n",
            "|    std                | 0.49     |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 72000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.34    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14399    |\n",
            "|    policy_loss        | 1.69e+04 |\n",
            "|    std                | 0.482    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 14400 |\n",
            "|    time_elapsed    | 622   |\n",
            "|    total_timesteps | 72000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 14500    |\n",
            "|    time_elapsed       | 623      |\n",
            "|    total_timesteps    | 72500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.32    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14499    |\n",
            "|    policy_loss        | 1.33e+04 |\n",
            "|    std                | 0.479    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=73000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 73000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.31    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14599    |\n",
            "|    policy_loss        | 2.53e+04 |\n",
            "|    std                | 0.477    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 14600 |\n",
            "|    time_elapsed    | 631   |\n",
            "|    total_timesteps | 73000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 14700    |\n",
            "|    time_elapsed       | 632      |\n",
            "|    total_timesteps    | 73500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.28    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14699    |\n",
            "|    policy_loss        | 8.22e+03 |\n",
            "|    std                | 0.47     |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=74000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 74000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.27     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 14799     |\n",
            "|    policy_loss        | 2.34e+04  |\n",
            "|    std                | 0.468     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 14800 |\n",
            "|    time_elapsed    | 639   |\n",
            "|    total_timesteps | 74000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 14900     |\n",
            "|    time_elapsed       | 641       |\n",
            "|    total_timesteps    | 74500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.27     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 14899     |\n",
            "|    policy_loss        | 1.13e+04  |\n",
            "|    std                | 0.466     |\n",
            "|    value_loss         | 9.31e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=75000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 75000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.25    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14999    |\n",
            "|    policy_loss        | 2.07e+04 |\n",
            "|    std                | 0.463    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 15000 |\n",
            "|    time_elapsed    | 648   |\n",
            "|    total_timesteps | 75000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 15100     |\n",
            "|    time_elapsed       | 649       |\n",
            "|    total_timesteps    | 75500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.23     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15099     |\n",
            "|    policy_loss        | 9.56e+03  |\n",
            "|    std                | 0.46      |\n",
            "|    value_loss         | 9.31e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=76000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 76000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.22     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15199     |\n",
            "|    policy_loss        | 2.25e+04  |\n",
            "|    std                | 0.456     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 15200 |\n",
            "|    time_elapsed    | 656   |\n",
            "|    total_timesteps | 76000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 15300     |\n",
            "|    time_elapsed       | 658       |\n",
            "|    total_timesteps    | 76500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.19     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15299     |\n",
            "|    policy_loss        | 2e+04     |\n",
            "|    std                | 0.451     |\n",
            "|    value_loss         | 9.3e+07   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=77000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 77000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.19    |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 15399    |\n",
            "|    policy_loss        | 2.06e+04 |\n",
            "|    std                | 0.45     |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 15400 |\n",
            "|    time_elapsed    | 666   |\n",
            "|    total_timesteps | 77000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 15500    |\n",
            "|    time_elapsed       | 667      |\n",
            "|    total_timesteps    | 77500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.18    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 15499    |\n",
            "|    policy_loss        | 6.61e+03 |\n",
            "|    std                | 0.446    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=78000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 78000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.17    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 15599    |\n",
            "|    policy_loss        | 1.9e+04  |\n",
            "|    std                | 0.447    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 15600 |\n",
            "|    time_elapsed    | 674   |\n",
            "|    total_timesteps | 78000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 15700     |\n",
            "|    time_elapsed       | 675       |\n",
            "|    total_timesteps    | 78500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.17     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15699     |\n",
            "|    policy_loss        | 1.28e+04  |\n",
            "|    std                | 0.446     |\n",
            "|    value_loss         | 9.3e+07   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=79000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 79000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.15    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 15799    |\n",
            "|    policy_loss        | 7.55e+03 |\n",
            "|    std                | 0.443    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 15800 |\n",
            "|    time_elapsed    | 683   |\n",
            "|    total_timesteps | 79000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 15900     |\n",
            "|    time_elapsed       | 684       |\n",
            "|    total_timesteps    | 79500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.11     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15899     |\n",
            "|    policy_loss        | 5.48e+03  |\n",
            "|    std                | 0.433     |\n",
            "|    value_loss         | 9.3e+07   |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 80000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.11    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 15999    |\n",
            "|    policy_loss        | 1.4e+04  |\n",
            "|    std                | 0.435    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 16000 |\n",
            "|    time_elapsed    | 691   |\n",
            "|    total_timesteps | 80000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 16100    |\n",
            "|    time_elapsed       | 693      |\n",
            "|    total_timesteps    | 80500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.12    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16099    |\n",
            "|    policy_loss        | 7.79e+03 |\n",
            "|    std                | 0.436    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=81000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 81000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.11    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16199    |\n",
            "|    policy_loss        | 2.46e+04 |\n",
            "|    std                | 0.435    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 16200 |\n",
            "|    time_elapsed    | 700   |\n",
            "|    total_timesteps | 81000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 16300    |\n",
            "|    time_elapsed       | 701      |\n",
            "|    total_timesteps    | 81500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.09    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16299    |\n",
            "|    policy_loss        | 4.79e+03 |\n",
            "|    std                | 0.432    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=82000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 82000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.07     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 16399     |\n",
            "|    policy_loss        | 2.99e+04  |\n",
            "|    std                | 0.427     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 16400 |\n",
            "|    time_elapsed    | 709   |\n",
            "|    total_timesteps | 82000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 16500    |\n",
            "|    time_elapsed       | 710      |\n",
            "|    total_timesteps    | 82500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.06    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16499    |\n",
            "|    policy_loss        | 1.9e+04  |\n",
            "|    std                | 0.425    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=83000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 83000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.06     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 16599     |\n",
            "|    policy_loss        | 2.67e+04  |\n",
            "|    std                | 0.426     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 16600 |\n",
            "|    time_elapsed    | 717   |\n",
            "|    total_timesteps | 83000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 16700    |\n",
            "|    time_elapsed       | 718      |\n",
            "|    total_timesteps    | 83500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.06    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16699    |\n",
            "|    policy_loss        | 9.12e+03 |\n",
            "|    std                | 0.425    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=84000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 84000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.06     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 16799     |\n",
            "|    policy_loss        | 2.81e+04  |\n",
            "|    std                | 0.425     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 16800 |\n",
            "|    time_elapsed    | 726   |\n",
            "|    total_timesteps | 84000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 16900    |\n",
            "|    time_elapsed       | 727      |\n",
            "|    total_timesteps    | 84500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.05    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16899    |\n",
            "|    policy_loss        | 4.59e+03 |\n",
            "|    std                | 0.424    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=85000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 85000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.02    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16999    |\n",
            "|    policy_loss        | 4.27e+04 |\n",
            "|    std                | 0.419    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 17000 |\n",
            "|    time_elapsed    | 734   |\n",
            "|    total_timesteps | 85000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 17100    |\n",
            "|    time_elapsed       | 736      |\n",
            "|    total_timesteps    | 85500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.983   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17099    |\n",
            "|    policy_loss        | 1.39e+04 |\n",
            "|    std                | 0.411    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=86000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 86000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.945   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17199    |\n",
            "|    policy_loss        | 1.73e+04 |\n",
            "|    std                | 0.404    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 17200 |\n",
            "|    time_elapsed    | 743   |\n",
            "|    total_timesteps | 86000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 17300    |\n",
            "|    time_elapsed       | 744      |\n",
            "|    total_timesteps    | 86500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.923   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17299    |\n",
            "|    policy_loss        | 8.28e+03 |\n",
            "|    std                | 0.399    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=87000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 87000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.917   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17399    |\n",
            "|    policy_loss        | 3.93e+03 |\n",
            "|    std                | 0.397    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 17400 |\n",
            "|    time_elapsed    | 752   |\n",
            "|    total_timesteps | 87000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 17500    |\n",
            "|    time_elapsed       | 753      |\n",
            "|    total_timesteps    | 87500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.911   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17499    |\n",
            "|    policy_loss        | 3.32e+03 |\n",
            "|    std                | 0.397    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 88000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.914    |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 17599     |\n",
            "|    policy_loss        | 1.51e+04  |\n",
            "|    std                | 0.398     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 17600 |\n",
            "|    time_elapsed    | 760   |\n",
            "|    total_timesteps | 88000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 17700    |\n",
            "|    time_elapsed       | 762      |\n",
            "|    total_timesteps    | 88500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.921   |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17699    |\n",
            "|    policy_loss        | 1.14e+04 |\n",
            "|    std                | 0.4      |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=89000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 89000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.9     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17799    |\n",
            "|    policy_loss        | 6.68e+03 |\n",
            "|    std                | 0.396    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 17800 |\n",
            "|    time_elapsed    | 769   |\n",
            "|    total_timesteps | 89000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 17900    |\n",
            "|    time_elapsed       | 770      |\n",
            "|    total_timesteps    | 89500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.872   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17899    |\n",
            "|    policy_loss        | 7.71e+03 |\n",
            "|    std                | 0.392    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 90000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.857   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17999    |\n",
            "|    policy_loss        | 2.07e+04 |\n",
            "|    std                | 0.389    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 18000 |\n",
            "|    time_elapsed    | 777   |\n",
            "|    total_timesteps | 90000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 18100    |\n",
            "|    time_elapsed       | 779      |\n",
            "|    total_timesteps    | 90500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.848   |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18099    |\n",
            "|    policy_loss        | 1.1e+04  |\n",
            "|    std                | 0.388    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=91000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 91000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.842   |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18199    |\n",
            "|    policy_loss        | 9.4e+03  |\n",
            "|    std                | 0.387    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 18200 |\n",
            "|    time_elapsed    | 786   |\n",
            "|    total_timesteps | 91000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 18300    |\n",
            "|    time_elapsed       | 787      |\n",
            "|    total_timesteps    | 91500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.837   |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18299    |\n",
            "|    policy_loss        | 6.83e+03 |\n",
            "|    std                | 0.387    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=92000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 92000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.824   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18399    |\n",
            "|    policy_loss        | 1.59e+04 |\n",
            "|    std                | 0.385    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 18400 |\n",
            "|    time_elapsed    | 795   |\n",
            "|    total_timesteps | 92000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 18500    |\n",
            "|    time_elapsed       | 796      |\n",
            "|    total_timesteps    | 92500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.812   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18499    |\n",
            "|    policy_loss        | 1.29e+03 |\n",
            "|    std                | 0.383    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=93000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 93000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.799    |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 18599     |\n",
            "|    policy_loss        | 1.2e+04   |\n",
            "|    std                | 0.381     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 18600 |\n",
            "|    time_elapsed    | 803   |\n",
            "|    total_timesteps | 93000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 18700    |\n",
            "|    time_elapsed       | 804      |\n",
            "|    total_timesteps    | 93500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.783   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18699    |\n",
            "|    policy_loss        | 1.23e+03 |\n",
            "|    std                | 0.378    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=94000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 94000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.78    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18799    |\n",
            "|    policy_loss        | 6.3e+03  |\n",
            "|    std                | 0.378    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 18800 |\n",
            "|    time_elapsed    | 812   |\n",
            "|    total_timesteps | 94000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 18900    |\n",
            "|    time_elapsed       | 813      |\n",
            "|    total_timesteps    | 94500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.765   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18899    |\n",
            "|    policy_loss        | 3.62e+03 |\n",
            "|    std                | 0.376    |\n",
            "|    value_loss         | 9.29e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=95000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 95000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.758   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18999    |\n",
            "|    policy_loss        | 5.96e+03 |\n",
            "|    std                | 0.375    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 19000 |\n",
            "|    time_elapsed    | 820   |\n",
            "|    total_timesteps | 95000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 19100    |\n",
            "|    time_elapsed       | 822      |\n",
            "|    total_timesteps    | 95500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.756   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19099    |\n",
            "|    policy_loss        | 6.87e+03 |\n",
            "|    std                | 0.375    |\n",
            "|    value_loss         | 9.38e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 96000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.744   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19199    |\n",
            "|    policy_loss        | 1.29e+04 |\n",
            "|    std                | 0.373    |\n",
            "|    value_loss         | 3.6e+08  |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 19200 |\n",
            "|    time_elapsed    | 829   |\n",
            "|    total_timesteps | 96000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 19300    |\n",
            "|    time_elapsed       | 830      |\n",
            "|    total_timesteps    | 96500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.741   |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19299    |\n",
            "|    policy_loss        | 7.78e+03 |\n",
            "|    std                | 0.373    |\n",
            "|    value_loss         | 9.33e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=97000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 97000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.726   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19399    |\n",
            "|    policy_loss        | 2e+04    |\n",
            "|    std                | 0.37     |\n",
            "|    value_loss         | 3.79e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 19400 |\n",
            "|    time_elapsed    | 837   |\n",
            "|    total_timesteps | 97000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 19500     |\n",
            "|    time_elapsed       | 839       |\n",
            "|    total_timesteps    | 97500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.726    |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 19499     |\n",
            "|    policy_loss        | 1.78e+03  |\n",
            "|    std                | 0.371     |\n",
            "|    value_loss         | 9.29e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=98000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 98000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.71    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19599    |\n",
            "|    policy_loss        | 9.09e+03 |\n",
            "|    std                | 0.369    |\n",
            "|    value_loss         | 3.77e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 19600 |\n",
            "|    time_elapsed    | 846   |\n",
            "|    total_timesteps | 98000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 19700    |\n",
            "|    time_elapsed       | 847      |\n",
            "|    total_timesteps    | 98500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.707   |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19699    |\n",
            "|    policy_loss        | 2.01e+03 |\n",
            "|    std                | 0.369    |\n",
            "|    value_loss         | 9.68e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=99000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 99000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.693   |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19799    |\n",
            "|    policy_loss        | 226      |\n",
            "|    std                | 0.367    |\n",
            "|    value_loss         | 4.09e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 19800 |\n",
            "|    time_elapsed    | 855   |\n",
            "|    total_timesteps | 99000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 19900    |\n",
            "|    time_elapsed       | 856      |\n",
            "|    total_timesteps    | 99500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.704   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19899    |\n",
            "|    policy_loss        | 3.43e+03 |\n",
            "|    std                | 0.369    |\n",
            "|    value_loss         | 9.38e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 100000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.679   |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19999    |\n",
            "|    policy_loss        | 1.17e+04 |\n",
            "|    std                | 0.365    |\n",
            "|    value_loss         | 3.81e+08 |\n",
            "------------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 115    |\n",
            "|    iterations      | 20000  |\n",
            "|    time_elapsed    | 863    |\n",
            "|    total_timesteps | 100000 |\n",
            "-------------------------------\n",
            "mean_reward:3003000.00 +/- 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x576 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5oAAAHgCAYAAADE0xIFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeVyN6f/48deptFij1b5mXzIIMbLvOzF2YZB9F8bYsmQZa9ZkG1vIbuxhCvmEyR7zNdlGoSS01/n90c8ZR8WJc5S8n49Hj4dz7uu+7/e5puk67/u+7veliIiIUCKEEEIIIYQQQmiJXkYHIIQQQgghhBAia5FEUwghhBBCCCGEVkmiKYQQQgghhBBCqyTRFEIIIYQQQgihVZJoCiGEEEIIIYTQKkk0hRBCCCGEEEJolSSaQgghhBBCCCG0ShJNIYQQQgghhBBapXGieeHCBdatW6f23p49e6hevTo2Nja4uLiQlJSk9QCFEEIIIYQQQnxbNE40Z8+ezfnz51Wv//77b5ydndHT08PW1pa1a9eyevVqnQQphBBCCCGEEOLboXGieefOHapVq6Z6vWPHDoyNjTl58iS7du2ia9eu/P777zoJUgghhBBCCCHEt0PjRPP169eYmpqqXp86dYoGDRqQO3duAGrXrs3Dhw+1H6EQQgghhBBCiG+KxommtbU1QUFBADx9+pRr167RsGFD1fbIyEgMDAy0H6EQQgghhBBCiG+KxplhmzZtWLduHbGxsVy+fBljY2Natmyp2n7jxg2KFi2qkyCFEEIIIYQQQnw7NL6jOWnSJNq2bYuXlxfPnz9n5cqVWFhYAMl3Mw8ePEiDBg10FqgQQgiRVQQFBXH48GG19/z8/OjYsSONGjVi5cqVGRSZEEIIoR2KiIgI5ZceJCkpidevX5M9e3ayZcumjbiEEEKILMvR0RGFQoGXlxcAT548oWbNmhgZGWFhYcHdu3dZsWIF3bt3z+BIhRBCiM+j8R3NDRs28OrVq9QPoqdHnjx5JMkUQgghNBAYGEidOnVUr3fu3ElSUhK+vr5cvHiRZs2a4eHhkYERCiGEEF9G40RzzJgxlClThj59+nDkyBESEhJ0GZcQQgiRZb169QozMzPV6xMnTvDjjz+SP39+AJo1a8bff/+dUeEJIYQQX0zjRPPPP/9k4MCBXL58mR49elCmTBnGjx9PQECALuMTQgghshwLCwvVkmAREREEBASo1TmIjY3NqNCEEEIIrdC46mzFihWpWLEiM2bM4Ny5c3h5ebFz507Wr19PiRIl6Nq1K46OjhQrVkyH4QohhBDfvgYNGrB27Vpy586Nr68vgFol9zt37lCwYMGMCk8IIYT4Yl9UDCg2NpY//viDLVu24OPjA0DNmjXp1q0bXbp0wdjYWGuBCiGEEFnF8+fP6d27NxcvXsTQ0JDp06fj7OwMQExMDOXKlaNLly64ubllcKRCCCHE59F46mxqLl++jI+PDwEBASiVSsqXL09sbCwjR47E1tYWPz8/bcX5Xbh3715Gh5BlSF9qj/Sldkg/ak9W6EsLCwv++OMPgoODefTokSrJBFAqlRw4cAAXF5cMjPDblxV+TzIL6Uvtkb7UDulH7dFlX6Y70bx79y6zZs2icuXKtG7dmj/++IOePXty7tw5fH19OX36NGfPnsXCwoIxY8ZoPWAPDw8qV66MlZUVDg4OnD9//qPtfX19cXBwwMrKiipVquDp6am2fe7cuZiamqr9lC5dWq2NUqlk7ty5lC1bFmtra1q1asXt27e1/tmEEEJ8H06dOoVSqSRPnjwYGhqqbTMxMaFSpUrkzZtX53HImCqEEEJXNE40V65cSf369alVqxbu7u5Uq1aN7du3c/v2bWbPnk2lSpVUbStXroyzs7PWK+Z5e3vj4uLC2LFjOXfuHHZ2djg6OvLo0aNU2wcHB9OlSxfs7Ow4d+4cY8aMYcKECezfv1+tnY2NDUFBQaqfDwfapUuX4u7ujpubG6dPn8bCwoIOHTrw+vVrrX4+IYQQ34fOnTtTvnx5pk6dyvXr1zMkBhlThRBC6JLGieaUKVMwMjJi0aJF3Llzhw0bNtCsWTP09fVTbV+1alXGjx+vtUAB3N3d6d69O3369KFMmTIsWLAAKyurFFdU39mwYQPW1tYsWLBAtTRLt27dWLFihVo7AwMDrKysVD/m5uaqbUqlklWrVjFq1CjatWtH+fLlWbVqFW/evGH37t1a/XxCCCG+D1u3bqVmzZp4eHjg4OCAvb09y5cv5+nTp18tBhlThRBC6JLGiebly5c5duwYTk5OmJqafrJ9uXLltPp8SVxcHH/99RcNGzZUe79hw4b4+/unus+lS5dStG/UqBFXr14lPj5e9V5wcDBly5alcuXK9OvXj+DgYNW2Bw8eEBoaqnYcExMT7O3t0zyvEEII8TEtW7Zk48aN3L17l6VLl2Jubs706dOpVKkSHTp0YOfOnURFRens/DKmCiGE0DWNlzcpUaKELuP4pLCwMBITE7GwsFB738LCgmfPnqW6z7Nnz6hfv36K9gkJCYSFhWFtbU316tVZuXIlNjY2vHjxggULFtC0aVMuXrxIvnz5CA0NVe334XG+5pVnITKTxMREWecvnQwNDXWaOHxPNOlLIyOjNGfcZCa5cuWiV69e9OrVi3///Zfdu3fj5eWFs7MzY8eOpVWrVnTv3h0HBwetnlfGVCFEZhAbG0tiYmK695MxVXt0OaZqnGhCcsn1gwcP8tdffxEZGUlSUpLadoVCkWIKTWbXpEkTtdfVq1fH1taWbdu2MWzYsM8+7udWcJIqWtojfak9H/alsbExJiYmKBSKDIro25MzZ06io6MzOows4VN9qVQqCQsLIyYmJtXtNjY2ugrtiyQmJhIfH09cXBxKpRJjY2POnj2Ll5cXFStWZM2aNZQvXz6jw/woGVOzLulL7ZG+/E+uXLnIli1buveTMVV7dDmmapxoPn78mDZt2hAcHEyePHmIjIwkb968REREkJSUhJmZGTly5ND0cOlmZmaGvr4+z58/V3v/+fPnWFpaprqPpaVlqu0NDAwwMzNLdZ+cOXNStmxZ7t+/D4CVlZVqv8KFC2t0Xvi8LzL37t3LtF+AvjXSl9rzYV9GRUVhYmICJE+Di4qKSnHRSaQUHR2t6jfxZTTpS319fYyNjSlevPhXiurzvHr1in379rFz5078/f0xMDCgadOmTJs2jWbNmqGnp8eRI0eYPHkyQ4cOVa1Z/aVkTBXpIX2pPdKX/3n3feLNmzc8fvw4XXc2ZUzVHl2OqRo/ozlt2jTCw8M5fvw4ly9fRqlU4unpyb///svUqVMxMTFJUXlOmwwNDbG1tU0xyPr4+FCzZs1U97Gzs0u1fdWqVdO8ehITE8O9e/dUg2HRokWxsrJSO05MTAwXLlxI87xCZHUKhYJ//vmH169fo1QqUSgU8iM/meonKSmJx48fqxKczObQoUP07t2bsmXLMmrUKGJjY5k3bx537txhy5YttGrVCgMDA/T09GjdujXjxo3jxo0bWju/jKlCiMzg7du3BAcHk5SUlOHjhvxof0zVONE8c+YM/fv3p0aNGujp/bebkZERY8aMwd7enkmTJqXr5Ok1dOhQtm3bxubNmwkKCmLixImEhITg5OQEwKBBgxg0aJCqvZOTE0+fPsXFxYWgoCA2b96cYvrOL7/8gq+vL8HBwQQEBNCnTx+ioqLo1q0bkPyF2tnZmaVLl3LgwAFu3brFkCFDyJEjB507d9bp5xUis0pMTOTNmzdqfwuEyGwMDAy0vsyWtvTq1YvLly8zePBg/P39OXXqFD///HOaa2dWqFABR0dHrcYgY6oQIqO9ePFCvkt8Iz5nTNV46uzbt28pVqwYgGpx6ffXvKpduza//vpruk6eXh07diQ8PJwFCxYQGhpKuXLl8PLyokiRIkDy9N73FStWDC8vLyZPnoynpyfW1ta4ubnRrl07VZt///2XAQMGEBYWhrm5OdWrV+fEiROqYwKMHDmS6Ohoxo8fT0REBNWqVcPb25tcuXLp9PMKkVklJiaiVCozOgwhPun9aqiZyd69e3FwcECh0Ow552rVqlGtWjWtxiBjqhAio8mjN9+W9I6pioiICI2+LVatWpVu3boxYcIEILkK7c8//6y6izlr1iw2btzI//3f/6UzZPGOzNvXHulL7UntGU0DAwNu376NgUG66ollep06daJTp050795d68fOzM+TPH36lM6dO+Ph4UG5cuVSbXP79m0GDBjA7t27yZ8//1eOUJ2mfRkZGUlYWBitW7f+ClGJzEbGAe2RvtQe6cv/REVFERIS8llFfTLzmPq59u/fz6ZNm3j27BlOTk5YW1uzePFiTp48qdPz6nJM1fhbor29PadPn1Ylmm3btmXFihUYGBiQlJTE6tWradasmcYnFkKIr+nly5esX7+eCxcuEBYWRs6cOSlRogQ9e/bEzs4uo8NLtytXrjB8+HAOHz6cYm3jnj170qBBA/r3759B0QlN+fv7f7SS+7sxVwghRObg6urKH3/8ASQXybGyssLBwYH+/ft/dvIbGRnJokWLGD58OA0aNCB79uzo6+tjb2+varN+/Xp8fHz4/ffftfI5vgaNE80hQ4bg4+NDTEwMxsbGTJ8+neDgYObMmQNA3bp1mTdvns4CFUKILzFlyhRiYmKYNGkShQoV4uXLl1y9epXIyEidnjcpKUmmGYsUIiIi6Nq1K//73/9UBbXe/Z68+7ckmkIIkTlVr16dX3/9lYSEBAIDA5k3b57qkYD3JSQkoK+v/8nHJEJCQkhMTKROnTqYm5ur3jcyMtJJ/F+LxolmhQoVqFChguq1qakp+/btIyIiAn19fXm2QgiRab1+/ZrAwECWLFlC9erVAbC2tk51mmhcXBzz58/nxIkT5MiRA0dHR3r06KHavmPHDo4cOcKTJ0/ImTMntWrVYtiwYaq/gYcPH2bx4sXMnDmTlStX8vDhQzZu3Ii5uTkbNmzg+PHjREZGUrx4cQYOHKiqtJmQkMDy5cvx8fFRLR/VtGlTnJ2dv/jzx8fHs27dujTPnZqLFy+ydOlSQkJCKFu2LB06dPjiOMR/pk2bxrVr11i7di01atTA1tYWb29vihYtyrJly7h69Sp79uzJ6DCFEEKkwtDQULWsU9OmTbly5Qp//vkn+fLlw8fHh27durFx40ZCQkI4duwYkZGRLF26lP/9738A1KhRg9GjR2Npacnhw4dVN+7eFX3bvXs3V65cUU2dPXz4MJ6engDUqVMHgMmTJ9OqVauv/dHT5YsfsPpwypYQ4vtUp479pxtpkZ/feY3bmpiYYGJigq+vL5UrV/7oFcKdO3fSv39/NmzYwIULF1iyZAlVqlShYsWKQPLdppEjR1KgQAFCQkJYvHgxixcvViuGFhcXx8aNG5kwYQKmpqaYmZkxb948QkJCmD59OhYWFly4cIEJEybg4eGBjY0Nu3bt4ty5c8ycORNra2ueP3/Ow4cPP7+D3jN79myePHmS5rk/FBoayqRJk2jTpg2dOnXi77//Zvny5VqJRSQ7duwYvXv3pnPnzoSHhwOgp6dHiRIlWLJkCT/99BOTJ09m7dq1GRypEEJ8Pfb/P4n6Ws77+WnlOEZGRiQkJADJdQ9OnDiBq6sr2bJlI1u2bLi4uGBkZKQaS3/77TdcXFxYv349jRs3xtzcnDFjxuDh4YGlpWWK/Kpx48b8888/+Pn5sWLFCiB5neLMLs1Ec/v27Z91wHclzIUQIrMwMDBgypQpuLm5ceDAAWxsbKhcuTINGjRQm6kByWsFvltmwdHRkd27dxMQEKBKNLt27apqmz9/foYMGYKLiwu//PKLqkR7YmIiY8aMoWzZskBy9U4fHx92796NtbU1AJ07dyYgIID9+/czbtw4QkJCKFy4MFWqVEGhUGBtbU2lSpU++dlSWxIiNjZW9e/Hjx9z8uTJj577Q3v37sXKyorRo0ejUCgoWrQojx49Yt26dZ+MR2jm5cuXqt+9d2tQvn37VrW9SZMmzJ49O0NiE0IIoblbt25x4sQJVWXw+Ph4fv31V/LlywfApUuX+L//+z+8vLxUxfSmT59O165dCQgIoEaNGuTOnRtAdXH6Q0ZGRpiYmKCvr5/q9swqzURzyJAhKd57N7/4w+eN3p93LImmECIzatCgAfb29gQGBnLjxg38/f3Zvn07AwcOpE+fPqp2JUuWVNvP3Nycly9fql5fvnyZzZs38+DBA968eUNSUhLx8fGEhYVhYWEBJBcHeP9O4d27d1EqlfTs2VPt2HFxcaqBqWXLlowaNYqffvoJOzs7ateuTa1atT65vtjy5ctTPLrwfvKoybk/9ODBAypUqKD2t/1doi20w9LSkhcvXgCQK1cucuXKxb1791TbX758SWJiYkaFJ4QQ4iP8/f1p3LgxiYmJJCQkULduXcaMGYO3tzeWlpaqJBOSx1Rzc3O1iu0FCxbE3Nyc4OBgatSokREf4atIM9EMDAxUe/3q1SucnZ3JmzcvAwYMoFSpUgD8/fffrFu3jlevXrFq1SrdRiuEEF/AyMgIOzs77Ozs6NevH3PnzsXT05Pu3bur7ip9uGSLQqFQVQMNCQlh3LhxtG3blp9//pncuXNz9+5dpk2bppoyA8nPbujr66teJyUloVAo8PDwSHH8d9N4y5Qpw+7du7l06RIBAQG4urpSqlQplixZ8tFkM3/+/Cmm2Lx/Dk3OLb6+GjVqcOHCBdXrxo0bs3z5cqytrUlKSmLlypXfZDVkIYT4HlSpUoWJEydiYGCAubm52vhqbGycgZFlLmkmmu8vrgzJdzgtLS3Zs2eP2lXuChUq0LZtWzp27MjKlStZuXKl7qIVQmRa6XlmMrMoXrw4iYmJxMXFqRLNj7l9+zYJCQmMGDFClUieP//pz126dGmUSiVhYWFp3kUEyJEjBw0aNKBBgwa0bNmSgQMH8vjx4xR/j9ND03O/r2jRopw5c0ZV+RTg5s2bnx2DSOnnn39m3759qkrus2bNokOHDgwePBhIvrMuldyFEN8bTZ+ZzOh1NI2NjSlUqJBGbYsWLcqLFy94+vSp6q7mkydPePHiBcWLF9f4nO+WlPyWaFwM6PDhw0ydOjXV8rwKhYJWrVrh6uqq1eCEEEIbXr16xS+//ELr1q0pWbIk2bNn586dO2zdupVq1aqRI0cOjY5TuHBhkpKS8PLywsHBgZs3b+Ll5fXJ/YoUKUKjRo2YPXs2w4cPp3Tp0kRGRnL16lUKFChA/fr12bFjB2ZmZtjY2GBgYKCqemtpaflFn71IkSI0bdr0o+f+UPv27dmxYwdLly6lQ4cO3L9/n3379n1RHEJd7dq1qV27tup1wYIFuXjxIjdv3kRfX5/SpUunuAMthBDi21OjRg1KlizJjBkzGDlyJACLFy+mdOnSGl8AhuQZTCEhIQQFBWFlZUX27NkxNDTUVdhaofEoplQqCQoKSnP7nTt3ZK04IUSmZGJiQoUKFfDy8uLJkyfExcVhYWFBkyZN6Nu3r8bHKVWqFKNGjeL3339n7dq1VKpUiaFDh6pVnE3L+PHj8fLyYuXKlTx79ozcuXNTrlw5fvjhBwCyZ8/Otm3bePToEQqFgtKlS7No0SKtTMGZMmUKmzZtSvPcH7K2tmbOnDksW7aM/fv3U6ZMGQYPHszMmTO/OBaRNj09PY0KQAkhhPh2KBQK5s2bx5IlSxg+fDjw3/Imn1pf833169fn7NmzjBw5ktevX38Ty5soIiIiNMoOnZ2d2bVrF9OmTaNfv36qOwBv377F09OTGTNm4OjoKM9pfoF79+6lutSASD/pS+35sC+joqIwMDDg9u3bcsclHTJ6mk9WomlfRkZGEhYWRuvWrb9CVB/n95kl9Ot85VL/WYmMA9ojfak90pf/iYqKIiQkhOjo6HTvK2Oq9uhyTNX4W+K8efN48OABv/76KzNmzMDKygpIXm8tMTGRWrVqMXfuXI1PLIQQQnwvWrdurXbl+v3nXz/m3RqbQgghxLdG40QzT548HDlyhMOHD3Py5EkePXoEQNOmTWnSpAktWrRI1+1fIYQQ4ntx8OBBtddxcXH8+uuvxMXF0atXL7VK7lu2bMHIyEimKgshhPimpXveW6tWrTL9fGAhhBAiM6lbt67a68mTJ2NsbMypU6dSLDMzYMAAWrduzcmTJ2nQoMHXDFMIIYTQmo+vBC6EEEIIrdu1axeOjo6prmVqYmJCly5dNKpoLIQQQmRWkmgKIYQQX1lUVBShoaFpbn/69OlnFcgQQgghMgtJNIUQQoivzMHBgdWrV7N///4U2/bv38+aNWtwcHDIgMiEEEII7ZC1CYQQQoivbOHChbRt2xYnJycsLS0pXrw4AP/88w/Pnj2jePHizJ8/P4OjFEIIIT6fJJpCCCHEV1agQAF8fX3ZsGGDWiX3ChUqMGrUKPr06SNrxAkhhPimaZxoDh06FCcnJ6pXr57q9suXL+Pp6Ym7u7vWghNCCCGyKmNjY5ydnXF2ds7oUIQQQgit0/gZzW3btvHPP/+kuf3Bgwds375dK0EJIURWMGzYMBYtWvRFx1i/fj09e/bUUkTfBk0+86JFixg2bNhXikgIIYQQ6aW1qbPh4eGplmkXQoiMVqdOnY9ub9GiBb/88stH93d1ddXJmoYHDhzA29ubx48fo6enh7W1NXXr1mXgwIFaP9fXcO/ePTw8PLh16xZv3rwhb968lC1blhEjRmBtbZ3R4QkhhBBaERQUxIABA6hQoQKrV6/O6HAypY8mmn5+fvj6+qpeHzx4kPv376doFxERgbe3NxUrVtR+hEII8YUOHDig+refnx9ubm5q72XURbJDhw6xZMkSRowYQfXq1UlISOD+/fvcuHFD5+eOj48nW7ZsWj3my5cvGTlyJHZ2dixYsIA8efIQEhLC+fPnefv2rVbPJYQQQmSkgwcP0qFDB44ePUpwcDDFihXT2bkSEhIwMPj2Sut8NOI///wTNzc3ABQKBQcPHuTgwYOpti1XrpyqrRBCZCZmZmaqf+fKlSvFe/v27WPbtm2EhoZiZWVFz549adu2LQCdOnUCUN3xtLa2Zs+ePTx+/Jjly5dz69YtoqKiKFKkCAMGDPjk3dP3+fr64uDgQPv27VXvFStWjIYNG6Zoe/LkSdasWcPLly+pXr06Li4umJqaAnD79m3WrFnD3bt3iY+Pp1SpUgwdOlTt4l+dOnUYM2YMAQEBXLp0iQ4dOjBs2DB8fX3x9PTkn3/+wczMjCZNmtCvXz9VEnrmzBk8PT159OgRRkZGlCxZklmzZpEvX74UMV6/fp3Xr18zZcoU1f758+enatWqau3+7//+j2XLlnHt2jWMjIyoW7cuo0aNImfOnKn2U2JiIqtWreLQoUMANGnShKSkJI37WQghhNCm2NhYTpw4wcqVK4mNjeXQoUMMGzaM6dOnExcXx5w5c1Rtk5KS6NSpE127duWnn35CqVSybds29u3bx4sXLyhUqBA9e/akWbNmQPI6yp07d2b69OkcOHCAGzduMHToUJo0acJvv/1GYGAgr169okCBAnTv3p1WrVqpzhUdHc3ChQs5e/YsxsbGdOnShevXr5MnTx7V95j4+HjWrVvH8ePHiYyMpGjRogwePJiaNWtqvZ8+mmiOHDmSgQMHolQqKVWqFIsXL1Z9+XpHoVBgYmKCsbGx1oMTQnw76pzQPMHSBr8mflo5ztmzZ/ntt98YMWIEdnZ2+Pv7s3DhQvLly0fdunXx8PCgdevWTJw4kTp16qCnl/xoe3R0NLVq1WLgwIEYGRlx6tQpJk+ezObNmylatKhG586XLx9XrlzhyZMnFCxYMM12ISEhnDp1irlz5xITE8Ovv/7K2rVrmTBhAgBRUVE0b96cUaNGoVAo2L17N+PGjWPnzp3kyZNHdRxPT08GDRrEsGHDUCgU+Pv7M2PGDEaNGkWVKlUIDQ1lwYIFxMfHM2zYMMLCwpg2bRqDBw+mfv36REdHf/Rua758+UhKSsLHx4cmTZqgUChStImOjmb06NGUL18eDw8PIiMjcXNzY86cOWoD8/t27NjBgQMHmDhxIqVKlWLnzp2cOHGCMmXKaNTPQgghvg3puVj7vjJlyuDp6Znqtn79+hEUFJTqNj+/z/su4ePjg7W1NSVLlqRZs2ZMnTqVwYMH07RpU6ZMmcKbN29UF0+vXr1KWFgYjRs3BmDt2rX4+PgwduxYihQpwo0bN3BzcyNXrlzY29urzrF69WqGDRvGpEmTMDAwIC4ujtKlS9OjRw9y5MhBQEAA8+fPx8rKSlWsdfny5Vy9epU5c+Zgbm7Oxo0bCQwMpF69eqrjzp49mydPnjB9+nQsLCw4d+4cEyZMwMPDAxsbm8/qj7R8NNE0MTFRlVcPDAzE3Nyc7NmzazUAIYTISNu3b6d58+Z07twZgCJFihAUFMTWrVupW7cuefPmBZLvhL5/F9TGxkbtD3KfPn3w9fXFx8eHvn37anTufv368ffff9OlSxcKFSpE+fLlsbOzo0mTJmpTZBITE5kyZYpq0GrXrh2HDx9Wba9WrZracceMGcPZs2e5ePGi6gopQKNGjdQuFrq6uqpdDS1UqBBDhgxh5syZDB06lBcvXpCQkECDBg1Uz1eWKFEizc9TsWJFevfujaurK7/99htly5alatWqNGvWTLX/iRMniImJYerUqeTIkQOACRMmMHz4cB4/fkyhQoVSHHfnzp306NGDRo0aAclV0K9cuaJBD2debm5utGnThvLly6e6/fbt26rkWgghROZy6NAh1fhatWpVjI2N+fPPP/nxxx/JkSMHPj4+tGnTBoDjx4/zww8/YG5uTnR0NDt27GDx4sXY2toCyctd3bp1iz179qglmp07d05RG6JHjx6qfxcsWJDLly9z4sQJqlevTlRUFIcPH2bq1KnY2dkBMGnSJDp06KDa5/Hjx5w8eZLdu3erxuX27dsTGBjI/v37GTdunFb7SeOqs5cvX/5okpmQkICrq6tWgvoYDw8PKleujJWVFQ4ODpw/f/6j7d9NTbOysqJKlSoprnb89ttvNGjQgMKFC1OyZEm6du3KrVu31No4OztjagTzgbYAACAASURBVGqq9vPuqoQQ4tsWHBxMpUqV1N6rXLnyR6tsQ/KdOXd3d3r06EHz5s1p3LgxQUFBhIaGanxuc3Nz1q5dy5YtW+jSpQtKpZL58+czYMAAYmJiVO2srKzUppWam5vz8uVL1euXL18yf/58fvrpJ5o2bUqTJk14+fIlISEhaucrW7as2uugoCA2b95M48aNVT/Tp08nOjqasLAwSpUqRfXq1enZsyeTJ09m7969audNzaBBgzh48CATJkygZMmSHDp0iB49ehAQEAAk93fJkiVVSSZApUqV0NPTS7XP37x5Q1hYmNo0YD09vTQTtG/FvHnzuHnzZprbb9++/VUeR5ExVQgh0ufx48dcu3aNJk2aAMmzO5s2bcqhQ4cwMDCgUaNGHD9+HIC4uDjOnj2rSkqDg4OJi4tj7NixamPvvn37ePLkidp5PhyzExMT2bRpE71796ZFixY0btyYs2fPqr53PHnyhISEBMqVK6fax8TEhOLFi6te3717F6VSSc+ePVXnbt26NefPn09xfm3Q+KnSfv36cfjwYRYuXKh6LuidmzdvMnjwYG7duvXRyo1fytvbGxcXFxYtWkStWrXw8PDA0dGRixcvUrhw4RTtg4OD6dKlCz169GDt2rVcvHiRsWPHYmZmRrt27YDkQbN///788MMPKJVK5syZQ/v27fH391fdyQCoX78+a9asUb02NDTU2ecUQmS81KZ9vm/FihX4+/szdOhQChcujLGxMbNmzSI+Pj7d5ypRogQlSpSgU6dOBAYGMmTIEE6dOqW605haAQClUqn6t6urK+Hh4arKroaGhowYMYKEhAS1fd7NUHknKSkJJyenVJ8JNTU1RV9fnyVLlnDz5k0uXbrEwYMHWb16NStWrPjo9Jo8efLQsGFDGjZsyODBg+nbty8bN25Mcx3mdz7V59+TN2/eaL1Y04dkTBVCiPQ7ePAgiYmJqhoO8N+YHBoaSrNmzRg0aBDPnz/n5s2bxMfH4+DgAKCqL/Buyuv7PhzrP3wscfv27Wzfvp1Ro0ZRokQJsmfPrqrdoKmkpCQUCgUeHh6q88XExGBsbKyTwogaJ5pubm7MmDEDPz8/li1bRpMmTVAqlfz222+qztq3b5/WA3yfu7s73bt3p0+fPgAsWLCAU6dO4enpybRp01K037BhA9bW1ixYsABInr8dEBDAihUrVIOit7e32j5r1qyhSJEiXLx4kRYtWqjeNzIySvELIYT4j7aemfzaihUrxvXr11VTXACuXbumVj3OwMCAxMREtf2uXbtG8+bNVdNaYmNjefLkSapf0NPj3ZXH6OhojfcJDAxk9OjRqik34eHhhIWFfXK/MmXK8ODBg1Snq76jUCioWLEiFStWxMnJiZ49e3Lq1CmNn+PIli0bBQsW5MWLF0Byfx8+fJi3b9+q7mpev36dpKSkVCv25cyZEzMzM27cuKGaIqxUKrl16xbm5uYaxZBZ3Lhxg+vXr6teX7hwIcXFAEiu5O7p6an1Z2U+JGOqECKz0fSZyejo6BQXT9OS1rObnyMhIYE//viDwYMHp3iedObMmRw+fJh+/fpRsGBBTpw4wY0bN/jxxx9Vs0KLFSuGoaEhISEhKR57+ZRr165Rp04dmjdvDiSPhQ8fPlQVOSxYsCAGBgbcvn1bVfchJiaGf/75R/W6dOnSKJVKwsLCVOdPT1+ml8aJ5sCBA2nUqBHOzs6qqkl3797l8uXL9OrVizlz5qRZMVAb4uLi+Ouvvxg+fLja+w0bNsTf3z/VfS5dupTiSn2jRo3Yvn17mqX937x5Q1JSUoq7thcuXKBUqVLkyZOHOnXqMHXqVCwsLL7wUwkhMlr37t355ZdfKFOmDHZ2dly8eJHjx4+rFabJnz8/ly9fpmrVqmTLlo3cuXNTuHBhzp07x48//oiBgQGenp7ExcWl69wLFizA3NycatWqYWlpyYsXL9i0aRPGxsaq5ys0UaRIEY4dO0b58uWJiYnB3d1do7thTk5OjB8/Hmtraxo1aoS+vj7379/n1q1bDB06lBs3bhAQEEDNmjXJmzcv9+7dIzQ0VG0azvv8/Pw4efIkjRs3pnDhwiiVSvz8/Lh48SL9+/cHoGnTpnh4eODq6sqAAQN4/fo18+fPx8HBIc2Et0uXLmzZsoUiRYpQokQJdu3aRVhY2DeXaB46dEitkvuGDRvYsGFDqm1NTU1Zu3atzmKRMVUIIdLvwoULRERE0LZtW7Vie4BqCqyTkxNNmzbl4MGDhISEMHv2bFWbHDly0K1bN1asWIFSqcTW1paoqChu3ryJnp6e6qJdagoXLsypU6cIDAzE1NSU3bt38/TpU1WimT17dlq1asWqVaswNTXFzMyMTZs2qe5iQvL3haZNmzJ79myGDx9O6dKlef78Obdu3aJAgQLUr19fq/2VrgVZSpYsyZEjR2jRogXbt29HoVAwc+bMFAOVLoSFhZGYmJhiILKwsODZs2ep7vPs2bMUHWZhYUFCQgJhYWGpLh7u4uJCpUqV1L7kNW7cmDZt2lC0aFEePnyIq6srbdu25cyZM2neZr537146P+GX7SdSkr7Unvf70tDQECMjI2JiYtDX18/AqD7Pu2Tw3R3DGjVqMGzYMHbs2MHSpUuxsrJixIgRVKtWTdVm4MCBrF69msOHD2Nubs7WrVsZOHAgCxcuZMiQIeTMmZOOHTsSHR1NYmKiar/ExES11x/epaxSpQpHjx5l7969REZGkitXLmxsbHBzc8PCwoLo6Gji4+NRKpVq+3743pgxY1i8eDH9+vXDzMyM3r178/LlS+Lj49X2i4uLU3tduXJlZs+eze+//8727dvR19enUKFCNG3alOjoaAwMDPjrr7/YtWsXb9++xcLCgp49e1KvXr1U77i+m7a7fPlynj9/jp6eHvnz52fgwIGq/gGYO3cuq1atYsCAARgaGmJvb8+QIUNU2z/8fO3btyc0NJS5c+cCyX+TGzZsyMOHDz965/ft27e8ePEixd8CXd8pTEvfvn1p3rw5SqWShg0bMnnyZNUzPu/LkSMHxYsX1+maaTKmivSSvtQe6ctkhoaGvH37ltjY2M/aPz0zf7Rl//792NraYmhomOL8tWvXZtWqVapn2devX4+pqSmVK1dWa9uzZ09y5szJ1q1bWbhwIdmzZ1c90x4dHa2q0RAbG6u2X9euXXn8+DFjx47FyMiIpk2b0rBhQx48eKBqN2DAAN6+fcvEiRMxNjamU6dOvHjxAj09PbXvDFu3bmXFihW8ePGCXLlyUbZsWSpUqKD1MVURERGhTHPrBx4+fMjQoUPx9fWlbdu2XL58mbCwMKZMmcKwYcM0Pcxnefr0KeXKlePw4cNqt6rd3NzYtWuXqtDE+6pVq0aXLl3Uqvb5+fnRqlUr7ty5k2JQnDx5Mt7e3hw9evSji64+ffqUSpUq4enpmWK5ly9x7969DPsClNVIX2rPh30ZFRWlmprxLS4enFF0OTXle6NpX0ZGRhIWFkbr1q2/QlTp4+vrS5kyZTLsLp6MqSI9pC+1R/ryP1FRUYSEhHxWwihjqmbi4uLo1KkT3bt3p1u3bqm20eWYqnHV2U2bNlG3bl1u377N5s2b2bRpE35+frRp04apU6fSokULgoODNT5xepmZmaGvr8/z58/V3n/+/DmWlpap7mNpaZlqewMDA7VlCiC5/O+ePXs4cODARwdESJ5GV6BAAe7fv5/+DyKEEOK7Z2ho+Mkkc/PmzTo7v4ypQgiR9dy9e5fjx4/z+PFj7t69i6urK1FRUarlwb42jRPNUaNGUbduXS5cuKAqmpEnTx7Wrl3L5s2b+fvvv/nxxx91FqihoSG2trb4+Piove/j40PNmjVT3cfOzi7V9u+es3pn4sSJqgGxdOnSn4wlLCyMp0+fSiEDIYQQn6VFixbMnDkz1SrFoaGhdOnShVGjRuns/DKmCiFE1rRjxw769u3LiBEjCA8Px93dPc0LiLqm8by3d9XpUtOmTRtq167N2LFjtRZYaoYOHcqgQYOoVq0aNWvWxNPTk5CQEJycnIDk9dsAVcl0Jycn1q1bh4uLC05OTvj7+7Nt2zY8PDxUxxw3bhw7d+7k999/x9TUVLUWTY4cOciZMydv3rxh3rx5tG3bFisrKx4+fMjMmTOxsLDIlNOxhBBCZH7Ozs4sXbqU48ePs2rVKtVarrt27WLixIkkJibi7u6u0xhkTBVCiKyldOnSWq2y+6U0TjTTSjLfMTc3Z9OmTV8c0Md07NiR8PBwFixYQGhoKOXKlcPLy4siRYoAyQuovq9YsWJ4eXkxefJkPD09sba2xs3NTa2i07sB8sMqTxMnTmTSpEno6+tz69YtduzYwatXr7CysuLHH39kw4YNqipPQgghRHq4urrSsmVLhgwZQuPGjRk9ejS3bt3i4MGD1K9fnxUrVqjK0euKjKlCCCF0KV3FgMLDw1m5ciV//vknz58/Z/Xq1djZ2REeHs66deto3749ZcqU0WW8WZo8IK490pfaI8WAtEMKF2hPVigG9M7bt29p3749ly9fBpITsveL7YjPJ+OA9khfao/05X+kGFDmkCmKAT148IC6deuyYsUK4uPjCQ4OVv1i5MuXD29vb9atW6fxiYUQQghdUio1vo6aISIjIxk7diwBAQFUrVqVHDly4OHhwYEDBzI6NCGEEOKLaZxoTps2DaVSycWLF9m1a1eKAbxly5acO3dO6wEKITKfzP4FXgiAhIQE1SLVmY2Pjw/29vbs378fV1dXTp48yZ9//knp0qXp27cvAwcOJCIiIqPDFEIIIT6bxonmmTNn+PnnnylWrFiqA3fRokX5999/tRqcECLzMTIyIi4uLtN+gRcCkqdkPXv2DENDw4wOJVUdO3bEwsKCM2fOMHToUBQKBcWKFePw4cPMmjWLQ4cOYW9vn9FhCiGEEJ9N4wesYmNjMTU1TXP7q1ev0NPTOG8VQnyj9PX1yZ49OwYGBoSFhclzmhp6+/ZtqktZiPT7WF8qlUoSEhJ49uwZcXFx/PDDD185Os1MnDiR8ePHo6+vn2Lb0KFDadq0Kc7OzhkQmRBCCKEdGn9DLFeuHH5+fvTr1y/V7YcPH6Zy5cpaC0wIkXnp6+tTvnx5rl+/zr///ktCQkJGh5TpvXjxAnNz84wOI0vQpC9NTU2pWrUqZmZmXymq9HFxcfnodhsbG44fP/6VohFCCCG0T+NE09nZmUGDBlGuXDk6dOgAQFJSEnfv3mX+/PkEBASwdetWnQUqhMh8KlWqpFr/T3ycVBrUnqzSl3FxcezYsUNVyX3GjBlUqVKFiIgI/vjjD+rVq6fzJU6EEEKkj6urK3/88QetW7dm0qRJattWrlzJ1q1bsbe3Z8GCBRkUYeahcaLp6OjI48ePmTNnDnPmzAGgU6dOAOjp6TFjxgxatGihmyiFEEKILCQ8PJw2bdpw69YtLC0tef78uar4T+7cuZk9ezZ37txhxowZGRypEEKID1lZWXHq1ClGjRqlWhokISGBo0ePYmVllcHRZR7peqhy9OjRXL16FVdXV/r370/fvn2ZMWMGAQEBDB8+XFcxCiGEEFnKtGnTePToEUePHuX8+fNqlZz19PRo27YtJ06cyMAIhRBCpKVkyZIULlyY06dPq967cOEChoaGVK1aVfXe7du3GTVqFC1btqRJkyY4Oztz48YN1farV69Sr149rly5onpv3759NGnShCdPnnydD6ND6a7iUahQIYYMGaKLWIQQQojvwtGjRxk0aBA1a9YkPDw8xfaSJUvy+++/Z0BkQgiRcerUqfNVz+fn5/fZ+7Zu3ZpDhw7RqlUrAA4dOkTLli3VVuGIioqiefPmjBo1CoVCwe7duxk3bhw7d+4kT548VK1ale7duzNr1iw2bdrEy5cvWb58OWPHjs0Sj058VrnIN2/eEBERkepaeoULF/7ioIQQQois7PXr1xQqVCjN7bGxsSQmJn7FiIQQQqRHkyZNWLFiBY8ePSJ79uz4+/szevRoPDw8VG2qVaumts+YMWM4e/YsFy9epFmzZgAMGDCA//3vf8ydO5eQkBDs7e1p2bLlV/0suqJxohkTE4ObmxtbtmxJ9errOx/bJoQQQggoUaIEV69epU+fPqluP336NOXKlfvKUQkhhNBU7ty5cXBw4NChQ+TKlYuqVatibW2t1ubly5esW7eOK1euEB4eTlJSErGxsYSEhKjaGBgYMH36dHr27EnevHlZtmzZ1/4oOqNxojl27Fi2b99Oq1atqF279kfX1BRCCCFE2vr06cPUqVOxt7enYcOGACgUCqKiopg/fz6nT59m+fLlGRylEEKIj2nVqhWurq6YmJgwYMCAFNtdXV0JDw9nxIgRWFtbY2hoyIgRI1IsC3fz5k2USqVq1miuXLm+1kfQKY0TzYMHD9K7d2+WLFmiy3iEEEKILG/QoEHcuXOHQYMGqb5Q9OvXj4iICBITExkwYAA9evTI4CiFEOLr0vSZyejoaFW114xUvXp1smXLxqtXr6hXr16K7YGBgYwePRp7e3sgeeZnWFiYWpt///2X3377jTFjxuDv78/MmTNZtWoVBgaf9YRjpqLxJ1AoFFSpUkWXsQghhBDfjcWLF/PTTz+xd+9e7t+/T1JSEsWLF6dDhw6qLyVCCCEyL4VCwaZNmwAwNDRMsb1IkSIcO3aM8uXLExMTg7u7O9myZVNtT0xMZNasWdja2tK+fXsaNGhAr1698PT0ZODAgV/tc+iKxolmy5YtOXPmDE5OTrqMRwghhPhu1KxZk5o1a2Z0GEIIIT5Tjhw50tw2adIk5s+fT79+/TA3N6d///6qNZMBNm/ezOPHj9m8eTMAefLk4ZdffmHcuHHUrFnzm7/Jp4iIiEhZOjYVf//9N/369cPW1pbevXtTqFAh9PX1U7SzsLDQepDfi3v37mFjY5PRYWQJ0pfaI32pHdKP2iN9KTQhvyfaI32pPdKX/4mKiiIkJITo6Oh075tZps5mBZr2ZWRkJGFhYbRu3VrjY2t8R7NGjRoAXL9+/aNre0nVWSGEEOLjlEolGzduZMuWLQQHB6td4X5HoVCkeJZHCCGE+FZonGhOmDABhUKhy1iEEEKI78Kvv/6Ku7s7lSpVokuXLlLJXQghRJajcaI5adIkXcYhhBBCfDe2b99O27Zt2bhxY0aHIoQQQuiEXkYHIIQQQnxvYmJiqF+/fkaHIYQQQuiMJJpCCCHEV1avXj2uXLmS0WEIIYQQOiOJphBCCPGVLVq0iICAABYuXMizZ88yOhwhhBDio5RKjRYqUaPxM5pCCCGE0I6qVauiVCqZM2cOc+bMIVu2bOjpqV/7VSgU/PvvvxkUoRBC6N7nJC8iYyQkJKS7MKwkmkIIIcRX1qFDB6nkLoT4rmXLlg2lUolSqZS/h5lcVFQUz549I0+ePOnaT6NEMzo6mmXLllGjRg0aNmz4WQEKIYQQItmqVasyOgQhhMhQ2bJlw8zMjFu3bqFQKNKVbL59+5b4+HgdRvf9+FhfKpVKEhISePbsGXFxcfzwww/pOrZGiaaJiQmLFy9m/vz56Tq4Lnh4eLBs2TJCQ0MpW7Ysc+fOxd7ePs32vr6+TJkyhTt37mBtbc3IkSPp169fuo4ZGxvLL7/8wp49e4iJiaFevXosWrSIggUL6uxzCiGEELomY6oQIiPlzZuXKlWq4O/vT2xsrMZTaV+8eIG5ubmOo/s+aNKXpqamVK1aFTMzs3QdW+OpsxUrVuT+/fvpOri2eXt74+LiwqJFi6hVqxYeHh44Ojpy8eJFChcunKJ9cHAwXbp0oUePHqxdu5aLFy8yduxYzMzMaNeuncbHnDRpEkeOHGH9+vXkzZuXKVOm0LVrV86ePYu+vv5X7QMhhBBCG2RMFUJkBjlz5qRRo0bp2ufevXvY2NjoKKLviy77UhEREaHRpYOzZ8/St29fVq9eTbNmzXQSzKc0atSIChUqsGzZMtV7P/zwA+3atWPatGkp2k+bNo2DBw+qlZAfPnw4d+7c4cSJExod89WrV5QqVQp3d3e6dOkCwOPHj6lUqRK7d+9O9/8YH/PuP7TpElOtHVMIIbKa/7X6n3zB0IJvaUw1NZVxUQghMqOIiIg0t2l8R3PFihXkzZuXbt26UaBAAYoVK4aJiYlaG4VCgZeX1+dH+hFxcXH89ddfDB8+XO39hg0b4u/vn+o+ly5dSvFMaaNGjdi+fTvx8fEolcpPHvOvv/4iPj5e7TiFChWiTJky+Pv7azXRFEIIIb4GGVOFEELomsaJ5p07d1AoFBQqVAiAhw8fpmijy4pRYWFhJCYmYmFhofa+hYVFmmuQPXv2jPr166don5CQQFhYGEql8pPHfPbsGfr6+inmJH/svJB8d/JzfO5+QgjxPfmSv5VyN/TbG1OFEEJ8ezRONK9fv67LOLKcz/kiI/PNhRBCM9/y38qYmBj27t1L6dKlqVatWkaHI4QQQujEN7OOppmZGfr6+jx//lzt/efPn2NpaZnqPpaWlqm2NzAwwMzMDKVS+cljWlpakpiYSFhYmFpFpufPn1O7dm1tfLQUIkalPddZaEaSdu2RvtQO6Uft+dZnfhgbGzNy5Ejmz5+fYYnmtzamfuwZoLTI/3PaI32pPdKX2iH9qD267Eu99DSOi4tj8+bN/Pzzz7Rv357AwEAgeQDYvn07T5480UmQAIaGhtja2uLj46P2vo+PDzVr1kx1Hzs7u1TbV61alWzZsml0TFtbW7Jly6bW5smTJwQFBaV5XiGEEOJjSpUqRWhoaIadX8ZUIYQQuqbv4uIyXZOG4eHhNGvWjC1btvDixQtu375N+/btKVasGIaGhnTv3p3o6GgaNGigs2Bz5crF3Llzsba2xtjYmAULFnD+/HlWrFhBnjx5GDRoEIcOHaJNmzYAFC9enKVLl/L8+XMKFy7MkSNHWLRoEa6urpQtW1ajYxobGxMSEoKHhwcVKlTg1atXjB49mty5czNjxgz09NKVq39UeHh4utenEamTvtQe6UvtkH7UnqzQl/ny5WPevHk0atQozTuIuiZjqtCU9KX2SF9qh/Sj9uiyLzWeOjtt2jQePXrE0aNHKVWqFKVKlVJt09PTo23btpw4cYIZM2boJFCAjh07Eh4ezoIFCwgNDaVcuXJ4eXlRpEgRILlE+vuKFSuGl5cXkydPxtPTE2tra9zc3FTrfWlyTIC5c+eir6+Pk5OTanHp1atXy3pfQgghPouvry/m5ubUq1cPOzs7ihcvnmol94ULF+osBhlThRBC6JLG62ja2NjQt29fpkyZQnh4OCVLlmTfvn04ODgAsH79embMmJFqNVqhGZlvrj3Sl9ojfakd0o/akxX6Mm/evJ9so1AoCA8P/wrRZE1Z4fcks5C+1B7pS+2QftQeXfalxnc0X79+rVraJDWxsbEkJiZqJSghhBAiK3v58mVGhyCEEELolMYPQ5QoUYKrV6+muf306dOUK1dOK0EJIYQQQgghhPh2aZxo9unTh23btuHl5UVSUhKQPK0nKiqK6dOnc/r0aZycnHQWqBBCCJHVnDlzhlmzZjFixAju3r0LwJs3b/Dz8/usJT2EEEKIzELjqbODBg3izp07DBo0iFy5cgHQr18/IiIiSExMZMCAAfTo0UNngQohhBBZRXR0ND179lRb5qNTp06ULl0aQ0ND+vTpw88//8zEiRMzMEohhBDi82mcaAIsXryYn376ib1793L//n2SkpIoXrw4HTp0wN7eXlcxCiGEEFnKrFmz8PX1Ze3atdSuXZuKFSuqthkaGtK+fXuOHj0qiaYQQohvVroSTYCaNWvKospCCCHEF9i3bx8DBgygc+fOqVaWtbGxYc+ePRkQmRBCCKEdGj+j2aZNGzZt2iSV8oQQQogvFBYWRpkyZdLcrlAoiImJ+YoRCSGEENqlcaL55MkTRo0aRZkyZXB0dGTHjh28fv1al7EJIYQQWVKhQoUICgpKc/vFixcpUaLEV4xICCGE0C6NE80rV67g4+PD4MGDCQoKwtnZmdKlS9OrVy/27dtHdHS0LuMUQgghsgxHR0c2bdrEhQsXVO8pFAoA1q9fz759++jWrVtGhSeEEEJ8sXQ9o2lra4utrS0zZ87k0qVLeHt7c+DAAQ4dOkSOHDlo0aIF69at01WsQgghRJYwZswYLl++TOvWrSlVqhQKhQIXFxfCw8MJDQ2lefPmDBkyJKPDFEIIIT6bxnc0P2RnZ8e8efO4efMmS5cuRU9PTwoXCCGEEBowNDRk165drF69mlKlSlG6dGkSEhKoUqUKq1atYtu2bejpffYQLYQQQmS4dFedfefRo0fs3bsXb29vrl27hp6eHvXq1dNmbEIIIUSW5ujoiKOjY0aHIYQQQmhduhLNp0+fsm/fPvbu3UtAQACQvNyJm5sb7du3x8LCQidBCiGEEFnJ0KFD6dy5Mw4ODnLnUgghRJakcaLZsmVL/P39SUpKwtbWlhkzZtCxY0cKFiyoy/iEEEKILOfAgQNs374dMzMz2rVrR4cOHahTp05GhyWEEEJojcaJZkREBJMmTaJTp04UL15clzEJIYQQWdq9e/c4duwYe/fuZfv27Xh6epI/f37at29Px44dqVatWkaHKIQQQnwRjRPN8+fP6zIOIYQQ4rthbGxMu3btaNeuHVFRURw5cgRvb2/Wr1/PqlWrKFKkCJ06dWLq1KkZHaoQQgjxWdJdDOjOnTscP36chw8fAlCkSBGaNm1K2bJltR6cEEIIkdVlz56dzp0707lzZ16/fs2OHTuYNWsWixcvlkRTCCHEN0vjRFOpVDJu3Dg2bNiAUqlUFS9ISkpi+vTp9OvXjwULFqgWnBZCCCGEZqKjozl27Bje3t6cPHmS6OhoSpQokdFhCSGEEJ9N40Rz6dKleHp60r17d4YNG4aNjQ2Q/JyJu7s7np6eFC5cmJEjR+ostZ8phQAAIABJREFUWCGEECKriIuL48SJE+zdu5ejR4/y9u1bChYsSP/+/enUqRO2trYZHaIQQgjx2TRONLds2ULbtm1xd3dXe79cuXKsWLGCyMhINm/eLImmEEII8QmDBw/myJEjvH79GktLS7p160anTp2oVatWRocmhBBCaIXGiebjx48ZOnRomtsdHBw4duyYVoISQgghsrJjx47Rvn17OnXqxI8//ihraQohhMhyNE40LSwsCAwMTHN7YGAgFhYWWglKCCGEyMru3buHgUG66/EJIYQQ3wyNR7kOHTrg7u5OoUKFGDRoELlz5wbg9evXrFmzhq1bt370jqcQQgghkr1LMiMiIjhz5oxaJff69etjamqakeEJIYQQX0zjRHPy5MncuHGDOXPm4ObmhqWlJQDPnj0jMTGRBg0aMGnSJJ0FKoQQQmQlS5cuZd68ecTGxqJUKlXvGxsbM2nSJEaMGJGB0QkhhBBfRuNE08TEhL1793LkyBFOnDjBo0ePAGjWrBnNmjWjefPmOgtSCCGEyEo2b97M9OnTcXBwwNnZmTJlygAQFBTE6tWrmT59Onnz5qVXr14ZHKkQQgjxedJdfaBly5YsXryY3bt3s3v3bhYvXvxVkszY2FjGjx9PiRIlKFCgAD/99BNPnjz55H4eHh5UrlwZKysrHBwcOH/+vGrby5cvGT9+PDVq1MDa2poKFSowZswYwsPD1Y5RqVIlTE1N1X6mT5+u7Y8ohBDiO7F69WocHBzYu3cvzZo1o1ixYhQrVoxmzZrh7e3Njz/+yKpVq3R2fhlThRBC6No3U+Zu0qRJHDx4kPXr16tKwnft2pXExMQ09/H29sbFxYWxY8dy7tw57OzscHR0VN2Nffr0KU+fPmXGjBmcP3+eNWvWcP78efr375/iWBMmTCAoKEj1M27cOJ19ViGEEFnb/fv3adWqFQqFIsU2hUJB69atuX//vs7OL2OqEEIIXfsmSt69evWKLVu24O7uToMGDQD+H3t3HldT/v8B/HWLSolL26VkyygpLYQMWTPVjD3FGPtEsi9NlrFNlGUsQ9myDcZI1qwzRnZfGUnD0GQoNCTlaiGl7u8PD+c3dyqunLqV1/Px6PFwz+dzzud9PnOnd+97zv0crFu3DjY2Njh16hS6dOlS5H4hISEYOHAghgwZAgBYsmQJfvvtN2zatAlz5sxBs2bNsH37dqF/o0aNMH/+fHh5eSEjI0NY8AgA9PX1YWJiUopnSUREH4uaNWsiMTGx2PbExETUrFmzVMZmTiUiorJQIa5oxsbGIi8vD507dxa2mZmZoWnTprh06VKR++Tm5iI2NlZpHwDo3LlzsfsAr1fR1dbWhq6urtL2VatWoWHDhvj000+xdOlS5ObmfsAZERHRx+yzzz7Dhg0bsGvXLqWFgBQKBcLDwxEWFgY3N7dSGZs5lYiIykKFuKL5+PFjaGpqwsDAQGm7kZERHj9+XOQ+aWlpyM/PL/Rsz7ftI5fLsWDBAgwePFjp+WajRo2Cra0tateujZiYGMydOxdJSUlYtWrVB54ZERF9jObMmYPLly/D19cX3377LRo1agTg9S21T548gaWlJebMmVMqYzOnEhFRWVBroRkYGIilS5e+tU9kZGSZxJKVlYUBAwagTp06mD9/vlLb2LFjhX83b94c+vr6GDZsGObNm4fatWsXebyEhIQSxVHS/agwzqV4OJfi4DyK50PmskmTJiJGUjK1a9dGVFQUNm/erLSSu42NDbp3744hQ4ZAW1v7vY7JnCreflQY51I8nEtxcB7FU1o59YMLzUePHuHZs2fC0uzvw9fXF/37939rHzMzM1y+fBn5+flIS0uDoaGh0Jaamoq2bdsWuZ+BgQE0NTWRmpqqtD01NVV4BugbWVlZ8PT0BADs2rULOjo6b43J0dERwOtPnotLiiX5QyYhIaFc/AFUGXAuxcO5FAfnUTyVZS61tbUxevRojB49WpTjMacqqyzvk/KAcykezqU4OI/iKc25VLnQ3LJlC6KjoxEaGipsmzZtGjZu3Ajg9aeS+/btK3QrztsYGBio1N/Ozg5Vq1ZFVFSUkLySk5MRHx+P1q1bF7mPlpYW7OzsEBUVhV69egnbo6Ki0KNHD+F1ZmYmPD09oVAoEBERgerVq78znj/++AMAuJABERGVG8ypRERUnmgGBATMVaXjuHHj0LBhQ7i6ugIAzp49i2nTpsHT0xP9+vXD4cOHkZ2dja5du4oepI6ODh49eoSwsDBYW1vj2bNnmDRpEmrUqIF58+ZBQ+P1mkatWrUC8P+fjurr6yMoKAgymQw6OjpYsmQJLly4gNWrV6NmzZrIzMxEnz59kJGRgU2bNkEikSA7OxvZ2dnQ0tKCpqYmoqOjsX//fujo6ODFixeIiorCN998g/bt22P48OGinmd6evp7FepUPM6leDiX4uA8iodz+WGYU+l9cS7Fw7kUB+dRPKU5lypf0UxKShKWNAeAffv2wdTUFGvXroWGhgaePXuGffv2ISgoqFQCDQoKgqamJoYNG4acnBx06NABa9euhaamptAnISEBaWlpwus+ffogPT0dS5YsQUpKCqysrBAeHg5zc3MAr1feu3z5MoD/T6RvREZGon379tDS0sK+ffuwaNEi5Obmol69ehg8eDAmTJhQKudJRERU2phTiYiotEnkcrni3d0AU1NTLFy4UCg27e3t4eLighUrVgAAtm3bhmnTpuHRo0elF20lx/vNxcO5FA/nUhycR/FwLkkVfJ+Ih3MpHs6lODiP4inNuVT5OZr169fH6dOnAQBXr15FYmKi0vO0Hj9+DH19ffEjJCIiIiIiogpF5UJz+PDh2LdvH5ydndG7d2+YmpoK39cEgP/973+wtLQslSCJiIgqk0WLFuHPP/8stv3mzZtYtGhRGUZEREQkLpULzZEjR2LlypVo1KgR3N3dsXfvXmHJ8qdPnyI1NVVYvY6IiIiKFxwcjBs3bhTbzkKTiIgquvd6jubgwYMxePDgQttr1aqFU6dOiRUTERHRRy0rKwtVq1ZVdxhEREQl9l6FJgBkZGTgypUrSE1NRceOHQs9qJmIiIgKu379uvDMSAC4ePEiXr16VaifXC7Hpk2buNAFERFVaO9VaH7//fdYtmwZnj9/DolEgn379sHY2BhpaWlo3rw5FixYIPpzsIiIiCqDQ4cOCbfDSiQSbN68GZs3by6yr1Qqxfr168syPCIiIlGpXGhu2rQJgYGBGDx4MDp16oRhw4YJbQYGBnB3d8f+/ftZaBIRERVh6NCh+Oyzz6BQKNC5c2fMmDED3bp1K9RPT08PDRs2RJUq733TERERUbmhchZbt24devXqhZUrVyI9Pb1Qu62tLdasWSNqcERERJWFTCaDTCYDAERGRqJp06YwMjJSc1RERESlQ+VVZxMTE+Hi4lJsu1QqxdOnT0UJioiIqDL79NNPWWQSEVGlpvIVTalUitTU1GLbb968CRMTE1GCIiIiqkz8/PwgkUiwcuVKaGpqws/P7537SCQSrF69ugyiIyIiEp/Khaarqyu2bt2KkSNHFmq7fv06fvzxxyIffUJERPSxO3PmDDQ0NFBQUABNTU2cOXMGEonkrfu8q52IiKg8U7nQnDVrFqKiotC2bVu4urpCIpFgx44d2Lp1Kw4fPoy6devC39+/NGMlIiKqkP79WJOiXhMREVU2Kn9H08TEBKdOnUL37t0RGRkJhUKB3bt348SJE/D09MSvv/6K2rVrl2asREREFVKHDh1w4sQJ4fXOnTuRlJSkxoiIiIhKl0pXNHNzc3H58mXIZDKsXLkSK1euxJMnT1BQUABDQ0NoaKhcrxIREX10bty4gSdPngiv/fz8sG7dOtSvX1+NUREREZUelSrEKlWqoFevXjh58qSwzdDQEMbGxiwyiYiI3sHc3BwnT55EVlYWAEChUPA7mEREVKmpVCVqaGjA3NxcSJBERESkOh8fH+zevRvm5uaoXbs2JBIJfHx8ULt27WJ/DAwM1B02ERFRiam8GNDo0aOxevVqDBo0iM/+IiIieg++vr6wt7fHuXPn8PjxY4SFhaFjx45o3LixukMjIiIqFSoXms+fP4euri4cHBzg4eGBBg0aoFq1akp9JBIJxo8fL3qQREREFV2bNm3Qpk0bAMCGDRswYMAAeHp6qjkqIiKi0qFyoTl37lzh37t27SqyDwtNIiKid3v69Km6QyAiIipVKhea165dK804iIiIPjq//PILfvnlF9y7dw/A60WDPvvsM3Tt2lXNkREREX0YlQtNc3Pz0oyDiIjoo5GTk4MhQ4bg119/hYaGBmQyGQDg5MmT2LRpE7p164Yff/wR2traao6UiIioZPhsEiIiojIWFBSEX375Bf7+/rhz5w6uX7+O69ev4+7duwgICMCvv/6K4OBgdYdJRERUYipf0QSAP//8E+vWrUNsbCwyMjJQUFCg1C6RSBAbGytqgERERJXNnj17MGjQIAQEBCht19fXh7+/P+7fv4/du3djzpw5aoqQiIjow6h8RfPixYvo3Lkzjh49CplMhsTERDRo0AB16tTB/fv3oaenB2dn59KMlYiIqFJITU2Fvb19se12dnZITU0tw4iIiIjEpXKhuWDBAtSrVw+XL19GaGgoAGDy5Mk4duwYjh49iuTkZPTr16/UAiUiIqosTE1NcebMmWLbz5w5A1NT0zKMiIiISFwqF5qxsbH46quvULNmTWhovN7tza2zrVu3xpAhQ7BgwYLSiZKIiKgSGThwIA4cOIBx48bh5s2byMvLQ15eHm7evInx48cjMjISgwYNUneYREREJaZyoSmRSFCzZk0AgK6uLgAgPT1daLewsMDNmzdFDu//vXz5EtOmTUOjRo1Qt25deHt7Izk5+Z37hYWFwdbWFiYmJnBxccGFCxeU2j08PCCVSpV+hg8frtRHLpfDx8cH5ubmMDc3h4+PD+RyuajnR0REH4/Jkydj0KBB2L59O9q1aweZTAaZTIZ27dph27ZtGDRoECZNmlRq4zOnEhFRaVO50DQ3N0diYiIAQFtbG/Xr10dUVJTQfuHCBdSuXVv0AN+YPn06IiMjsXHjRhw5cgSZmZnw8vJCfn5+sfvs3bsXAQEBmDJlCs6cOQMnJyd4enri/v37Sv2+/PJLxMfHCz/Lly9Xah85ciTi4uIQERGBiIgIxMXFYdSoUaVynkREVPlpaGhg1apVOHfuHL799lsMGTIEQ4YMwbfffotz587hhx9+gEQiKbXxmVOJiKi0qbzqbOfOnbFv3z5hBbwhQ4Zg/vz5uHfvHhQKBc6dO4eJEyeWSpDPnj3Dtm3bEBISgk6dOgEA1q1bBxsbG5w6dQpdunQpcr+QkBAMHDgQQ4YMAQAsWbIEv/32GzZt2qS0kp+uri5MTEyKPEZ8fDxOnDiBY8eOwcnJCQCwfPlyuLm5ISEhAU2aNBHzVImIqJJ7/vw5vLy84OXlhUGDBsHa2rpMx2dOJSKisqDyFc0pU6Zgy5YtyMvLAwBMnDgRM2fOxNOnT5GZmYmAgADMmDGjVIKMjY1FXl4eOnfuLGwzMzND06ZNcenSpSL3yc3NRWxsrNI+wOuC+b/77NmzB40aNUKbNm0wa9YsZGZmCm3R0dGoXr06WrduLWxr06YN9PT0ih2biIioOLq6urh27dpbrx6WJuZUIiIqCypf0ZRKpbCzsxNeSyQSTJ06FVOnTi2VwP7t8ePH0NTUhIGBgdJ2IyMjPH78uMh90tLSkJ+fDyMjo7fu4+npiXr16kEmk+HWrVuYN28ebty4gX379gljGxgYKN3CJJFIYGhoWOzYAJCQkPDe5/kh+1FhnEvxcC7FwXkUz4fMZXm4aubs7IwLFy4IVwfLEnMqlQTnUjycS3FwHsVTWjlV5UKzNAQGBmLp0qVv7RMZGVmqMQwdOlT4t7W1NRo0aIAuXbogNjZWqbB+XyX5Q4a3DYmHcykezqU4OI/iqQxzuXjxYvTp0wfffvstRowYAXNzc2FF95JiTlVWGd4n5QXnUjycS3FwHsVTmnP5XoVmfHw8duzYgcTERMjlcigUCqV2iUSCgwcPqnw8X19f9O/f/619zMzMcPnyZeTn5yMtLQ2GhoZCW2pqKtq2bVvkfgYGBtDU1Cz0wOvU1FQYGxsXO569vT00NTVx584d2NnZwdjYGGlpaVAoFMInsAqFAk+ePHnrcYiIiIrj5OQEhUKBkJAQhISEQENDA1WrVlXqI5FI8M8//6h8TOZUIiIqT1QuNH/++Wf4+fmhatWqsLCwgFQqLdTnv4XnuxgYGBS6dacodnZ2qFq1KqKiouDp6QkASE5ORnx8vNL3PP5NS0sLdnZ2iIqKQq9evYTtUVFR6NGjR7Fj3bhxA/n5+cJCBk5OTsjKykJ0dLQwVnR0NLKzs4sdm4iI6G169+4t+qqyzKlERFSeqFxoBgcHw9bWFhERESolMjHVrFkTX331FebMmQMjIyPUqlULM2fOhLW1NTp27Cj0a9WqFb7++mv4+PgAAPz8/DBq1Cg4OjqidevW2LRpEx49eoRhw4YBAO7evYvw8HC4urqidu3aiI+Px6xZs2Bra4s2bdoAAJo2bYquXbti0qRJWLFiBQBg0qRJ6N69Oy/ZExFRiaxZs0ZtYzOnEhFRWVC50Hz06BHGjRtX5kXmG0FBQdDU1MSwYcOQk5ODDh06YO3atdDU1BT6JCQkIC0tTXjdp08fpKenY8mSJUhJSYGVlRXCw8Nhbm4OAKhatSpOnz6NtWvXIjs7G6ampnB1dUVAQIDSccPCwuDv74++ffsCANzc3LB48eIyOnMiIqoscnJycOTIESQlJcHAwACurq6QyWRlHgdzKhERlTaJXC5X6X7XLl26oFOnTpg1a1Zpx0RERFTpPHz4EO7u7khKShK+aqKrq4uff/4Z7du3V3N0RERE4lJ5ibsFCxZg+/bt+N///lea8RAREVVKgYGBuHfvHsaMGYNdu3YhKCgIOjo6+Oabb9QdGhERkeiKvXX2zQIB/6avrw93d3dYWFjAzMxM6VYY4PUKeeHh4eJHSUREVMGdOnUKAwYMQGBgoLDN2NgYI0eORHJyMkxNTdUYHRERkbiKLTRv3bpV5Ip4ZmZmyMnJwe3btwu1ib2CHhERUWWRkpJSaGXVNm3aQKFQ4MGDByw0iYioUim20Pzjjz/KMg4iIqJKLT8/Hzo6Okrb3rzOyclRR0hERESlRuVVZ4mIiOjDJCYm4sqVK8LrjIwMAK9XeK1evXqh/o6OjmUWGxERkZhUXgzov86ePYtx48bB09MTM2fOxP3798WM66MTFhYGW1tbmJiYwMXFBRcuXFB3SOVaUFAQpFKp0s8nn3witCsUCgQFBcHS0hIymQweHh64efOmGiMuP86fPw9vb29YWVlBKpVix44dSu2qzJ1cLoePjw/Mzc1hbm4OHx8fyOXysjyNcuFdc+nr61vofdq1a1elPi9fvsS0adPQqFEj1K1bF97e3khOTi7L01C7ZcuWoVOnTqhXrx4aN24MLy8v/Pnnn0p9Ksv7MigoCN26dRN+3jziw9/fX2l7165d0a1bNzVHW3Exp74f5tSSY04VD3OqOMpTTn1roRkcHIw6dergyZMnStt37NiBnj17Yvv27Thx4gRCQ0PRuXNn3Lt3770Gp9f27t2LgIAATJkyBWfOnIGTkxM8PT1ZvL9DkyZNEB8fL/z8+w+JlStXIiQkBIsWLcLJkydhZGSE3r17IzMzU40Rlw/Z2dlo1qwZgoODUa1atULtqszdyJEjERcXh4iICERERCAuLg6jRo0qy9MoF941lwDQsWNHpffp7t27ldqnT5+OyMhIbNy4EUeOHEFmZia8vLyQn59fFqdQLpw7dw4jRozA8ePHcfDgQVSpUgW9evXC06dPhT6V4X0ZEhKC1atXF/opavubbfT+mFNLhjm1ZJhTxcOcKo7ylFPf+hxNDw+PQp8ovHz5Ek2aNIGGhgZ+/PFHODo64pdffsGYMWPg5eWFFStWvFcA9PoZpdbW1vjhhx+EbQ4ODujZsyfmzJmjxsjKr6CgIBw8eBAXL14s1KZQKGBpaYmvv/4aU6dOBQC8ePECTZo0wXfffYdhw4aVdbjllqmpKRYvXowvv/wSgGpzFx8fj9atW+PYsWNo06YNAODixYtwc3PD5cuX0aRJE7Wdjzr9dy6B15++pqenY9euXUXu8+zZM1hYWCAkJAT9+/cHADx48AA2NjaIiIhAly5dyiT28iYrKwvm5ubYsWMH3Nzc+L6k98Kc+v6YU8XBnCoe5lTxqDOnvvWK5p07d2BnZ6e07fTp08jMzMT48ePRoUMH6OnpoXfv3ujfvz9OnTpVgtP/uOXm5iI2NhadO3dW2t65c2dcunRJTVFVDImJibC0tIStrS2GDx+OxMREAEBSUhJSUlKU5rRatWpwdnbmnL6DKnMXHR2N6tWrK62e2aZNG+jp6XF+i3Dx4kVYWFjA0dER48ePR2pqqtAWGxuLvLw8pfk2MzND06ZNP+q5zMrKQkFBAaRSKQC+L0l1zKklx5wqPv7uEh9z6vtTZ059a6H59OlTyGQypW1nz56FRCJB9+7dlbbb2dnh0aNHKg9Mr6WlpSE/Px9GRkZK242MjPD48WM1RVX+tWzZEqGhoYiIiMAPP/yAlJQUuLq6Ij09HSkpKQDAOS0BVebu8ePHMDAwUHqckUQigaGhIef3P7p27Yq1a9fiwIEDCAwMxJUrV9CjRw+8fPkSwOu51NTUhIGBgdJ+H/t7NSAgADY2NnBycgLA9yWpjjm1ZJhTSwd/d4mLObVk1JlT37rqrImJCR4+fKi07eLFi9DV1YWlpaXSdg0NDWhpaak8MNGH+O8iGS1btoSdnR1++ukntGrVSk1RESl7s9ALAFhbW8POzg42NjY4fvw4evToocbIyq8ZM2bgf//7H44dOwZNTU11h0P0UWBOpYqAOfX9qTunvvWKpqOjI3bu3CmsMHT9+nVcvXoVLi4uhYKNj4/nw6ZLwMDAAJqamkqX/gEgNTUVxsbGaoqq4qlevTosLS1x584dmJiYAADntARUmTtjY2OkpaVBofj/r3crFAo8efKE8/sOderUQd26dXHnzh0Ar+cyPz8faWlpSv0+1vfq9OnTsWfPHhw8eBANGjQQtvN9SapiThUHc6o4+LurdDGnvl15yKlvLTS/+eYbPHz4EI6OjnB3d4ebmxskEgkmTpyo1E+hUODQoUNK9/GSarS0tGBnZ4eoqCil7VFRUZzP95CTk4OEhASYmJigfv36MDExUZrTnJwcXLx4kXP6DqrMnZOTE7KyshAdHS30iY6ORnZ2Nuf3HdLS0vDw4UPhl7ydnR2qVq2qNN/JycnCl/A/Jt98842QEP/9WAWA70tSHXOqOJhTxcHfXaWLObV45SWnagYEBMwtrtHQ0BAdOnRAYmIi/vnnH1hbW2PZsmVwdnZW6nf27FmcOXMGvr6+aNiwocqD02v6+voICgqCTCaDjo4OlixZggsXLmD16tWoWbOmusMrl2bNmgUtLS0UFBTg9u3bmDZtGu7cuYPly5dDKpUiPz8fK1asQOPGjZGfn4+ZM2ciJSUFK1asgLa2trrDV6usrCzcunULKSkp2LZtG5o1a4YaNWogNzcXNWvWfOfcGRoa4vfff0dERARsbGyQnJyMSZMmwcHB4aNbjv1tc6mpqYn58+ejevXqePXqFf744w+MGzcO+fn5WLJkCbS1taGjo4NHjx4hLCwM1tbWePbsGSZNmoQaNWpg3rx50NAo8aOOK5SpU6fi559/xpYtW2BmZobs7GxkZ2cDeF04SCQSvi9JZcyp7485teSYU8XDnCqO8pRT3/p4Eyo7YWFhWLlyJVJSUmBlZYWFCxeiXbt26g6r3Bo+fDguXLiAtLQ0GBoaomXLlpg5c6bw3WGFQoHg4GBs2bIFcrkcjo6OWLp0KZo1a6bmyNXv7Nmz+OKLLwptHzBgANasWaPS3Mnlcvj7++Po0aMAADc3NyxevFhY0exj8ba5XLZsGb788kvExcXh2bNnMDExQfv27TFz5kyYmZkJfV++fIlZs2YhIiICOTk56NChA77//nulPpVdce+bb775BtOnTweg2v/TfF/SG8yp74c5teSYU8XDnCqO8pRTWWgSERERERGRqD6Oa8hERERERERUZlhoEhERERERkahYaBIREREREZGoWGgSERERERGRqFhoEhERERERkahYaBIREREREZGoWGgSUblgY2ODvn37qjsMIiKiCo85lcoDFppEpWTHjh2QSqXCj4mJCSwtLdGnTx+sXbsWmZmZ6g6RiIioQmBOJap4qqg7AKLKLiAgAA0bNkReXh4eP36Mc+fOYfr06QgJCcHOnTvRvHlzdYdIRERUITCnElUcLDSJSlmXLl3QqlUr4fXkyZNx+vRpeHt7Y8CAAYiOjka1atXUGOHHQ6FQICcnh/NNRFRBMaeWH8yp9C68dZZIDVxcXDBt2jTcv38f4eHhwvbr169jzJgxsLOzg4mJCRo1aoThw4fj/v37Qp+///4bUqkUq1evLnTc69evQyqVYuPGjcWOnZSUBKlUiuXLl2Pr1q2ws7ODsbExOnXqhJiYGKW+Hh4e8PDwKHQMX19f2NjYFHnMsLAwtGjRAnXq1EHPnj1x7949KBQKfP/997C2toZMJoO3tzfS0tKKjO/06dNwcXGBiYkJHB0dsXPnzkJ9Xr58ieDgYDg4OMDY2BhWVlaYPn06nj9/rtRPKpVi0qRJ2Lt3L5ydnWFsbIy9e/cWOzdERFTxMKcyp1L5xCuaRGri5eWF+fPn4+TJkxgyZAgAICoqCrdv34a3tzfq1KmDu3fvYtOmTbhy5QouXrwIXV1dNG7cGE5OTggPD8fYsWOVjhkeHg4tLS306dPnnePv3bsX2dnZGDZsGCQSCVauXImvvvp4BRdhAAAgAElEQVQKsbGxqFq1aonOac+ePcjNzcXXX38NuVyOH374AUOHDkWXLl1w6tQpjB8/Hnfv3sW6deswY8YMrFu3Tmn/xMREDB48GEOGDIG3tzd2794NX19faGtrC+ekUCgwaNAgnD9/HoMHD4alpSXi4+OxceNG3Lp1C3v37oVEIhGOeeHCBRw4cABff/01TExM8Mknn5To3IiIqPxiTmVOpfKHhSaRmpiamqJGjRq4e/eusG3EiBEYN26cUj83Nzd0794dkZGR8PLyAgB4e3tj8uTJuHXrFiwtLQEABQUF2LNnD1xdXVGrVq13jp+cnIyYmBhIpVIAgIWFBQYOHIjffvsNn332WYnO6Z9//lE6ZkFBAZYtW4YXL17gzJkzQrJ98uQJ9u7dixUrVijdcvP3338jLCwM/fr1AwAMHToUHTp0wOzZs9GrVy9oaGggIiICJ06cQGRkJD799FNhX3t7e/j4+CAqKgqdO3cWtv/11184ffo0bG1tS3RORERU/jGnMqdS+cNbZ4nUqHr16sjKyhJe6+rqCv/OyspCeno6LCwsULNmTcTGxgptffr0gba2Nnbt2iVsO3v2LJKTk4XE+S49evQQkhcAODs7A3j9CWhJ/feYjo6OAID+/fsrfaLr6OiIvLw8JCcnK+1vZGSk9MlxtWrVMHjwYDx48ADXr18HAOzbtw8WFhawsrJCWlqa8NOuXTtIJBKcPXtW6ZitW7dmQiQi+ggwpzKnUvnCK5pEapSVlQVDQ0PhtVwux9y5c3HgwAE8ffpUqW9GRobwb6lUCjc3N+zevRuzZ8+GRCJBeHg4atWqhe7du6s0tpmZmdLrN8lMLpeX9HQKHbNGjRoAXn/SXNT2/47VsGFDaGgof/7VuHFjAMC9e/dga2uLv//+GwkJCcL2/0pNTVV63aBBg/c7CSIiqpCYU5lTqXxhoUmkJsnJycjIyECjRo2EbUOHDsWlS5fg5+cHW1tb6OvrQyKRYPjw4SgoKFDa39vbG/v378f58+fRsmVLREZGol+/ftDS0lJpfE1NzSK3KxQK4d8SiUTp9Rv5+fnvdUxVxlJVQUEBLC0tERwcXGS7TCZTes3V8IiIKj/mVOZUKn9YaBKpyZtbdN5890Eul+PUqVMICAhAQECA0C8nJ6fIT0S7du0KIyMj7Nq1C6mpqcjIyFD5Fh9VSaXSIm/7+feKfWK6e/cuCgoKlD6B/fvvvwEA5ubmAF5/QhsbGwsXFxelBQqIiOjjxZxaGHMqqRu/o0mkBqdPn8aSJUtQv3599O/fHwCERPDfTyRDQ0MLffIKAFWqVIGnpycOHDiAbdu2oVGjRmjdurWocTZs2BAJCQl48uSJsO2PP/7ApUuXRB3njdTUVKWl0l+8eIEff/wRpqamwkO4e/fujcePHxe53PzLly+RmZlZKrEREVH5xJxaNOZUUjde0SQqZb/99hvu3LmDV69eITU1FWfOnEFUVBTq1auHnTt3QkdHB8Dr71h8+umn+OGHH5CXl4d69erh4sWLuHDhAmrXrl3ksb29vREaGoqTJ08qfWIrlkGDBiEkJAR9+vTBV199hdTUVGzevBmWlpalknwaN26MKVOmIC4uDnXr1kV4eDgSEhKwYcMG4Y8GLy8vHDhwAFOnTsX58+fRpk0bKBQK3L59G/v27cOWLVvQvn170WMjIiL1Y05VHXMqqRsLTaJS9uZ7D1paWqhVqxaaNWuGoKAgfPnll9DX11fqGxYWhoCAAGzevBmvXr2Cs7MzDh48iJ49exZ5bFtbW1hbW+PGjRui3+IDAE2bNsXatWuxcOFCzJw5E02bNsW6deuwe/dunDt3TvTxGjRogGXLlmH27Nm4desWTE1NERISAk9PT6GPhoYGtm/fjjVr1mDnzp04cuQIdHR00KBBA4wYMUL4lJaIiCof5lTVMaeSuknkcvn7f3OYiMqNTp06QUtLC8ePH1d3KERERBUacyqRePgdTaIKLC4uDlevXsWAAQPUHQoREVGFxpxKJC5e0SSqgP7880/ExsYiNDQUKSkpuHbtmtKDqYmIiEg1zKlEpYNXNIkqoAMHDsDPzw85OTnYuHEjEyIREVEJMacSlQ5e0SQiIiIiIiJR8YomERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJioUmERERERERiYqFJhEREREREYmKhSYRERERERGJSrRC8+LFi9iwYYPStj179qBly5Zo0qQJAgICUFBQINZwREREREREVE6JVmguWLAAFy5cEF7fvn0bvr6+0NDQgJ2dHdavX4+1a9eKNRwRERERERGVU6IVmrdu3YKjo6Pw+ueff4aOjg5OnDiB3bt3w8vLC9u3bxdrOCIiIiIiIiqnRCs0MzMzIZVKhde//fYbOnXqhBo1agAA2rZti3v37ok1HBEREREREZVTohWaMpkM8fHxAICHDx8iLi4OnTt3FtozMjJQpUoVsYYjIiIiIiKickq0yu+LL77Ahg0b8PLlS1y5cgU6Ojpwd3cX2q9fv4769euLNRwRERERERGVU6Jd0Zw+fTp69OiB8PBwpKamIjQ0FEZGRgBeX82MjIxEp06dxBquUkpISFB3CJUG51I8nEtxcB7Fw7kkVfB9Ih7OpXg4l+LgPIqnNOdStEJTT08P69evR2JiIuLi4tCzZ0+hrXr16vjzzz8xc+ZMlY93/vx5eHt7w8rKClKpFDt27FBqVygUCAoKgqWlJWQyGTw8PHDz5k2lPjY2NpBKpUo/c+fOVepz//59eHl5oW7dumjUqBH8/f2Rm5ur1OfcuXNwcXGBiYkJWrRogU2bNql8HkREROomRk6Vy+Xw8fGBubk5zM3N4ePjA7lcrtTnxo0bcHd3h0wmg5WVFRYtWgSFQqHU58CBA2jdujWMjY3RunVrREZGls5JExGRWolWaG7evBnPnj0rehANDdSsWRNVq1ZV+XjZ2dlo1qwZgoODUa1atULtK1euREhICBYtWoSTJ0/CyMgIvXv3RmZmplI/f39/xMfHCz9Tp04V2vLz8+Hl5YWsrCwcOXIEGzduxMGDB5UK4sTERPTv3x9OTk44c+YMJk+eDH9/fxw4cEDlcyEiIlInMXLqyJEjERcXh4iICERERCAuLg6jRo0S2jMyMtC7d28YGxvj5MmTCA4OxqpVq7B69WqhT3R0NIYPHw5PT0+cPXsWnp6eGDp0KH7//ffSnQAiIipzohWakydPRtOmTTFkyBAcOXIEr169+qDjubq6Yvbs2ejZsyc0NJTDVCgUWLNmDSZOnIiePXuiWbNmWLNmDbKyshAREaHUV19fHyYmJsJP9erVhbaTJ0/i5s2bWLduHezs7NCpUyfMmzcPP/74IzIyMgC8LqBlMhmWLFkinN+AAQOUEicREVF59qE5NT4+HidOnMCKFSvg5OQEJycnLF++HMePHxduu9q9ezdevHiBNWvWoFmzZujZsycmTJiA0NBQ4armmjVr0L59e0ydOhVNmzbF1KlT8emnn2LNmjVlOyFERFTqRFsM6OzZswgPD8fevXtx8OBB1K5dG3369IGXlxdatmwp1jAAgKSkJKSkpCitalutWjU4Ozvj0qVLGDZsmLB91apVWLZsGUxNTdGrVy+MHz8eWlpaAF5/stq0aVOYmZkJ/bt06YKXL18iNjYWHTp0QHR0tNI4b/rs3LkTeXl573WVloiIqLxRJadGR0ejevXqaN26tdCnTZs20NPTw6VLl9CkSRNER0ejbdu2SldMu3TpggULFiApKQkNGjTA5cuX4ePjozR+ly5dsH79+lI7P6m0Zqkd++Mh7t9xHzfOpTg4j2K5fLn0ji1aodm8eXM0b94c8+bNw5kzZxAeHo5du3Zh48aNaNSoEby8vODp6YkGDRp88FgpKSkAICw29IaRkREePnwovB41ahRsbW1Ru3ZtxMTEYO7cuUhKSsKqVasAAI8fPy50DAMDA2hqauLx48dCn44dOxYa59WrV0hLS4NMJvvg8yH62OTn5+Ply5fqDqNMaWlp4fnz5+oOo1JQZS61tbWhqalZRhFVbKrk1MePH8PAwAASiURol0gkMDQ0VMqXdevWLXSMN20NGjRASkpKkeO8OQYREVUeoj/YUiKRwMXFBS4uLli2bBmOHj2Kbdu2ISgoCEFBQWjdujUGDBiA/v37Q0dHR+zhlYwdO1b4d/PmzaGvr49hw4Zh3rx5qF27dqmOXdIVnLiKlng4l+IRey51dHRQrVo1pT9aK7vq1avjxYsX6g6jUnjXXCoUCqSlpSEnJ6fI9iZNmpRWaFRKPiyn8soHEdHbfMjfeW/LqaIXmv925coVREVF4ffff4dCoYC1tTVevnyJCRMmYOHChdi4cSPatWv33sc1MTEBAKSmpqJevXrC9tTUVBgbGxe7n6OjIwDgzp07qF27NoyNjXHp0iWlPmlpacjPzxeOY2xsjNTUVKU+qampqFKlCgwMDIodqyR/yCQkJPAPIJFwLsUj9lw+f/4c1apVw6tXr3D37l3k5eUVWpWyMnrx4kWRi7DQ+1NlLjU1NaGjo4OGDRuWUVQVlyo51djYGGlpaVAoFMIHRAqFAk+ePHlnvnzT9masovq8LXcDzKlERKWptH5Xil5o/vXXX9i1axd2796NBw8ewNDQEIMGDYK3tzdsbGwAAHFxcfDz88PkyZMLFXqqqF+/PkxMTBAVFQUHBwcAQE5ODi5evIj58+cXu98ff/wB4P+TqpOTE5YuXYrk5GSYmpoCAKKioqCtrQ07Ozuhz6FDh5SOExUVBXt7e34/k6iECgoKkJCQgIKCAkgkko/iyubHcp5lQZW5LCgowIMHD6BQKNCoUaMyiqxiUiWnOjk5ISsrC9HR0cL3NKOjo5GdnS28dnJywty5c5GTkyPcsRQVFYU6deqgfv36AIBWrVohKioK48ePF8aPiopS+u6n2OTyolfEJ9WxaBcP51IcnEfxlOYNgKIVmqGhoQgPD0dcXBy0tLTg5uaGJUuWoGvXroW+J2NrawtfX1+MGzeu2ONlZWXhzp07AP7/D4a4uDjUqlUL9erVg6+vL5YtW4YmTZrAwsICS5cuhZ6eHvr16wfgdQK8fPky2rdvjxo1auDq1auYMWMG3NzchE9sO3fuDCsrK4wePRqBgYF4+vQpZs+ejcGDB6NGjRoAgGHDhmHDhg0ICAjAsGHDcOnSJfz0008ICwsTa+qIPjqZmZnIy8tDlSqlelMFfeSqVKmC27dvs9DEh+fUpk2bomvXrpg0aRJWrFgBAJg0aRK6d+8u/LHXr18/LFq0CGPGjMHUqVNx+/ZtrFixAv7+/sIHA6NHj4a7uzuWL18ODw8PHDp0CGfPnsWxY8fUMCtERFSaJHK5XJR71mrVqgUnJyd4e3ujd+/ekEqlb+1/8+ZNHDhwAAEBAUW2nz17Fl988UWh7QMGDMCaNWugUCgQHByMLVu2QC6Xw9HREUuXLkWzZs0AALGxsZg6dSr++usv5Obmol69eujTpw8mTJgAXV1d4Xj379/H1KlTcebMGejo6MDT0xPfffcdtLW1hT7nzp3DjBkzcOvWLchkMkycOBHDhw8vyTS9FT+dEQ/nUjylcetsVlYWUlJSCj1moTLjrbPiUXUuMzIykJ6eDg8PjzKIqnRkZGTgypUrSE1NRceOHd95i2lxPjSnAoBcLoe/vz+OHj0KAHBzc8PixYuV8v2NGzcwdepUxMTEQCqVYtiwYfjmm2+UrkAfOHAAgYGBSExMRMOGDTFr1iz06NGjROf1NswD4uFciodzKQ7Oo3hKcy5FKzTv3LnDT40/EP+nEQ/nUjwsNMVRUQvNvn37om/fvhg4cKC6QxG8T6GZlpaGzz//vAyiEt/333+PZcuW4fnz55BIJNi3bx9cXFyQlpaG5s2bY8GCBaXyoWdlwTwgHs6leDiX4uA8iqc051K0v/JYZBJRZRYYGIh27dph8+bNSttjYmLQrl07yOXy9zrWtGnT3trn0qVLaNeuXaHHPvTv3x/du3dHfn6+sC0nJwcdO3ZEZGSkyjH81+HDh9G1a9cS70/i2rRpEwIDA9GvXz9s3rxZacEsAwMDuLu7Y//+/WqMkIiI6O1E/YJUTk4OIiMjERsbi4yMDBQUFCi1SyQSrF69WswhiYjKjJaWFn766Sf06tULtWrVKtWxbG1tUaVKFVy9ehXdu3cH8Pp5h48fP4a+vj7++usvWFlZAXi9wFpeXp6wsvb7evXqlWhxkzjWrVuHXr16YeXKlUhPTy/UbmtrizVr1qghMiIiItWIVmg+ePAAX3zxBRITE1GzZk1kZGSgVq1akMvlKCgogIGBAfT09MQajoiozDk4OCA1NRVbtmzBpEmTiu139+5dhISEIDY2Ftra2mjZsiV8fHxgamqKjRs3Ct9xe/N4p1WrVgmrfb5RrVo1WFlZISYmRig0Y2JiYGVlBRMTE+Hfb7bXqVMHdevWRUFBAbZu3YqDBw/i6dOnqFevHnx8fNC+fXsAwMOHD9GvXz/MnTsXBw8exPXr1+Hn54fly5crxTR8+HCMGDECAJCbm4vFixfj119/hZ6eHjw9PfHll1+KNa1UhMTERPj6+hbbLpVK8fTp0zKMiIiI6P2IVmjOmTMH6enp+OWXX9CoUSNYWFhg06ZNaNOmDUJCQrB582YcOHBArOGIqBJp1865TMc7f/5CifbT0NDA6NGjMX36dHh6esLMzKxQnydPnsDPzw+ff/45xo4di1evXmH9+vWYPXs2NmzYgAEDBiAxMREZGRmYPXs2AAirXP+Xg4MDfv31V+F1TEwM7O3tIZPJcPr0aaHYi4mJEQrV8PBw/PTTT5g2bRosLS1x/PhxzJgxAxs3bsQnn3wiHGvt2rUYO3Yspk+fDg0NDRQUFGDdunUIDw8HAKXvQO7atQsjRozA5s2bcfHiRaxYsQItWrRA8+bNSzSP9G5SqbTQ8yb/7ebNm8KjuoiIiMoj0b6jeerUKYwYMQKtWrVSWuBDW1sbkydPhrOzM6ZPny7WcEREauHs7AwbGxusX7++yPZ9+/bBwsICY8aMQYMGDWBhYYFZs2bh1q1buHXrFnR1daGtrQ0tLS0YGBjAwMCg2GfyOjg44J9//sGjR48A/H+haWdnh2vXruHVq1d4/vw5bt26Jdw2u3PnTgwYMACurq4wNzfH119/jRYtWmDnzp1Kx+7Xrx86deqEunXrQiaTQU9PDxKJRIjp36tzOzk5oV+/fjAzMxMK7N9//12M6aRiuLq6YuvWrUVetbx+/Tp+/PFHuLu7qyEyIiIi1Yh2RTM7OxsNGjQA8Pp7TMDrZ+W90bZtW+HTeyKiimzMmDEYNWoUbt26VagtPj4esbGxRS6sk5ycrPS4iHexsbGBlpYWrly5Ant7e6SlpcHGxgY6OjrQ1dXFrVu3kJmZifz8fDg4OCA7OxtPnjyBra2t0nFsbW1x8eJFpW2WlpYqx9G4cWOl14aGhrxts5TNmjULUVFRaNu2LVxdXSGRSLBjxw5s3boVhw8fRt26deHv76/uMImIiIolWqFZp04d4VN3PT091KpVC3/88YewrPz9+/eL/dSeiKgiadasGTp27IjQ0FAMHTpUqU2hUMDZ2Rljx45V2p6Tk4M6deq81zja2tqwtrbG1atXAQBWVlbQ0dEBANjb2+Pq1avIzMxEvXr1YGRkhOzs7GKP9e/nGAIQjqOKKlWUU4VEIim02BuJy8TEBKdOncJ3332HgwcPQqFQYPfu3dDX14enpyfmzp2L2rVrqztMIiKiYolWaDo7O+PkyZPCJ6w9evTA6tWrUaVKFRQUFGDt2rXCghZERP9W0u9MqtOoUaPw5Zdf4tKlS0rbP/nkE5w8eRIymUypQPv3sx+rVq2qcqHm4OCAQ4cOQaFQwN7eXthub2+P06dPIzMzU7htVk9PD4aGhoiLi0PLli2FvnFxccIdJ8WpWrWq0iNTSP0MDQ2xcuVKrFy5Ek+ePEFBQQEMDQ0/qufPEhFRxSVathozZgw+//xz5OTkAADmzp2LVq1aYeHChQgODoaDgwOCg4PFGo6ISK3MzMzQo0cPYfGcN/r06YOsrCx8++23uHHjBpKTk3H58mUsW7ZMuOIok8lw584dJCUlQS6Xv/XxIg4ODkhJScGZM2cKFZrXrl3DX3/9pbRi7cCBA7Fz5078+uuvuHfvHjZs2IBr165hwIABbz2fOnXqIDc3F9HR0ZDL5cLvciofDA0NYWxszCKTiIgqDNGuaFpbW8Pa2lp4LZVKsX//fsjlcmhqakJfX1+soYiIyoXhw4cLjyp5w8jICGvXrsXatWsxZcoUvHz5EiYmJnBwcBC+v96jRw9cvXoVI0aMwIsXL4p8vMkb1tbW0NbWRl5eHmxsbITt9evXR/Xq1ZGenq60r6enJ54/f47Q0FCkp6fD3NwcCxYsQJMmTd56LjY2NujVqxfmzp2LZ8+eKT3ehMreokWL3tlHIpHwe5pERFRuSeRyuULdQdBrCQkJ7/xjkFTDuRSP2HP5/PlzZGVlISUl5aO6OvPvW2fpw6g6lxkZGUhLSxPWCqhIatWqVWybRCKBQqGARCJBenp6GUZVsTAPiIdzKR7OpTg4j+Ipzbks8RXN/y6Vr6p33b5FRET0sStqVd+CggLcu3cPYWFhuHDhAiIiItQQGRERkWpKXGiOGTOm0LY3qxoqFIoitwMsNImIiEpCQ0MDDRo0QGBgIL7++mv4+/sjLCxM3WEREREVqcSF5rVr15ReP3v2DL6+vqhVqxZGjhwJCwsLAMDt27exYcMGPHv2DGvWrPmwaImIiAjOzs6YM2eOusMgIiIqVokLTXNzc6XXY8aMgbGxMfbs2aN0BdPa2ho9evRAnz59EBoaitDQ0JJHS0RERLh69epH9R1nIiKqeERbdfbw4cP49ttvCz0UHHh966yHhwcCAwPFGo6IiKjSKm4dhGfPnuHChQuIjIzE4MGDyzgqIiIi1YlWaCoUCsTHxxfbfuvWrULf3SQiIqLCiloH4Q0DAwNMmjSJjzYhIqJyTbRC08PDA5s3b4a5uTmGDx8OPT09AEB2djY2bdqELVu2wNPTU6zhiIiIKq3/roMAvL47SCqV8rnURERUIYhWaAYHByMpKQmzZ8/GvHnzYGJiAgBISUlBfn4+2rRpg6CgILGGIyIiqrT+uw4CERFRRSNaoVmzZk0cOXIEhw8fxokTJ3D//n0AgKurK7p16wY3N7civ79JRERERERElYtoheYbHh4e8PDwEPuwRET0kTp8+DCWL1+OEydOqDuUUmNra/veH8ZKJBLExsaWUkREREQfRvRCk4ioMgoMDMTRo0fx+eefY/r06UptoaGh2LFjB5ydnbFkyZJSi+Hhw4fo168fatSogd27d6N69epC29ixY9GwYUNMmTLlvY4VFhYGKyurYvv5+PigYcOGSud8/PhxzJ8/H35+fhg4cKCwff369Th27Bj27t1bgrN7rW/fvujbt6/ScT8G7dq1410/RERUqbDQJCJSkYmJCX777TdMnDgR1apVAwC8evUKx44dE76XXhZycnKwbds2+Pr6lvpYDg4OOHnypNK2mJgYmJiY4OrVq0oFYUxMDOzt7Us0Tl5eHqpWrfpBsVZka9asUXcIREREouLTnomIVNS4cWPUq1dPqfC6ePEitLS0ChVYN2/exMSJE+Hu7o5u3bphwoQJuH79utB+9epVdOjQATExMcK2/fv3o1u3bkhOTn5rHJ6enti9ezdSU1OL7aNQKLBjxw54enqiU6dO+Oqrr3D8+HGhvV+/fgCAkSNHol27dhg7dmyRx3FwcEBycjJSUlKEbTExMfjqq69w7do15OfnAwBevHiBmzdvwtHREQDw999/Y8KECejUqRM+++wzBAYGIisrSzhGYGAgpk2bhu3bt6NXr17o1asXxo4di0ePHiEkJATt2rVDu3btlGL5/fffMWjQIHz++ecYO3Ys/vnnn7fOExEREakPr2gSkdq1+7XduzuJ6Hy38yXe9/PPP8ehQ4eE76IfOnQI7u7uhYqe58+f47PPPsPEiRMhkUiwa9cuTJ06Fbt27ULNmjVhb2+PgQMH4rvvvsPWrVvx9OlTrFq1ClOmTIGpqelbY+jUqROuXr2KsLCwQrfxvrF+/XpERUVhypQpMDc3x/Xr17Fo0SLo6+vD2dkZYWFhGDlyJJYtWwYLC4tiryba2tqiatWqiImJgZubGx49eoTU1FS4ublhy5YtiI+PR7NmzRAXF4dXr17BwcEBL168wKRJk9CsWTOEhYUhIyMDixYtwsKFC7Fw4ULh2FevXoWenh6WLVsGhUIBIyMjDBkyBB4eHujdu7dSHLm5udi2bRtmzJgBhUKBJUuWYMmSJVi+fPk7/5tVZHl5efjrry5akYoAACAASURBVL+QkZGBgoKCQu3/LcaJiIjKCxaaRETvoVu3bli9ejXu378PXV1dXLp0CZMmTUJYWJhSvzdX9t4YO3Yszp07h//973/o3r07gNdXEy9fvoygoCA8evQIzs7OcHd3VymOMWPGYMKECfDy8kKjRo2U2l68eIGff/4Zy5cvh52dHQCgbt26+PPPP7Fnzx44OztDKpUCAGrUqAEDA4Nix9HR0YGVlZVQaF65cgVWVlbQ0dGBvb09YmJi0KxZM8TExMDU1BQymQwHDx5ETk4Ovv32W+GZyv7+/hg3bhwePHgAMzMzAIC2tjZmzJgBLS0tYTwNDQ3o6uoWiik/Px+TJ09G/fr18eLFCwwYMABBQUFQKBSV8ruNCoUC3333HTZs2IDs7Oxi+6Wnp5dhVERERKoT7dZZPz8//P7778W2X7lyBX5+fiof7/z58/D29oaVlRWkUil27Nih1K5QKBAUFARLS0vIZDJ4eHjg5s2bQntSUhLGjh2LFi1aQCaToUWLFpg3bx5evHihdBypVFroZ9OmTUp9bty4AXd3d8hkMlhZWWHRokVQKBQqnwsRVR41atSAi4sLDh06hKNHj8Le3h4ymaxQv6dPn2Lx4sXw9vaGq6srvvjiCzx9+hSPHj0S+lSpUgVz587FhQsX8PTpU/j7+6sch729PZycnLB27dpCbYmJicjNzcWUKVPQtWtX4Wf//v3vvC23KI6OjsItvv/+HuabQvPNdgcHB2H8xo0bC0UmANjY2EBDQwN3794VtjVs2FCpyHwbLS0t1K9fX3htaGiIvLw8ZGZmvvf5VAQrVqzA8uXL0bdvX6xduxYKhQJz587F8uXLYWVlBRsbG+zbt0+08WxsbIrMh/379wcABAUFFWr75JNPlI7xrrwMAHK5HD4+PjA3N4e5uTl8fHwgl8tFOw8iIio/RLui+dNPP6Fjx45o2bJlke1JSUnYuXMnQkJCVDpednY2mjVrhgEDBmD06NGF2leuXImQkBCEhISgSZMmWLx4MXr37o3Lly9DX18fCQkJyM/Px7Jly9C4cWPEx8dj4sSJSE9Px8qVK5WO9cMPPwhXGIDXf0i+kZGRgd69e8PZ2RknT55EQkIC/Pz8oKuri3Hjxql0LkRUuXh4eCAwMBDVqlXDyJEji+wTGBiI9PR0jB8/HjKZDAUFBfD398erV6+U+t24cQMKhQJZWVmQy+XQ19dXOQ5fX18MHTq00CMu3txiuXjx4kKLFFWp8v6/9h0cHLB582Y8fPgQV69eFW7XtbOzw6pVq5CRkYH4+HihKHmbf199fLOgkio0NTWLPE5Rt5NWBtu3b0ePHj2wYsUK4aplixYt4OLiAm9vb3Tp0gXnzp2Di4uLKONFRUUJ37cFgEePHqFjx47o1auXsK1JkyY4dOiQ8Pq//03elZeB11fxHzx4gIiICADA+PHjMWrUKOzatUuU8yAiovKjzG6dTU9Ph7a2tsr9XV1d4erqCuD1LWL/plAosGbNGkycOBE9e/YE8HrFvib/x96dh1VV7Y8ffx8RENFEZXAEEpBJlBQBLTUVB5xyQhxCr0bmUN40cCiHHJIccsgpb2KpWdcwTEyuWoppyqCVQ+olS1GcAFEQCBzg/P7wx/7eE4MMG47C5/U853k8e6+912ev53gWn7P3WsvBgZ07dzJ27FjlF/x8tra2vPPOO3zwwQcFEs169eoVOWNkWFgY2dnZbNiwARMTE1xcXPj9999Zv349b775ZpV8ZEuIylaeMZP64OHhgaGhIenp6XTu3LnQMqdPn2bq1Kl07NgRgBs3bpCamqpT5saNG6xYsYJp06YRGxvLggUL2LBhQ4mTQTs7O3r37s369et17gza2tpiZGTErVu3CjzCmy+/jpIkaq1atcLIyIg9e/Zw584d3NzcALCxsaF27dr8+9//Jjc3V7mjaWtry969e8nKylLuap49e5a8vDxsbW2LrcvQ0LDKJo+lce3aNeUpoBo1Hj98dP/+feDxI8f+/v5s3LiR9957T5X6zM3Ndd5v27aNunXr6oyVrVmzZpF9ZUn65fj4eH744Qf27duHp6cnACtXrsTX15eLFy/i4OCgyrUIIYR4OpQr0Tx27Bg//fST8n7Pnj1cunSpQLm0tDTCw8Np1apVeapTXLlyhaSkJLp166ZsMzExoWPHjsTGxjJ27NhCj8vIyFDGJf2vmTNnMnXqVGxsbAgICOAf//iH0rHHxcXRoUMHnV/eu3fvzgcffMCVK1ee+EeTEKLq0Wg0bNmyBaDIRz+tra3Zv38/Li4u5OTksGbNGp0Jd3Jzc1m4cCHu7u4MHDhQmRl28+bNjB8/vsSxBAYGMnz4cODxo6gApqamjBgxgrVr16LVanF3d+evv/7i3Llz1KhRg1deeYX69etjbGxMbGwsjRs3xsjISGddzv9lZGREq1atCAsLU8Zn5nN3dycsLAxbW1tlXGXPnj3ZtGkTixYtIjAwkIyMDJYuXUqXLl2U8ZlFadSoEadPn6ZXr14YGhoW+p1dHZiZmZGTkwM8fsrGyMhI57FnY2PjChufqdVq2bZtG/7+/jp9X0JCAk5OThgZGeHh4cHcuXOVPrAk/XJcXBx16tTBy8tLKePt7Y2pqSmxsbGSaAohRBVTrkTz6NGjLFmyBHj8h9eePXvYs2dPoWXzxzaqIX+afQsLC53tFhYW3Lx5s9Bjrl69ypo1a5g2bZrO9nfffZdOnTphamrKjz/+yOzZs0lNTSU4OBiA5ORkmjRpUqCe/H1FJZoXL14s9XWV5zhRkLSletRsSyMjIx4+fEh2drbyg86zIDc3l9zcXGWcd37s+e//vn/atGmsXLmScePG0bBhQ0aPHk16erpy7V988QWJiYl8+umnZGdnY2RkxPTp03n33Xdxd3dX7hr+r/zE4/79+0o9zz33HIMGDWLHjh069b/66qvUqVOH7du3s3z5cmrXro2dnR3+/v5KmcmTJ7Nt2zY+++wzWrVqxYoVK4q8/tatW/PLL7/QqlUrnbHurVq14uDBg7Rp00Zne0hICBs2bCAwMBAjIyM6duzIpEmTimyvfAEBAaxatQo/Pz8ePnzIDz/8wMOHD9FqtTpl8+/u5eTkFDhHvqysLG7fvl3g8/ssJDTOzs6cPXsWePxZa9u2LaGhofTs2ZO8vDw+//zzCruOqKgorly5wujRo5VtHh4erF+/HgcHB27fvs2yZcvo2bMnMTExNGjQoET9cnJyMg0bNtR5Ekij0WBubk5ycnKxMUmfqn/SluqRtlSHtKN6ytOWxfVFmrS0tDLPapOdnU12djZarRZ7e3tWrlzJgAEDdCvQaDAxMdH5Bby0mjZtytKlSxk1ahQAsbGx9OrVi7Nnz9K8eXOl3OTJk7l58ybh4eE6xycnJ9OvXz/c3NzYtGlTsY+7rl69mo8++oirV68CMGjQIJo0aaIztjQxMRE3NzcOHDigPP6jBnl0SD3SlupRuy3/+usvMjMzSUpKeqYSzfLKzs4u1ZhEUbSStuW9e/dITU2lX79+lRCVurZv305oaCiRkZHUqlWL6OhoBg0axIMHD4DHjxh/+eWXdO/eXfW6x4wZQ2Jios56sX+XmZmJu7s7b7/9Nm+++WaJ+uWPPvqIrVu3cvr0aZ1ztWnThjFjxhT4Ibi8pB9Qj7SleqQt1SHtqJ6KbMty3dE0MTFROvvTp09jbm5O7dq1VQmsOPljRFJSUnQ6tJSUFCwtLXXKJiUlMWDAAJydndm4ceMTx1S2a9eOe/fukZycjKWlJZaWlgUWRc9///e6hBBCCDWMGjVK+XEVoEOHDsTExPCf//wHAwMDunfvjp2dner1pqSkEBkZyfLly4stV6dOHZycnJThMiXply0tLUlNTdVZkkar1XL79m3pT4UQogpS7XbCzz//XGyS+ejRIxYtWqRKXTY2NlhZWREVFaVsy8nJITo6Wmfsx61bt+jXrx8tW7YkNDS0RBNsnD17llq1alGvXj0APD09iY6OVh5Zg8ePFTVu3Fhnqn0hhBCiItna2jJx4kTGjx9fIUkmPJ5B3tjYmCFDhhRbLicnh4sXLyoJZkn6ZU9PTzIzM4mLi1PKxMXFkZWVpdN3CyGEqBpUSzTHjRtHYGBgoethnTt3jq5du7Jy5coSny8zM5MzZ85w5swZ8vLyuHbtGmfOnCExMRGNRsPEiRNZvXo1ERERnD9/nkmTJmFqasrQoUMBuHnzJn379sXS0pKQkBBSU1NJSkoiKSlJmcL9P//5D1u2bOH8+fNcvnyZrVu3EhISwpgxY5QZcocOHYqJiQmTJk3i/PnzREREsGrVKiZNmiQzzgohhKgQ7u7uLFy4kN9++63S6tRqtWzdupXBgwcXmBhq9uzZ/PTTTyQkJHDy5EnGjBnDX3/9xYgRIwBK1C87Ojri4+PD1KlTiYuLIy4ujqlTp9KrVy95BE4IIaog1ZY3WbJkCfPnz+fYsWN8/PHH9OjRA61Wy4oVK5S13L799tsSn+/XX3+lf//+yvuQkBBCQkIYMWIEGzZs4J///CfZ2dkEBweTlpZGu3btCA8PV9bqOnToEH/++Sd//vlngdluT58+jY2NDYaGhmzatIn33ntPmXZ/1qxZvP7660rZevXqsWvXLoKCgujatStmZmZMnjyZN998s5wtJoQQQhTOxsaG1atXs3LlSlq2bMngwYMZPHgw9vb2FVbn0aNH+fPPP/nXv/5VYN+NGzcIDAwkNTUVc3NzPDw8+P7777G2tlbKPKlfBti0aRPTp09X7pj6+vqydOnSCrsmIYQQ+lOuyYD+7s8//2TixImcPHmS4cOH8/vvv/Pzzz8TEBDA4sWLi5w6XzwmA5vVI22pHpkMSB0yGZB6qsNkQAC3b9/m22+/ZdeuXcTExKDVamnVqhVDhw5l0KBBOmMhRUHSD6hH2lI90pbqkHZUT0W2pap/5dnZ2REZGUm7du346quv+OWXX1iwYAEff/yxJJlCCCEqlVar2u+oemFubk5gYCB79+7l3LlzLFq0CGNjY+bNm0ebNm3o1auXvkMUQgghiqRqonn16lUGDRrEyZMnGTBgAE2aNGHx4sWsXbtWzWqEEFXAs54EiKffo0ePqsxY+kaNGjFp0iT279/P6tWrqVOnDidOnNB3WEIIIUSRVEs0t2zZwksvvcSFCxfYunUrW7Zs4dixY/Tv3585c+bg6+tLQkKCWtUJIZ5RxsbG5OXlkZeXp+9QRBX2119/kZycjJGRkb5DUcWxY8cICgrCycmJt99+GwMDA1599VV9hyWEEEIUSbXJgN5++218fX1ZvXo1FhYWwOOJdP71r3/Rv39/pk2bRqdOnUhMTFSrSiHEM8jAwICGDRty9epVMjMzMTAw0HdIlSIrK4uHDx/qO4wqobi21Gq1PHr0iOTkZB48eEDbtm0rOTr1nDhxgvDwcHbv3s2tW7eoU6cOvXv3ZsiQIXTv3r1ES3YJIYQQ+qJaL7Vu3TpGjhxZ6L7+/fvToUMH3nnnHbWqE0I8wwwNDWnXrh2xsbGkpaVVi7ubt2/fxtzcXN9hVAklaUszMzNeeOEFGjZsWElRqatVq1bcuHGDWrVq0aNHDwYPHkyvXr2oVauWvkMTQgghSkS1RLOoJDOfubk5W7ZsUas6IcQzzsDAgI4dO+o7jEojM+Sppzq0paurK3PnzqVPnz4ymZ4QQohnkqrP3dy5c4f169dz9OhRUlJS+OSTT/D09OTOnTt8+umnDBw4EEdHRzWrFEIIIaqcHTt26DsEIUQle/ToEVlZWfoO45lQq1Yt0tPT9R1GlVCStjQ1NS3TcA3VEs0rV67g6+vLnTt3cHFxISEhgezsbAAaNGhAeHg4KSkpLF++XK0qhRBCCCGEeOY9evSIjIwMzMzMqsxs2RXJ2NhYhhKo5EltqdVqSUtLo27duqVONlVLNOfNm4dWqyUmJoa6detib2+vs79Pnz7s3btXreqEEEIIIYSoErKysiTJFE8ljUaDmZkZ9+7do169eqU6VrXlTQ4fPszrr7+Ora1tof9JbGxsuHHjhlrVCSGEEEIIUWVIkimeVmX9bKqWaN6/fx8zM7Mi96enp1OjhmrVCSGEEEIIIYR4SqmW+Tk7O3Ps2LEi9+/du5fWrVurVZ0QQgghhBBCiKeUaonmxIkT2bVrF8uXL+fu3bsA5OXl8fvvvxMYGMjJkyeZPHmyWtUJIYQQ1cr9+/fZuXMnoaGhXLt2Td/hCCGEXvTt25dZs2ZVSj3BwcEVXk95XLlyBTMzM3799Vd9h1Io1RJNPz8/5syZw5IlS2jfvj0AQ4YMwdvbm2+//Zb58+fj6+urVnVCCCFElRUcHEyXLl2U97m5ufj6+jJ+/HiCgoLo0KED586d02OEQojqzszMrNjXxIkTn3j87t27KyS2rVu30qlTJ5o2bYq1tTUdO3Zk0aJFFVJXWURERNCgQQMSExML3d+9e3cCAwMrOSr1qbqO5tSpU/Hz8yMiIoJLly6Rl5fH888/T//+/bG1tVWzKiGEEKLK+uGHHxg0aJDyfteuXfz666989NFHtG7dmsDAQJYtW8bnn3+uvyCFENVafHy88u/9+/czZcoUnW36Wn5k27ZtzJgxg8WLF9OlSxcePnzIhQsXiIuL00s8hfH19aVhw4Zs376dmTNn6uw7f/48P//8M3PnztVTdOpRfXaeZs2aMWnSJJYvX86KFSt46623JMkUQgghSiEpKUmn79y7dy+tWrVi3LhxeHh4MG7cuKfqjyYhRPVjZWWlvPKXvfjfbeHh4bzwwgtYWFjwwgsvsGXLFuVYNzc3AMaMGYOZmZny/vLly4wYMYKWLVvSpEkTOnfuzL59+0oV13/+8x/69+/P2LFjadGiBY6OjgwcOJDFixcrZcpSz4MHD5g3bx4uLi40btyYrl27cvDgQWX/w4cPmT59Ok5OTlhaWuLq6sr7779f6LkMDQ0ZPnw4X375JVqtVmfftm3bsLW1pXPnzuzYsYOuXbvSrFkz7O3tGTNmTLGreBw9ehQzMzNSU1OVbYU9Xvvf//6XYcOG0axZM1xdXXnttddISkoq9vrLQtU7mvkyMzNJS0sr0HAAzZs3r4gqhRBCiCrDyMiI7Oxs4PFi2UeOHCEgIEDZb2Zmxp07d/QVnhCikpitKnpFh4qQ9naaKufZs2cPwcHBLF68mG7dunHw4EHeeecdLC0t8fX1JSoqCnt7ez7++GN69eqFgYEB8DiH6NGjB7Nnz8bExITw8HACAgI4duwYLVu2LFHdVlZWHDlyhISEhCJvdpWlnsmTJ3P58mU+/fRTmjZtyoEDBxg+fDiHDh3Czc2NTz75hL179xIaGoq1tTU3btzg4sWLRcYZEBDAmjVrOHLkiDJU4sGDB3z99ddMnDgRjUbDgwcPmDVrFi1btiQ1NZV58+bx2muv8Z///KdEbVGYW7du0adPHwICAli4cCFZWVksWbKEkSNH8v3336u6SohqiWZOTg5Llixh27ZtxXZ+0jEKIYQQxXNxceHrr7/G39+fiIgI7t69S48ePZT9V69exdzcXI8RCiFE0dauXYu/vz/jx48HwN7enlOnTrF69Wp8fX2V76969ephZWWlHOfm5qbc3QQICgpi37597N69u8QT88yYMYPffvsNd3d3WrRogYeHB127dmXo0KEYGhqWqZ7Lly+zc+dOzpw5o9w0Gz9+PIcPH+bzzz/no48+IjExETs7Ozp27IhGo6F58+Z4eXkVGWfLli3x9vZm27ZtSqIZGRlJWloao0aNAtD5gdHW1pYVK1bg6enJ9evXadq0aYna4+9CQ0Np1aoV8+fPBx7ncBs3bsTW1pZff/2Vdu3alem8hVEt0XznnXf46quv6Nu3Lx06dCh2TU0hhBBCFG3GjBn4+/vTokULALy9vXnxxReV/fv376dt27b6Ck8IIYoVHx+vJEv5OnTo8MQ7cfl31/bv38+tW7d49OgROTk5uLq6lrjuRo0a8f3333P+/HmOHTtGXFwcU6dOZf369ezfv5/atWuXup7Tp0+j1Wrx9vbW2X7//n06d+4MwMiRIxk0aBDt2rWjW7du9OjRgx49ehR7hzAgIIB33nmHtLQ0zMzM+OKLL/Dx8aFx48YAnDp1iiVLlnD27Fmdp0WvXbtW5kTz9OnTHD9+XDleq9Wi0WiAxwn1U5lo7tmzh9GjR7Nq1Sq1TimEEEJUS126dOHHH38kKiqK5557jsGDByv77t69y0svvUTfvn31GKEQQpRefkJTlDlz5vDDDz+wcOFC7OzsqF27NhMmTODBgwelrsvFxQUXFxdef/11oqOj8fX1ZdeuXYwaNarU9eTl5aHRaDh06JByVzRf/qRH7u7unDlzhkOHDvHjjz8yceJEWrVqxbfffltksjlw4EBmzpzJzp076d27N4cOHWLr1q3A46R7yJAhvPzyy2zcuBELCwtSU1Px9fUtMs78ev53+OKjR48KXEvPnj2VWXjv37+PsbExABYWFsW2aWmplmhqNBratGmj1umEEEKIas3R0RFHR8cC2+vXr09ISIgeIhJCVDa1xkxWNkdHR2JjYxk9erSyLTo6GicnJ+W9oaEhubm5OsfFxMQwfPhwXnnlFeDxY52XL1/Gzs6uXPHk15uVlVWmelq3bo1WqyUpKUm5g1mYunXr8sorr/DKK68wcuRIfHx8uHTpEvb29oWWNzU1ZciQIcrQQ3Nzc3r37g3AxYsXSU1NZc6cOcpY04iIiGKvM/+R5Fu3bin/Pnv2rE6ZNm3asGvXLpo3b46hoSE5OTkVNkOwaqM9+/Tpw+HDh9U6nRBCCFHtHT58mIULFzJlyhR+//134PEkFseOHSMt7dn8A1QIUfW99dZb7Nixg08//ZQ///yTjRs3EhYWxpQpU5Qy1tbW/PjjjyQlJSnfZ3Z2dnz33XecOnWKc+fOMX78eO7fv1+quqdNm8bSpUuJiYnh6tWrnDhxggkTJlC7dm26detWpnrs7e0ZNmwYkyZNYvfu3SQkJPDrr7+yZs0aJflbu3YtO3fuJD4+nkuXLhEWFsZzzz1HkyZNio03ICCA06dPs379ekaMGEHNmo/vAzZr1gxjY2M+/fRTEhIS2L9/v87MuYVp0aIFzZo148MPP+SPP/7g0KFDLFu2TKdMYGAg9+7dY+zYsZw8eZIrV65w+PBh/vnPf5KRkfHE9i0N1RLNd955h8uXLzNlyhROnjzJrVu3SElJKfASQgghRPGys7MZMmQIgwcPZuXKlXzxxRfcvHkTeDwj7ZgxY9i4caOeoxRCiML169ePpUuXsn79ery8vPjkk0/46KOP8PX1VcosWrSIo0eP4urqSqdOnQD44IMPsLCwoE+fPvj5+dG+fXs6dOhQqrpffvllfv75Z8aOHYuHhwevvvoq8Hg94vw7i2WpZ926dYwaNYq5c+fSvn17/P39OXbsGNbW1sDju5kff/wx3bt3p0uXLpw9e5awsDBq165d7HnbtWuHi4sLaWlpOpP/mJubs2HDBvbu3YuXlxdLlizhgw8+KPZchoaGhIaGkpCQwEsvvURISEiB9TgbN27M/v37qVGjBkOGDKFLly4EBQVhZGSkPEKrFk1aWlrBNUjKoH79+v930mKev5ZZZ4t28eJFHBwc9B1GlSBtqR5pS3VIO6qnOrTlu+++S2hoKOvWraNDhw7KOJ/8mQmDgoL4+eefiYqK0nOkT6/q8DmpLNKW6imqLdPT05W1KMWTVeTjntVNSduyLJ9R1cZoTp8+/YkDfIUQQgjxZN9++y2BgYEMHTq00B9oHRwc+Oabb/QQmRBCCFEyqj06O2vWLGbOnPnEV0kdO3aM4cOH4+zsjJmZGdu3b9fZr9VqCQkJwcnJiUaNGtG3b18uXLigUyYtLY3x48djbW2NtbU148ePLzCm5dy5c/Tp04dGjRrh7OzMkiVLdGZqAti9ezdeXl5YWlri5eXFnj17Stk6QgghRMmlpqYWOhFQPo1GQ05Ojqp1hoSEYGZmpvP634XLK7PfFUII8exTLdFUW1ZWFi4uLnz44YeYmJgU2L969WrWrVvHkiVLOHToEBYWFgwaNEhnEGtgYCBnzpxh586dyiKrb7zxhrL/3r17DBo0CEtLSw4dOsSHH37ImjVrWLt2rVImLi6OcePG4efnx9GjR/Hz8+Mf//gHJ0+erNgGEEIIUW01a9aM+Pj4IvfHxMQoa2yqycHBgfj4eOV1/PhxZV9l9btCCCGqBtUenVVbz5496dmzJwCTJk3S2afVatmwYQNvv/22Mi3xhg0bcHBwYOfOnYwdO5b4+Hh++OEH9u3bh6enJwArV67E19dXeT4+LCyM7OxsNmzYgImJCS4uLvz++++sX7+eN998E41Gw4YNG+jUqRNBQUHA4+majx49yoYNGwgNDa2QazdbZVYh5xVCiKrgRN8T+g6hwvn5+bF27Vr69eun3NnMH54SGhrKt99+y4IFC1Svt2bNmlhZWRXYXpn9rhBCiKrhqb2jWZwrV66QlJSkTFEMYGJiQseOHYmNjQUe34msU6cOXl5eShlvb29MTU11ynTo0EHnjmn37t25efMmV65cAeDEiRM69eSXyT+HEEIIobZp06bRoUMH+vXrh6+vLxqNhpkzZ+Lk5ERQUBC9evUq8COsGhISEnBycqJ169aMGzeOhIQEoHL7XSGEEFXDU3tHszhJSUkAWFhY6Gy3sLBQpn9PTk6mYcOGOr+OajQazM3NSU5OVsr8fW2b/HMmJydja2tLUlJSofXkn6MoFy9eLMOVlf04IYSoTsrzXfkszJ5pZGREWFgYYWFhfPvtt2g0Gh49ekSbNm0YNGgQ/v7+qt/98/DwYP369Tg4OHD79m2WLVtGz549iYmJqdR+tzDSp+qftKV6CmvLWrVqqb60RFWn9jj16qwkbXnv3r1C85/i+tRnMtF8FpTlDxmZPlwIIUqmDQCq8AAAIABJREFUunxX+vn54efnVyl19ejRQ+e9h4cH7u7ufPnll7Rv375SYiiK9Kn6JW2pnuKWN5HlOkpOljdRT0nb8rnnnqN58+alOvczmWjmjx9JSUnRueCUlBQsLS0BsLS0JDU1Fa1Wq/y6qtVquX37tk6ZlJQUnXPnv88vY2VlVWiZ/P0VIe3ttCcXEsWSTlE90pbqkHZUj9xZqRx16tTBycmJS5cu0a9fP6By+l0hhBBVg+qJZkZGBomJiaSlpRU6XfmLL75Y7jpsbGywsrIiKiqKtm3bAo+z8ejoaGVyBE9PTzIzM4mLi1PGi8TFxZGVlaW89/T05P3339fJ5KOiomjcuDE2NjYAtG/fnqioKKZMmaLUHxUVpTMGRQghhCiPyZMnl/oYjUZTobO15uTkcPHiRTp16lSp/a4QQoiqQbVE886dOwQHBxMREUFubm6B/fm/cBa28HRhMjMzuXTpEgB5eXlcu3aNM2fOUL9+fZo3b87EiRNZsWIFDg4O2Nvbs3z5ckxNTRk6dCjweHZYHx8fpk6dyqpVqwCYOnUqvXr1Uu4qDB06lCVLljBp0iSCgoL4448/WLVqFdOnT1d+jZ0wYQJ9+vRh5cqV9O3bl++++46jR4+yb9++creZEEIIAXDkyJFSj7lUe4zm7Nmz6d27N82aNVPGaP7111+MGDECjUZTaf2uEKJ62b59O9OnT+f69ev6DkWoTLVEc8qUKezbt4833niDDh06YGZWviU6fv31V/r376+8DwkJISQkhBEjRrBhwwb++c9/kp2dTXBwMGlpabRr147w8HDq1q2rHLNp0yamT5/OkCFDAPD19WXp0qXK/nr16rFr1y6CgoLo2rUrZmZmTJ48mTfffFMp4+XlxebNm1m0aBGLFy/m+eefZ/PmzXh4eJTr+oQQQoh8Z8+e1XcI3Lhxg8DAQFJTUzE3N8fDw4Pvv/8ea2trgErrd4UQz46JEyfy1VdfKe8bNGhA+/btWbhwIS1bttRjZOJpoElLSyv4fGsZNG3alNdee61C1vWqLmQMl3qkLdUjbakOaUf1SFuKkpDPiXqkLdVT3GRA9erV00NE5TNx4kRu3rzJxo0bAbh58yZz587l1q1bxMXFlegcZbmjKZMBqaekbVmWz6hq62iamJgov3oKIYQQQgghqj5jY2OsrKywsrLC3d2dSZMm8fvvv5OdnQ3A+++/j4eHB40aNcLNzY25c+cWu5zG5cuXGTFiBC1btqRJkyZ07ty5wJA1Dw8Pli1bxttvv03z5s1xcXHh448/1imTnp7OtGnTcHR0xMrKCk9PT8LDw5X9sbGx9OnTh8aNG+Ps7My0adO4d++eii0jVHt0dtiwYXz33XcEBgaqdUohhBCi2jp48CBr167l1KlT3Lt3r9AJ9ko674EQ4tlkZla5dznT0tLLdXxGRgbh4eG4uLhgYmICQO3atVm7di2NGzcmPj6eadOmYWRkxOzZsws9R2ZmJj169GD27NmYmJgQHh5OQEAAx44d03kcd/369cyaNYspU6bw/fffM2PGDLy9vfH09ESr1TJs2DDS0tJYt24d9vb2XLx4UUlwz507x+DBg5k5cyZr1qzh7t27zJo1izfffJOtW7eWqw3E/ylzovnzzz/rvO/Xrx8//fQTgwcP5tVXX6VZs2YYGBgUOK5du3ZlrVIIIYSoFvbu3UtAQABOTk4MGTKE0NBQ/Pz80Gq17N27FwcHB3x9ffUdphBC8MMPP9C0aVMAsrKyaNasGV9//bWyf/r06cq/bWxsmDZtGmvWrCky0XRzc8PNzU15HxQUxL59+9i9ezfBwcHK9m7dujF+/HgA3njjDTZu3MiPP/6Ip6cnhw8fJi4ujpiYGBwdHQGwtbVVjv34448ZNGgQb731lrLto48+onPnzqSkpGBhYVGOFhH5ypxo+vj4FJghLv/X1sOHDxcoX9pZZ4UQQojqasWKFbi7u3PgwAHS09MJDQ1l1KhRdOnShYSEBHx8fLCzs9N3mEIIQceOHVm9ejUAaWlpbNq0icGDB/PDDz/QrFkzdu/ezYYNG7h06RJZWVnk5uYWukJFvqysLJYsWcL+/fu5desWjx49IicnB1dXV51yf3/fqFEjZV3eM2fO0KhRIyXJ/LvTp09z6dIldu3apWzLz2MuX74siaZKypxorlu3Ts04hBBCCPH/nT9/njlz5lCzZk3l6aD8P8xsbW0ZN24cK1euxM/PT59hCiEEtWvXpkWLFsr7NWvWYG1tzeeff06vXr0YN24cM2bMYPHixdSrV4/IyEjmzJlT5PnmzJnDDz/8wMKFC7Gzs6N27dpMmDCBBw8e6JQzNDTUea/RaAodYlCYvLw8Ro8ezaRJkwrsa9y4cYnOIZ6szInmyJEj1YxDCCGEEP+fsbGxMgugqakpGo1G+aUeHs/0fvnyZX2FJ4SoJOUdM6kPGo2GGjVqkJ2dTUxMDI0bN9Z5fDYxMbHY42NiYhg+fDivvPIK8HhW1MuXL5fqKY7WrVtz69Yt4uPjC72r2aZNGy5cuKCTIAv1qTbrbP/+/fnxxx+L3H/kyBGddTGFEEIIUbgWLVrwxx9/AI9/tXd0dCQiIkLZHxkZSaNGjfQVnhBCKO7fv09SUhJJSUnEx8czffp0MjMz6d27N/b29ty8eZOvv/6ahIQEQkND+eabb4o9n52dHd999x2nTp3i3LlzjB8/nvv375cqpi5duuDh4cHo0aM5ePAgCQkJREVF8d133wGP1wX+5ZdfmDp1qvIY7b59+3j77bfL3A6iINUSzZ9++onk5OQi99++fZtjx46pVZ0QQghRZfn4+BAeHs7Dhw+Bx2vVRUZG0rZtW9q2bcuBAwcYN26cnqMUQojHc7M4Ojri6OiIj48Pv/zyC59//jmdOnXC19eXKVOmMGvWLF588UWioqJ49913iz3fBx98gIWFBX369MHPz4/27dvToUOHUsVUo0YNwsLC8PLyYvz48Xh5eTFz5kzlO7VVq1ZERkZy9epV+vXrx0svvcSCBQtkbKbKNGlpaSV7mPkJ6tevz7/+9a8ix4usWbOGpUuXPvF2eXUmCyKrR9pSPdKW6pB2VE91aMuHDx+SkZFB/fr1lYn3vv76a3bv3o2BgQG+vr6MGDFCz1E+3arD56SySFuqp6i2TE9Pp169yl3K5FmWk5OjDC8Q5VPStizLZ7Rc62ju3buXyMhI5f3nn39e6IyzaWlp/Pjjj7K0iRBCCFEChoaGNGjQQGfbsGHDGDZsmJ4iEkIIIUqnXIlmfHw8u3fvBh4P/P355585ffq0ThmNRkPt2rV58cUXCQkJKU91QgghRLX08OFDTp48ya1bt3BwcKBVq1b6DkkIIYQoVrkSzWnTpjFt2jTg8aOza9askanWhRBCiDI4ePAg4eHhzJ8/H3Nzc2X7H3/8wYgRI/jzzz+Vba+88gqbNm1Slj4RQgghnjblSjTz5eTksG7dOpkiWAghhCij7du3c/HiRZ0kE+CNN97gjz/+wN/fn3bt2vH999+ze/duPD09mThxop6iFUIIIYqnyqyztWrVYurUqZw9e1aN0wkhhBDVzq+//krXrl11tp07d45ffvmFIUOG8Mknn/D666/z9ddf4+XlRVhYmJ4iFUIIIZ5MteVN7OzsSEpKUut0QgghRLWSnJxc4MmggwcPotFoGDlypM72vn37KutsCiGEEE8j1RLN4OBgPv30U86dO6fWKYUQQohqo1atWuTk5Ohsi4mJQaPR4OHhobO9fv36PHjwoDLDE0JUoJo1a5KVlYVWq8qqg0KoRqvVkpWVRc2apR9xqcoYTYCffvoJc3NzOnfujKenJ88//zwmJiY6ZTQaDcuXL1erSiGEEKLKsLe35/Dhw0yYMAGAv/76i2PHjuHq6spzzz2nU/bWrVuysLgQVYipqSn379/n3r17+g7lmXDv3r0C34uibErSlrVq1cLY2LjU51Yt0dy8ebPy75iYGGJiYgqUkURTCCGEKFxgYCDjx4/nzTffxNvbm4iICDIyMnj11VcLlP3xxx9xdnbWQ5RCiIpibGxcpj/mq6Pk5GSaN2+u7zCqhIpsS9USzbt376p1KiGEEKLa8fPz48SJE4SGhrJ9+3YARo4cSWBgoE65Cxcu8NNPP7FkyRJ9hCmEEEKUiGqJphBCCCHKZ+nSpQQHB3PlyhWaN2+OlZVVgTINGzbk0KFD2Nvb6yFCIYQQomRUTzT/+9//cuDAAa5evQqAtbU1PXv2xMnJSe2qhBBCiCrHwsKi2PGXlpaWWFpaVmJEQgghROmplmhqtVqCgoL47LPP0Gq11KjxeELbvLw83n//fcaNG8eyZcvQaDRqVSmEEEIIIYQQ4imk2vImq1evZvPmzYwYMYLjx4+TlJREUlISx48fZ+TIkWzevJmPP/5YreqEEEIIIYQQQjylVEs0t23bxoABA1i3bh3Ozs7UrFmTmjVr4uzszNq1a+nXrx9bt25VqzohhBBCCCGEEE8p1RLNa9eu0aVLlyL3d+nShWvXrqlVnRBCCCGEEEKIp5RqiaaFhQWnT58ucv/p06dVX1w6IyODmTNn0qpVKxo1akTPnj355ZdflP1mZmaFvoKCgpQyEydOLLDfx8dHp5779+8THBxMixYtaNKkCcOHD+f69euqXosQQgihLytWrKBr1640b94cOzs7/P39OX/+vE4ZtfrLxMRE/P39adKkCS1atGD69Ok8ePCgwq9RCCFE5VIt0Rw0aBDbtm1j2bJl3Lt3T9mekZHB8uXL2b59O4MHD1arOgCmTJnCoUOH2LBhA8ePH6dr164MHDiQGzduABAfH6/z+ve//w3AwIEDdc7z8ssv65QLCwvT2T9r1iz27NlDaGgokZGRZGRk4O/vT25urqrXI4QQQujDTz/9xGuvvcb+/fuJiIigZs2aDBw4sMAa2eXtL3Nzc/H39yczM5PIyEhCQ0OJiIjgvffeq7RrFUIIUTlUm3X23Xff5bfffmPx4sUsWbJEmXo9OTmZ3NxcunbtyqxZs9SqjuzsbCIiIti6dSudOnUCHndw+/btY/PmzcyePbvA+mORkZHY29vz0ksv6Ww3NjYudK0ygPT0dLZt28a6devo2rUrABs3bsTNzY3Dhw/TvXt31a5JCCFE9TR58uRSH6PRaFi7dq0q9YeHh+u837hxI9bW1sTExODr66tsL29/eejQIS5cuMDZs2dp1qwZAPPnz2fKlCnMmTOH5557TpXrEUIIoX+qJZomJibs2rWLyMhIDhw4oIzH7NWrF7169aJ3795qVQXAo0ePyM3NpVatWgXiiI6OLlA+MzOT8PBwZsyYUWBfdHQ09vb21KtXjxdffJE5c+Yoj/meOnWKhw8f0q1bN6V8s2bNcHR0JDY2VhJNIYQQ5XbkyJFSL/9VkcuFZWZmkpeXh5mZmc728vaXcXFxODo6KkkmQPfu3bl//z6nTp2ic+fOFXZNQgghKle5Es3x48fTsWNHvLy8cHZ2BqBPnz706dNHleCKU7duXTw9PVm+fDnOzs5YWVmxc+dO4uLiaNGiRYHyO3fu5MGDB4wYMUJnu4+PD/3798fGxoarV6+yaNEiBgwYwOHDhzE2NiY5ORkDAwMaNmyoc5yFhQXJyckVeo1CCCGqh7Nnz+o7BB0zZ87Ezc0NT09PZZsa/WVycnKB+RoaNmyIgYGB9KlCCFHFlCvRDA8PJywsDI1Gg5mZGZ6ennTo0IGOHTvi7u6OoaGhWnEWauPGjUyePBkXFxcMDAxo06YNQ4cO5dSpUwXKbtmyhT59+mBubq6zfciQIcq/XV1dcXd3x83Njf379zNgwIAyx3bx4sVKPU4UJG2pHmlLdUg7qqc8beng4KBiJFXPu+++S0xMDPv27cPAwEDZXlH9ZUlIn6p/0pbqkbZUh7SjeiqqTy1XopmYmMiJEyeIiYkhNjaW48ePs3//fjQaDcbGxrzwwgt07NgRb29vPD09VR978fzzzxMZGUlWVhYZGRk0atSIsWPHYmtrq1PuzJkz/Prrr8ydO/eJ52zcuDFNmjTh0qVLAFhaWpKbm0tqaqpOkpqSkkKHDh2KPE9Z/pC5ePGi/AGkEmlL9UhbqkPaUT3SlhVn1qxZhIeHs2fPngJ96d+Vpb+0tLQkNjZW5zypqank5uYqczsURvpU/ZK2VI+0pTqkHdVTkW1ZrkTTxMSEzp07K2Mq8vLy+O2334iOjiY2Npa4uDiio6PRaDTUqFEDZ2dnjh49qkrg/8vU1BRTU1PS0tI4ePAgCxYs0Nm/ZcsWbGxsePnll594rtTUVG7evKlMdpB/ZzYqKgo/Pz8Arl+/Tnx8PF5eXqpfixBCCAFw8OBB1q5dy6lTp7h37x5arbZAmTt37qhW34wZM9i1axd79uyhZcuWTyxflv4yf8jL9evXadq0KQBRUVEYGxvj7u6u2rUIIYTQP9UmAwKoUaMGrVu3pnXr1rzxxhtotVr27dvH6tWriY2N5dy5c2pWx8GDB8nLy8PBwYHLly8zZ84cWrZsyahRo5Qyf/31F2FhYUyZMqXAxAmZmZl8+OGHDBgwACsrK65evcqCBQuwsLCgX79+ANSrV4+AgADmzZuHhYUF9evX57333sPV1bVEiasQQghRWnv37iUgIAAnJyeGDBlCaGgofn5+aLVa9u7di4ODg85ssOUVFBTEjh07+OKLLzAzMyMpKQl4/ENunTp1VOsvu3XrhrOzMxMmTGDRokXcvXuXuXPnMnr0aJlxVgghqhhVE80HDx7w888/ExMTQ0xMDHFxcaSnp1O3bl26d++u+h3Ae/fuMX/+fG7cuEH9+vUZMGAAs2fP1hkbGh4eTlZWlk7ymc/AwIDz58/z73//m/T0dKysrOjUqROfffYZdevWVcqFhIRgYGDA2LFjycnJoXPnznzyySc6Y1eEEEIItaxYsQJ3d3cOHDhAeno6oaGhjBo1ii5dupCQkICPjw92dnaq1bdp0yYAXnnlFZ3tM2bMYNasWar1lwYGBuzYsYOgoCB69+5NrVq18PPzY+HChapdixBCiKeDJi0treCzOCWUlpamJJUxMTGcOnWK+/fvY2tri5eXl/Jydnau0GnYqwp53lw90pbqkbZUh7SjeqpDWzZu3Jg5c+YwadIk0tLSeP755/nmm2+UpUMWL17Md999x/Hjx/Uc6dOrOnxOKou0pXqkLdUh7aiep3aMpp2dHQYGBrzwwgt4enoyefJkvL29C0xdLoQQQoiSMzY2VtaJNjU1RaPRkJKSouxv2rQply9f1ld4QgghxBPVKM/BBgYGPHz4kJSUFG7fvs3du3e5e/euWrEJIYQQ1VKLFi34448/ADA0NMTR0ZGIiAhlf2RkJI0aNdJXeEIIIcQTlXt5k/wxmbGxscydO5d79+5hZmZG+/bt8fb2xtvbm7Zt22JsbKxWzEIIIUSV5uPjw9atW5k/fz6GhoZMnDiRf/7zn7Rt2xaAy5cvF5hhXQghhHialCvRNDY2pmPHjnTs2FHZdu7cOSXx/Oyzz1iwYAFGRka0adMGb29v6RiFEEKIJwgODmbChAnUrPm4mx49ejS1atVi9+7dGBgYEBwczIgRI/QcpRBCCFE0VWedBXB1dcXV1ZXXXnuNvLw8Dhw4wKpVq4iNjeXkyZOSaAohhBBPYGhoSIMGDXS2DRs2jGHDhukpIiGEEKJ0yjVG8+9ycnI4evQoy5YtY8iQIdja2jJy5EhiY2OpVasW3t7ealYnhBBCVElt2rQhMjKyyP379u2jTZs2lRiREEIIUTrluqOZmppKdHS0srzJmTNnePToEVqtlgYNGvDiiy/SoUMHvL29eeGFF3TWtxRCCCFE4a5evUpWVlaR+7OyskhMTKzEiIQQQojSKVeiaW9vj0ajQavVYmNjw+DBg5XE0tHRUa0YhRBCiGqnuPWn//jjD+rWrVuJ0QghhBClU65E8/XXX6djx454e3vLNOtCCCFEOXz55Zd89dVXyvvly5ezZcuWAuXS0tI4f/48vXv3rszwhBBCiFIpV6K5dOlSteIQQgghqrXs7GxSU1OV95mZmdSoUXAqBVNTU8aNG8eMGTMqMzwhhBCiVFSfdVYIIYQQpffaa6/x2muvAdC6dWs+/PBD+vTpo+eohBBCiLKRRFMIIYR4ypw5c0bfIQghhBDlIommEEII8ZQ6cOAABw4c4OrVqwBYW1vTu3dvfHx89ByZEEIIUTxJNIUQQoinTE5ODmPGjOH777+nRo0ayoR7hw4dYvPmzfTo0YOtW7dibGys50iFEEKIwhWcZUAIIYQQehUSEsKBAweYPn06ly5d4rfffuO3337j8uXLzJw5k++//54PP/xQ32EKIYQQRVIt0WzQoAFhYWFF7g8PD6dBgwZqVSeEEEJUWd988w2vvvoqM2fO5LnnnlO2161bl+nTpzNq1Khi+1whhBBC31RLNLVabbH78/Lyil18WgghhBCPpaSk8MILLxS5393dnZSUlEqMSAghhCgdVR+dLS6RPHnyJGZmZmpWJ4QQQlRJTZs25ciRI0XuP3LkCE2bNq3EiIQQQojSKVeiuWHDBtq0aUObNm0AmDVrlvL+f1+2trb861//olevXqoELYQQQlQ1X331FVeuXAFg5MiR7N69m7feeosLFy7w8OFDHj58yIULF5gyZQp79uzh1Vdf1XPEQgghRNHKNeushYUFTk5OAFy9epXGjRvTuHFjnTIajQZTU1Pc3d0JDAwsT3VCCCFElTV58mQ2btyIjY0N06ZN48qVK3zxxRds375deWJIq9Wi1WoJCAhg6tSpeo5YCCGEKFq5Es2hQ4cydOhQAPr160dwcDBdunRRJTAhhBCiOvnfuQ5q1KjBmjVrmDBhAgcOHCAxMRGA5s2b07NnT1xdXfUVphBCCFEiqq2j+d1336l1KiGEEEIArq6uklQKIYR4Jqk6GdCdO3dYtGgRvXr1om3btsTFxSnblyxZQnx8vJrVCSGEEFWKzM4uhBCiqlDtjuaVK1fw9fXlzp07uLi4kJCQQHZ2NvB4jc3w8HBu377NsmXL1KpSCCGEqFImT57MW2+9VaKyGo2GGzduVHBEQgghRNmolmjOmzcPrVZLTEwMdevWxd7eXmd/nz592Lt3r1rVCSGEEFVOu3btsLW11XcYQgghRLmplmgePnyYKVOmYGtry507dwrst7GxkV9ehRBCiGKMHTsWPz8/fYchhBBClJtqYzTv37+PmZlZkfvT09OpUUPVIaFkZGQwc+ZMWrVqRaNGjejZsye//PKLsn/ixImYmZnpvHx8fArEHRwcTIsWLWjSpAnDhw/n+vXrOmUSExPx9/enSZMmtGjRgunTp/PgwQNVr0UIIYSoTjZt2kTr1q2xsrKiS5cuHD9+XN8hCSGEUJFqmZ+zszPHjh0rcv/evXtp3bq1WtUBMGXKFA4dOsSGDRs4fvw4Xbt2ZeDAgTp3Tl9++WXi4+OVV1hYmM45Zs2axZ49ewgNDSUyMpKMjAz8/f3Jzc0FIDc3F39/fzIzM4mMjCQ0NJSIiAjee+89Va9FCCGEqC7Cw8OZOXMm77zzDkeOHMHT0xM/Pz9lGRchhBDPPtUenZ04cSJvvPEGzs7ODBo0CIC8vDx+//13li5dysmTJ9m+fbta1ZGdnU1ERARbt26lU6dOwOOkcd++fWzevJnZs2cDYGxsjJWVVaHnSE9PZ9u2baxbt46uXbsCsHHjRtzc3Dh8+DDdu3fn0KFDXLhwgbNnz9KsWTMA5s+fz5QpU5gzZw7PPfecatckhBBCVAfr1q1j5MiRjBkzBoBly5Zx8OBBNm/ezLx581Sty2xV0U9bCSFEdXei74kKO7dqiaafnx/Xrl1j8eLFLF68GIAhQ4YAjxeenj9/Pr6+vmpVx6NHj8jNzaVWrVo6201MTIiOjlbeR0dHY29vT7169XjxxReZM2cOFhYWAJw6dYqHDx/SrVs3pXyzZs1wdHQkNjaW7t27ExcXh6Ojo5JkAnTv3p379+9z6tQpOnfurNo1CSGEqL7u3r2r7xAqxYMHDzh16lSB2XW7detGbGysnqISQgihNtUSTYCpU6fi5+dHREQEly5dIi8vj+eff57+/furPote3bp18fT0ZPny5Tg7O2NlZcXOnTuJi4ujRYsWAPj4+NC/f39sbGy4evUqixYtYsCAARw+fBhjY2OSk5MxMDCgYcOGOue2sLAgOTkZgOTkZCUxzdewYUMMDAyUMoW5ePFima6rrMeJgqQt1SNtqQ5pR/WUpy0dHBxUjESUVmpqKrm5uQX61v/te/9O+lQhhKg4FdWnqppowuM7gpMmTVL7tIXauHEjkydPxsXFBQMDA9q0acPQoUM5deoU8H93VAFcXV1xd3fHzc2N/fv3M2DAgAqNrSx/yFy8eFH+AFKJtKV6pC3VIe2oHmnL6kf6VCGEqDgV9V2peqJZmZ5//nkiIyPJysoiIyODRo0aMXbs2CLvnjZu3JgmTZpw6dIlACwtLcnNzSU1NRVzc3OlXEpKCh06dFDK/P1RnvxfYy0tLSvmwoQQQogqKv+poJSUFJ3tKSkpFdKvpr2dpvo5qxtJ2tUjbakOaUf1VOSTH+VKNNu0aVOq8hqNRrnbqCZTU1NMTU1JS0vj4MGDLFiwoNByqamp3Lx5U5kcyN3dHUNDQ6KiopR1y65fv058fDxeXl4AyuO5169fp2nTpgBERUVhbGyMu7u76tcihBBCVGVGRka4u7sTFRXFwIEDle1RUVEV/rSREEKIylOuRNPJyalE5RITE7lw4QIajaY81RVw8OBB8vLycHBw4PLly8yZM4eWLVsyatQoMjMz+fDDDxkwYABWVlZcvXqVBQsWYGFhQb9+/QCoV68eAQEBzJs3DwsLC+rXr897772Hq6srL7/8MvB4cgJnZ2cmTJjAokWLuHv3LnPnzmX06NEy46wQQghRBpMnT+aNN97RVrIwAAARmElEQVSgXbt2eHl5sXnzZm7dusXYsWP1HZoQQgiVlCvR3LFjR7H7ExMTWb58uXIHMCAgoDzVFXDv3j3mz5/PjRs3qF+/PgMGDGD27NkYGhry6NEjzp8/z7///W/S09OxsrKiU6dOfPbZZ9StW1c5R0hICAYGBowdO5acnBw6d+7MJ598goGBAQAGBgbs2LGDoKAgevfuTa1atfDz82PhwoWqXosQQghRXQwePJg7d+6wbNkykpKScHZ25uuvv8ba2lrfoQkhhFCJJi0tTav2Sa9du8ZHH33El19+CcDo0aOZOnUqTZo0UbsqIYQQQgghhBBPGVUnA7p+/TofffQR27dvByAgIIBp06ZJgimEEEIIIYQQ1YgqiebfE8xXX32VadOmKZPnCCGEEEIIIYSoPsqVaF6/fp0VK1awfft2tFqtJJhCCCGEEEIIIco3RtPKyoqHDx/i5ubGtGnTaNas2ROPadeuXVmrE0IIIYQQQgjxDChXolm/fv3/O9ETli7RarVoNBru3LlT1uqEEEIIIYQQQjwDapTn4HXr1imvtWvXFvvKLyMKt2nTJlq3bo2VlRVdunTh+PHj+g7pqRYSEoKZmZnOq2XLlsp+rVZLSEgITk5ONGrUiL59+3LhwgU9Rvz0OHbsGMOHD8fZ2RkzMzNlbHW+krRdWloa48ePx9raGmtra8aPH09aWlplXsZT4UltOXHixAKfUx8fH50y9+/fJzg4mBYtWtCkSROGDx/O9evXK/My9G7FihV07dqV5s2bY2dnh7+/P+fPn9cpI59LURrSp5aO9KllJ32qeqRPVcfT1KeWK9EcOXJkqV+ioPDwcGbOnMk777zDkSNH8PT0xM/Pj8TERH2H9lRzcHAgPj5eef3vHxKrV69m3bp1LFmyhEOHDmFhYcGgQYPIyMjQY8RPh6ysLFxcXPjwww8xMTEpsL8kbRcYGMiZM2fYuXMnO3fu5MyZM7zxxhuVeRlPhSe1JcDLL7+s8zkNCwvT2T9r1iz27NlDaGgokZGRZGRk4O/vT25ubmVcwlPhp59+4rXXXmP//v1ERERQs2ZNBg4cyN27d5Uy8rkUJSV9atlIn1o20qeqR/pUdTxNfWqFrKMpSqd79+64urry8ccfK9vatm3LK6+8wrx58/QY2dMrJCSEiIgIoqOjC+zTarU4OTnx+uuvExQUBEB2djYODg4sXLiQsWPHVna4T62mTZuydOlSRo0aBZSs7eLj4/Hy8mLfvn14e3sDEB0dja+vLydOnMDBwUFv16NPf29LePzr6507d9ixY0ehx6Snp2Nvb8+6desYNmwY8HgdYjc3N3bu3En37t0rJfanTWZmJtbW1mzfvh1fX1/5XIpSkT619KRPVYf0qeqRPlU9+uxTy3VHU5TfgwcPOHXqFN26ddPZ3q1bN2JjY/UU1bMhISEBJycnWrduzbhx40hISADgypUrJCUl6bSpiYkJHTt2lDZ9gpK0XVxcHHXq1MHLy0sp4+3tjampqbRvIaKjo7G3t6ddu3ZMmTKFlJQUZd+pU6d4+PChTns3a9YMR0fHat2WmZmZ5OXlYWZmBsjnUpSc9KllJ32q+uS7S33Sp5aePvtUVdbRFGWXmppKbm4uFhYWOtstLCxITk7WU1RPPw8PD9avX4+DgwO3b99m2bJl9OzZk5iYGJKSkgAKbdObN2/qI9xnRknaLjk5mYYNG+pMAKbRaDA3N5fP7N/4+PjQv39/bGxsuHr1KosWLWLAgAEcPnwYY2NjkpOTMTAwoGHDhjrHVff//zNnzsTNzQ1PT09APpei5KRPLRvpUyuGfHepS/rUstFnnyqJpngm9ejRQ+e9h4cH7u7ufPnll7Rv315PUQmha8iQIcq/XV1dcXd3x83Njf379zNgwAA9Rvb0evfdd4mJiWHfvn0YGBjoOxwhqgXpU8WzQPrU0tN3nyqPzupZw4YNMTAw0Ln1D5CSkoKlpaWeonr21KlTBycnJy5duoSVlRWAtGkZlKTtLC0tSU1NRav9v+HdWq2W27dvS/s+QePGjWnSpAmXLl0CHrdlbm4uqampOuWq62d11qxZfPPNN0RERGBra6tsl8+lKCnpU9Uhfao65LurYkmfWrynoU+VRFPPjIyMcHd3JyoqSmd7VFSUznPRong5OTlcvHgRKysrbGxssLKy0mnTnJwcoqOjpU2foCRt5+npSWZmJnFxcUqZuLg4srKypH2fIDU1lZs3bypf8u7u7hgaGuq09/Xr15VB+NXJjBkzlA7xf5dVAPlcipKTPlUd0qeqQ767Kpb0qUV7WvpUg5kzZ75fvksR5VW3bl1CQkJo1KgRtWrVYtmyZRw/fpy1a9dSr149fYf3VJo9ezZGRkbk5eXxxx9/EBwczKVLl1i5ciVmZmbk5uayatUq7OzsyM3N5b333iMpKYlVq1ZhbGys7/D1KjMzk//+978kJSWxbds2XFxc/l979x9TVfnAcfx90QCd4qUhaGDxa3mRIBPLhj+Y2TLmyiKVSyoSzjYz2zAriMbKP4xlUrpAWZD0ayQgRmy1thQRwdnKCKlRSJRGS5G6IiQIwvcP59n3Kn1Fdvnei35eG9s9D899znPOLvvwPM855+Ll5cWFCxeYMGHCNc+dj48P33zzDSUlJURERNDS0kJKSgozZsy46R7H/r/O5ahRo9i0aRPjxo2jt7eXY8eOsX79ei5evMiWLVvw8PDA09OTP//8k7y8PMLDwzl79iwpKSl4eXnx2muv4eZ2c8wFbty4kU8++YSCggICAgLo7Oyks7MTuDRwMJlM+lzKoClTr58ydeiUqY6jTHUMV8pUfb2Ji8jLy2Pbtm2cOnWKsLAwNm/ezOzZs53dLZeVnJxMTU0NbW1t+Pj4MHPmTNLT07FYLMCl5f3MzEwKCgqw2WxERUXx5ptvMm3aNCf33Pmqqqp45JFHripPSEhgx44dgzp3NpuNF198kS+++AKA2NhY3njjDeOJZjeL/3Uus7KyWL58OXV1dZw9exY/Pz/mzp1Leno6AQEBRt3u7m5eeeUVSkpK6OrqYt68eWzdutWuzo3u3z43L730EmlpacDg/qb1uZTLlKnXR5k6dMpUx1GmOoYrZaoGmiIiIiIiIuJQN8casoiIiIiIiPzfaKApIiIiIiIiDqWBpoiIiIiIiDiUBpoiIiIiIiLiUBpoioiIiIiIiENpoCkiIiIiIiIOpYGmiLiEiIgInnjiCWd3Q0REZMRTpoor0EBTZJh8/PHHmM1m48fPzw+LxUJcXBw7d+7k3Llzzu6iiIjIiKBMFRl5Rju7AyI3utTUVIKCgujp6eH06dMcOnSItLQ0srOzKSws5K677nJ2F0VEREYEZarIyKGBpsgwW7BgAffee6+xvWHDBiorK7FarSQkJPD1118zZswYJ/bw5tHf309XV5fOt4jICKVMdR3KVLkWXTor4gQxMTG88MILnDx5kqKiIqO8vr6eZ555hunTp+Pn50dwcDDJycmcPHnSqNPU1ITZbOadd965qt36+nrMZjP5+fn/uu/ffvsNs9nMW2+9xfvvv8/06dPx9fVl/vz5HD161K7uokWLWLRo0VVtrF27loiIiAHbzMvL4+6772by5MksXryYEydO0N/fz9atWwkPD2fSpElYrVba2toG7F9lZSUxMTH4+fkRFRVFYWHhVXW6u7vJzMxkxowZ+Pr6EhYWRlpaGv/8849dPbPZTEpKCqWlpURHR+Pr60tpaem/nhsRERl5lKnKVHFNWtEUcZL4+Hg2bdrE/v37WbVqFQAVFRUcP34cq9XK5MmTaW5u5r333uPbb7/l8OHDjB07lpCQEO677z6Kiop49tln7dosKirC3d2duLi4a+6/tLSUzs5OnnrqKUwmE9u2bWPlypXU1tZyyy23DOmY9uzZw4ULF1izZg02m43t27eTlJTEggULOHDgAM899xzNzc3k5uby8ssvk5uba/f+X3/9lcTERFatWoXVaqW4uJi1a9fi4eFhHFN/fz8rVqygurqaxMRELBYLP/30E/n5+TQ0NFBaWorJZDLarKmpoaysjDVr1uDn58edd945pGMTERHXpUxVporr0UBTxEn8/f3x8vKiubnZKFu9ejXr16+3qxcbG8vChQspLy8nPj4eAKvVyoYNG2hoaMBisQDQ19fHnj17eOihh/D29r7m/ltaWjh69ChmsxmA0NBQnnzySfbt28fDDz88pGP6448/7Nrs6+sjKyuL8+fPc/DgQSNsz5w5Q2lpKW+//bbdJTdNTU3k5eWxZMkSAJKSkpg3bx4ZGRk89thjuLm5UVJSwldffUV5eTlz5swx3nvPPffw9NNPU1FRwQMPPGCU//zzz1RWVhIZGTmkYxIREdenTFWmiuvRpbMiTjRu3Dg6OjqM7bFjxxqvOzo6+OuvvwgNDWXChAnU1tYav4uLi8PDw4Pdu3cbZVVVVbS0tBjBeS2PPvqoEV4A0dHRwKUZ0KG6ss2oqCgAli1bZjejGxUVRU9PDy0tLXbvnzhxot3M8ZgxY0hMTOT333+nvr4egL179xIaGkpYWBhtbW3Gz+zZszGZTFRVVdm1OWvWLAWiiMhNQJmqTBXXohVNESfq6OjAx8fH2LbZbLz66quUlZXx999/29Vtb283XpvNZmJjYykuLiYjIwOTyURRURHe3t4sXLhwUPsOCAiw274cZjabbaiHc1WbXl5ewKWZ5oHKr9xXUFAQbm72818hISEAnDhxgsjISJqammhsbDTKr9Ta2mq3HRgYeH0HISIiI5IyVZkqrkUDTREnaWlpob29neDgYKMsKSmJI0eOsG7dOiIjIxk/fjwmk4nk5GT6+vrs3m+1Wvn000+prq5m5syZlJeXs2TJEtzd3Qe1/1GjRg1Y3t/fb7w2mUx225ddvHjxutoczL4Gq6+vD4vFQmZm5oC/nzRpkt22noYnInLjU6YqU8X1aKAp4iSXL9G5fO+DzWbjwIEDpKamkpqaatTr6uoacEb0wQcfZOLEiezevZvW1lba29sHfYnPYJnN5gEv+/nvJ/Y5UnNzM319fXYzsE1NTQDcfvvtwKUZ2traWmJiYuweUCAiIjcvZerVlKnibLpHU8QJKisr2bJlC3fccQfLli0DMILgyhnJnJycq2ZeAUaPHs3SpUspKyvjww8/JDg4mFmzZjm0n0FBQTQ2NnLmzBmj7NixYxw5csSh+7mstbXV7lHp58+f54MPPsDf39/4Eu7HH3+c06dPD/i4+e7ubs6dOzcsfRMREdekTB2YMlWcTSuaIsNs3759/PLLL/T29tLa2srBgwepqKhgypQpFBYW4unpCVy6x2LOnDls376dnp4epkyZwuHDh6mpqeHWW28dsG2r1UpOTg779++3m7F1lBUrVpCdnU1cXBwrV66ktbWVXbt2YbFYhiV8QkJCeP7556mrq+O2226jqKiIxsZG3n33XeOfhvj4eMrKyti4cSPV1dXcf//99Pf3c/z4cfbu3UtBQQFz5851eN9ERMT5lKmDp0wVZ9NAU2SYXb7vwd3dHW9vb6ZNm8brr7/O8uXLGT9+vF3dvLw8UlNT2bVrF729vURHR/PZZ5+xePHiAduOjIwkPDycH374weGX+ABMnTqVnTt3snnzZtLT05k6dSq5ubkUFxdz6NAhh+8vMDCQrKwsMjIyaGhowN/fn+zsbJYuXWrUcXNz46OPPmLHjh0UFhby+eef4+npSWBgIKtXrzZmaUVE5MajTB08Zao4m8lms13/ncMi4jLmz5+Pu7s7X375pbO7IiIiMqIpU0UcR/doioxgdXV1fPfddyQkJDi7KyIiIiOaMlXEsbSiKTIC/fjjj9TW1pKTk8OpU6f4/vvv7b6YWkRERAZHmSoyPLSiKTIClZWVsW7dOrq6usjPz1cgioiIDJEyVWR4aEVTREREREREHEormiIiIiIiIuJQGmiKiIiIiIiIQ2mgKSIiIiIiIg6lgaaIiIiIiIg4lAaaIiIiIiIi4lAaaIqIiIiIiIhD/QfHyCG+nIwhCgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mean_reward</td><td>▁███████████████████████████████████████</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/fps</td><td>▁▆▇▇▇▇▇█████████████████████████████████</td></tr><tr><td>train/entropy_loss</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/explained_variance</td><td>▁▅▇█▆█▆▇▇▇▇▅▇▆█▆▇▇▇▇▇▇▆▅▇▅▇▇▇▆█▇▆▆▇▇▇▇▇▇</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/policy_loss</td><td>▁▂▅▅▄▅▆█▄▃▅▅▄▃▃▃▄▄▄▂▄▅▄▂▂▃▃▂▂▃▂▁▃▃▁▁▂▁▂▂</td></tr><tr><td>train/std</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/value_loss</td><td>▁▂▅▅▄▄▆█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>1000.0</td></tr><tr><td>eval/mean_reward</td><td>3003000.0</td></tr><tr><td>global_step</td><td>100000</td></tr><tr><td>time/fps</td><td>115.0</td></tr><tr><td>train/entropy_loss</td><td>-0.6787</td></tr><tr><td>train/explained_variance</td><td>0.0</td></tr><tr><td>train/learning_rate</td><td>0.0007</td></tr><tr><td>train/policy_loss</td><td>11692.10156</td></tr><tr><td>train/std</td><td>0.36478</td></tr><tr><td>train/value_loss</td><td>381375904.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">stellar-haze-7</strong>: <a href=\"https://wandb.ai/nishamdev/StockTrading/runs/16thhgmp\" target=\"_blank\">https://wandb.ai/nishamdev/StockTrading/runs/16thhgmp</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 3 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221124_215606-16thhgmp/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from stocktrade import Stocktrade\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    timesteps = 100000\n",
        "    algo = \"A2C\"\n",
        "    Stocktrade.stocktrade(algo,timesteps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment#4 - A2C over 100 steps (Hyper Parameter Tuning)\n",
        "\n",
        "**Experiment A2C algo with gamma=0.8 , learning_rate=0.000010 , ent_coef=0.3**"
      ],
      "metadata": {
        "id": "CQTN9evoLqds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### 6.2  Running A2C algorithm over 100 time steps\n",
        "from stocktrade1 import Stocktrade\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    timesteps = 100000\n",
        "    algo = \"A2C\"\n",
        "    hyper_param = True\n",
        "    Stocktrade.stocktrade(algo,timesteps,hyper_param)"
      ],
      "metadata": {
        "id": "XO5CWxIzIOyv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4659263f-d10f-4ed6-fb05-6a9a0250d275"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Reinforcement-learning-Live-Trading/wandb/run-20221124_221107-1wvgfvb0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/nishamdev/StockTrading/runs/1wvgfvb0\" target=\"_blank\">swept-elevator-8</a></strong> to <a href=\"https://wandb.ai/nishamdev/StockTrading\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
            "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Logging to runs/1wvgfvb0/A2C_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Reinforcement-learning-Live-Trading/env/stock_trading_env.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  prev_cost + additional_cost) / (self.shares_held + shares_bought)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 392      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.84    |\n",
            "|    explained_variance | 2.44e-06 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 8.74e+03 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 1.81e+07 |\n",
            "------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=1000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.84     |\n",
            "|    explained_variance | -8.34e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 8.52e+03  |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.16e+07  |\n",
            "-------------------------------------\n",
            "New best mean reward!\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 116  |\n",
            "|    iterations      | 200  |\n",
            "|    time_elapsed    | 8    |\n",
            "|    total_timesteps | 1000 |\n",
            "-----------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 151       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.84     |\n",
            "|    explained_variance | -1.31e-06 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | 4.21e+03  |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 2.67e+06  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.85     |\n",
            "|    explained_variance | -4.77e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 8.32e+03  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.08e+07  |\n",
            "-------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 116  |\n",
            "|    iterations      | 400  |\n",
            "|    time_elapsed    | 17   |\n",
            "|    total_timesteps | 2000 |\n",
            "-----------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 135       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.85     |\n",
            "|    explained_variance | -4.77e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 5.3e+03   |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.79e+06  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=3000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.85     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 9.64e+03  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.14e+07  |\n",
            "-------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 115  |\n",
            "|    iterations      | 600  |\n",
            "|    time_elapsed    | 25   |\n",
            "|    total_timesteps | 3000 |\n",
            "-----------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 128       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.85     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | 5.96e+03  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 7.68e+06  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.86    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | 1.29e+04 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.16e+07 |\n",
            "------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 115  |\n",
            "|    iterations      | 800  |\n",
            "|    time_elapsed    | 34   |\n",
            "|    total_timesteps | 4000 |\n",
            "-----------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 125       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 35        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.87     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 4.31e+03  |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.69e+06  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=5000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.86    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 9.58e+03 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.16e+07 |\n",
            "------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 115  |\n",
            "|    iterations      | 1000 |\n",
            "|    time_elapsed    | 43   |\n",
            "|    total_timesteps | 5000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 123      |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 44       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.88    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 2.16e+04 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 5.4e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 6000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.87     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 7.28e+03  |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.11e+07  |\n",
            "-------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 116  |\n",
            "|    iterations      | 1200 |\n",
            "|    time_elapsed    | 51   |\n",
            "|    total_timesteps | 6000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 122      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 52       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.87    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 3.87e+03 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 2.71e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=7000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.86     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 1.19e+04  |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.41e+07  |\n",
            "-------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 116  |\n",
            "|    iterations      | 1400 |\n",
            "|    time_elapsed    | 60   |\n",
            "|    total_timesteps | 7000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 121      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 61       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.86    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 4.16e+03 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.9e+06  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.86    |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 9.3e+03  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.06e+07 |\n",
            "------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 116  |\n",
            "|    iterations      | 1600 |\n",
            "|    time_elapsed    | 68   |\n",
            "|    total_timesteps | 8000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 121      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 70       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.87    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 4.23e+03 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 2.85e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=9000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.87    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 8.19e+03 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 1.06e+07 |\n",
            "------------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 116  |\n",
            "|    iterations      | 1800 |\n",
            "|    time_elapsed    | 77   |\n",
            "|    total_timesteps | 9000 |\n",
            "-----------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 120      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 78       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.87    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 3.41e+04 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.49e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.86    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 1.15e+04 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.98e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 2000  |\n",
            "|    time_elapsed    | 86    |\n",
            "|    total_timesteps | 10000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 120      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 87       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.85    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 5.54e+03 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 2.66e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=11000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 11000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.85    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | 5.14e+04 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 4.37e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 2200  |\n",
            "|    time_elapsed    | 94    |\n",
            "|    total_timesteps | 11000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 119       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 96        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.84     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 1.54e+04  |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 4.53e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.84     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 7.19e+03  |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.18e+07  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 2400  |\n",
            "|    time_elapsed    | 103   |\n",
            "|    total_timesteps | 12000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 119      |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 104      |\n",
            "|    total_timesteps    | 12500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.84    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | 6.86e+03 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 2.85e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=13000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.84    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | 8.03e+03 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 1.33e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 2600  |\n",
            "|    time_elapsed    | 112   |\n",
            "|    total_timesteps | 13000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 119      |\n",
            "|    iterations         | 2700     |\n",
            "|    time_elapsed       | 113      |\n",
            "|    total_timesteps    | 13500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.84    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2699     |\n",
            "|    policy_loss        | 4.8e+03  |\n",
            "|    std                | 0.998    |\n",
            "|    value_loss         | 5.39e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.83     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 3.15e+04  |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 1.7e+08   |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 2800  |\n",
            "|    time_elapsed    | 120   |\n",
            "|    total_timesteps | 14000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 122      |\n",
            "|    total_timesteps    | 14500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.84    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | 4.02e+03 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 4.19e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.83    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | 1.29e+04 |\n",
            "|    std                | 0.996    |\n",
            "|    value_loss         | 2.71e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 3000  |\n",
            "|    time_elapsed    | 129   |\n",
            "|    total_timesteps | 15000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 130      |\n",
            "|    total_timesteps    | 15500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.8     |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | 3.77e+03 |\n",
            "|    std                | 0.981    |\n",
            "|    value_loss         | 2.71e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 16000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.8      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3199      |\n",
            "|    policy_loss        | 8.48e+03  |\n",
            "|    std                | 0.983     |\n",
            "|    value_loss         | 1.51e+07  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 3200  |\n",
            "|    time_elapsed    | 137   |\n",
            "|    total_timesteps | 16000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 139      |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.79    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 7.68e+03 |\n",
            "|    std                | 0.977    |\n",
            "|    value_loss         | 1.5e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=17000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 17000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.81    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | 2.82e+04 |\n",
            "|    std                | 0.984    |\n",
            "|    value_loss         | 1.56e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 3400  |\n",
            "|    time_elapsed    | 146   |\n",
            "|    total_timesteps | 17000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 147      |\n",
            "|    total_timesteps    | 17500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.8     |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | 5.12e+03 |\n",
            "|    std                | 0.983    |\n",
            "|    value_loss         | 5.28e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 18000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.8     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | 7.64e+03 |\n",
            "|    std                | 0.983    |\n",
            "|    value_loss         | 1.09e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 3600  |\n",
            "|    time_elapsed    | 155   |\n",
            "|    total_timesteps | 18000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 118       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 156       |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.8      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 3.13e+03  |\n",
            "|    std                | 0.979     |\n",
            "|    value_loss         | 2.76e+06  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=19000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 19000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.79    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 5.67e+04 |\n",
            "|    std                | 0.979    |\n",
            "|    value_loss         | 5.85e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 3800  |\n",
            "|    time_elapsed    | 163   |\n",
            "|    total_timesteps | 19000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 165      |\n",
            "|    total_timesteps    | 19500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.78    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | 4.22e+03 |\n",
            "|    std                | 0.971    |\n",
            "|    value_loss         | 2.76e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.79    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | 7.19e+03 |\n",
            "|    std                | 0.975    |\n",
            "|    value_loss         | 1.18e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 4000  |\n",
            "|    time_elapsed    | 172   |\n",
            "|    total_timesteps | 20000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 118      |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 173      |\n",
            "|    total_timesteps    | 20500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.79    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 2.98e+04 |\n",
            "|    std                | 0.975    |\n",
            "|    value_loss         | 1.37e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=21000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 21000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.78    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 2.17e+04 |\n",
            "|    std                | 0.973    |\n",
            "|    value_loss         | 5.22e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 4200  |\n",
            "|    time_elapsed    | 180   |\n",
            "|    total_timesteps | 21000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 182      |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.76    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | 6.07e+03 |\n",
            "|    std                | 0.961    |\n",
            "|    value_loss         | 6.34e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 22000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.76    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | 9.33e+04 |\n",
            "|    std                | 0.961    |\n",
            "|    value_loss         | 1.4e+09  |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 4400  |\n",
            "|    time_elapsed    | 189   |\n",
            "|    total_timesteps | 22000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 191      |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.75    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 2.85e+04 |\n",
            "|    std                | 0.956    |\n",
            "|    value_loss         | 1.05e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=23000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 23000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.75    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | 8.71e+03 |\n",
            "|    std                | 0.957    |\n",
            "|    value_loss         | 1.14e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 4600  |\n",
            "|    time_elapsed    | 198   |\n",
            "|    total_timesteps | 23000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 200      |\n",
            "|    total_timesteps    | 23500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.74    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | 3.84e+03 |\n",
            "|    std                | 0.955    |\n",
            "|    value_loss         | 2.83e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 24000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.73    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4799     |\n",
            "|    policy_loss        | 8.44e+03 |\n",
            "|    std                | 0.95     |\n",
            "|    value_loss         | 1.21e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 4800  |\n",
            "|    time_elapsed    | 207   |\n",
            "|    total_timesteps | 24000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 208       |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.75     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | 3.46e+03  |\n",
            "|    std                | 0.955     |\n",
            "|    value_loss         | 2.72e+06  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 25000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.75     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4999      |\n",
            "|    policy_loss        | 7.07e+04  |\n",
            "|    std                | 0.957     |\n",
            "|    value_loss         | 9.89e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 5000  |\n",
            "|    time_elapsed    | 216   |\n",
            "|    total_timesteps | 25000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 217      |\n",
            "|    total_timesteps    | 25500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.74    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | 3.38e+03 |\n",
            "|    std                | 0.953    |\n",
            "|    value_loss         | 2.57e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.74    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | 1.18e+05 |\n",
            "|    std                | 0.952    |\n",
            "|    value_loss         | 2.29e+09 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 5200  |\n",
            "|    time_elapsed    | 224   |\n",
            "|    total_timesteps | 26000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 225      |\n",
            "|    total_timesteps    | 26500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.73    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | 1.14e+04 |\n",
            "|    std                | 0.949    |\n",
            "|    value_loss         | 4.27e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=27000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 27000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.71    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | 1.97e+04 |\n",
            "|    std                | 0.939    |\n",
            "|    value_loss         | 7.39e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 5400  |\n",
            "|    time_elapsed    | 233   |\n",
            "|    total_timesteps | 27000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 234      |\n",
            "|    total_timesteps    | 27500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.7     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | 6.71e+03 |\n",
            "|    std                | 0.936    |\n",
            "|    value_loss         | 7.06e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.71    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | 8.48e+03 |\n",
            "|    std                | 0.941    |\n",
            "|    value_loss         | 1.16e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 5600  |\n",
            "|    time_elapsed    | 241   |\n",
            "|    total_timesteps | 28000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 243      |\n",
            "|    total_timesteps    | 28500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.7     |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | 5.71e+03 |\n",
            "|    std                | 0.933    |\n",
            "|    value_loss         | 3.64e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=29000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.69    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | 1.99e+04 |\n",
            "|    std                | 0.929    |\n",
            "|    value_loss         | 4.97e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 5800  |\n",
            "|    time_elapsed    | 250   |\n",
            "|    total_timesteps | 29000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 5900      |\n",
            "|    time_elapsed       | 251       |\n",
            "|    total_timesteps    | 29500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.7      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5899      |\n",
            "|    policy_loss        | 2.28e+04  |\n",
            "|    std                | 0.935     |\n",
            "|    value_loss         | 5.02e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.69    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | 8.87e+03 |\n",
            "|    std                | 0.933    |\n",
            "|    value_loss         | 1.11e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 6000  |\n",
            "|    time_elapsed    | 258   |\n",
            "|    total_timesteps | 30000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 6100     |\n",
            "|    time_elapsed       | 260      |\n",
            "|    total_timesteps    | 30500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.72    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6099     |\n",
            "|    policy_loss        | 4.28e+03 |\n",
            "|    std                | 0.945    |\n",
            "|    value_loss         | 2.87e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=31000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 31000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.73     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6199      |\n",
            "|    policy_loss        | 1.32e+05  |\n",
            "|    std                | 0.952     |\n",
            "|    value_loss         | 1.14e+09  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 6200  |\n",
            "|    time_elapsed    | 267   |\n",
            "|    total_timesteps | 31000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 6300     |\n",
            "|    time_elapsed       | 268      |\n",
            "|    total_timesteps    | 31500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.72    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6299     |\n",
            "|    policy_loss        | 5.98e+03 |\n",
            "|    std                | 0.947    |\n",
            "|    value_loss         | 6.44e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.72     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 9.51e+04  |\n",
            "|    std                | 0.945     |\n",
            "|    value_loss         | 9.88e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 6400  |\n",
            "|    time_elapsed    | 276   |\n",
            "|    total_timesteps | 32000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 6500     |\n",
            "|    time_elapsed       | 277      |\n",
            "|    total_timesteps    | 32500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.72    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6499     |\n",
            "|    policy_loss        | 3.58e+03 |\n",
            "|    std                | 0.947    |\n",
            "|    value_loss         | 2.72e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=33000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 33000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.71    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6599     |\n",
            "|    policy_loss        | 9.16e+03 |\n",
            "|    std                | 0.943    |\n",
            "|    value_loss         | 1.05e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 6600  |\n",
            "|    time_elapsed    | 284   |\n",
            "|    total_timesteps | 33000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 6700     |\n",
            "|    time_elapsed       | 285      |\n",
            "|    total_timesteps    | 33500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.72    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6699     |\n",
            "|    policy_loss        | 3.96e+03 |\n",
            "|    std                | 0.945    |\n",
            "|    value_loss         | 2.58e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.72     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | 7.95e+04  |\n",
            "|    std                | 0.947     |\n",
            "|    value_loss         | 1.37e+09  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 6800  |\n",
            "|    time_elapsed    | 293   |\n",
            "|    total_timesteps | 34000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 6900     |\n",
            "|    time_elapsed       | 294      |\n",
            "|    total_timesteps    | 34500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.72    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6899     |\n",
            "|    policy_loss        | 1.18e+04 |\n",
            "|    std                | 0.945    |\n",
            "|    value_loss         | 2.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 35000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.7     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6999     |\n",
            "|    policy_loss        | 5.02e+04 |\n",
            "|    std                | 0.934    |\n",
            "|    value_loss         | 8.2e+08  |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 7000  |\n",
            "|    time_elapsed    | 301   |\n",
            "|    total_timesteps | 35000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 7100     |\n",
            "|    time_elapsed       | 303      |\n",
            "|    total_timesteps    | 35500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.69    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7099     |\n",
            "|    policy_loss        | 2.17e+04 |\n",
            "|    std                | 0.929    |\n",
            "|    value_loss         | 1.42e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 36000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.69    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7199     |\n",
            "|    policy_loss        | 1.32e+05 |\n",
            "|    std                | 0.933    |\n",
            "|    value_loss         | 2.09e+09 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 7200  |\n",
            "|    time_elapsed    | 310   |\n",
            "|    total_timesteps | 36000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 7300     |\n",
            "|    time_elapsed       | 311      |\n",
            "|    total_timesteps    | 36500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.69    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7299     |\n",
            "|    policy_loss        | 3.69e+03 |\n",
            "|    std                | 0.933    |\n",
            "|    value_loss         | 4.06e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=37000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 37000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.69    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7399     |\n",
            "|    policy_loss        | 2.31e+04 |\n",
            "|    std                | 0.932    |\n",
            "|    value_loss         | 7.17e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 115   |\n",
            "|    iterations      | 7400  |\n",
            "|    time_elapsed    | 319   |\n",
            "|    total_timesteps | 37000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 7500     |\n",
            "|    time_elapsed       | 320      |\n",
            "|    total_timesteps    | 37500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.68    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7499     |\n",
            "|    policy_loss        | 1.3e+04  |\n",
            "|    std                | 0.928    |\n",
            "|    value_loss         | 4.66e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 38000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.68    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7599     |\n",
            "|    policy_loss        | 3.7e+04  |\n",
            "|    std                | 0.927    |\n",
            "|    value_loss         | 1.72e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 7600  |\n",
            "|    time_elapsed    | 327   |\n",
            "|    total_timesteps | 38000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 7700     |\n",
            "|    time_elapsed       | 328      |\n",
            "|    total_timesteps    | 38500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.68    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7699     |\n",
            "|    policy_loss        | 1.75e+04 |\n",
            "|    std                | 0.926    |\n",
            "|    value_loss         | 8.79e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=39000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 39000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.66    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7799     |\n",
            "|    policy_loss        | 4.45e+04 |\n",
            "|    std                | 0.917    |\n",
            "|    value_loss         | 5.95e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 7800  |\n",
            "|    time_elapsed    | 336   |\n",
            "|    total_timesteps | 39000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 7900     |\n",
            "|    time_elapsed       | 337      |\n",
            "|    total_timesteps    | 39500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.66    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7899     |\n",
            "|    policy_loss        | 3.08e+04 |\n",
            "|    std                | 0.915    |\n",
            "|    value_loss         | 2.6e+08  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 40000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.65     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7999      |\n",
            "|    policy_loss        | 7.4e+04   |\n",
            "|    std                | 0.913     |\n",
            "|    value_loss         | 1.07e+09  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 8000  |\n",
            "|    time_elapsed    | 344   |\n",
            "|    total_timesteps | 40000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 8100     |\n",
            "|    time_elapsed       | 345      |\n",
            "|    total_timesteps    | 40500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.66    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8099     |\n",
            "|    policy_loss        | 2.45e+04 |\n",
            "|    std                | 0.918    |\n",
            "|    value_loss         | 1.07e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=41000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 41000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.66    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8199     |\n",
            "|    policy_loss        | 3.85e+04 |\n",
            "|    std                | 0.918    |\n",
            "|    value_loss         | 4.26e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 8200  |\n",
            "|    time_elapsed    | 353   |\n",
            "|    total_timesteps | 41000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 8300     |\n",
            "|    time_elapsed       | 354      |\n",
            "|    total_timesteps    | 41500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.67    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8299     |\n",
            "|    policy_loss        | 2.13e+04 |\n",
            "|    std                | 0.92     |\n",
            "|    value_loss         | 1.35e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 42000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.66    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8399     |\n",
            "|    policy_loss        | 5.04e+04 |\n",
            "|    std                | 0.916    |\n",
            "|    value_loss         | 5.34e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 8400  |\n",
            "|    time_elapsed    | 361   |\n",
            "|    total_timesteps | 42000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 8500      |\n",
            "|    time_elapsed       | 362       |\n",
            "|    total_timesteps    | 42500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.65     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8499      |\n",
            "|    policy_loss        | 2.47e+04  |\n",
            "|    std                | 0.913     |\n",
            "|    value_loss         | 8.85e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=43000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 43000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.64    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8599     |\n",
            "|    policy_loss        | 3.98e+04 |\n",
            "|    std                | 0.907    |\n",
            "|    value_loss         | 3.59e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 8600  |\n",
            "|    time_elapsed    | 370   |\n",
            "|    total_timesteps | 43000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 8700     |\n",
            "|    time_elapsed       | 371      |\n",
            "|    total_timesteps    | 43500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.64    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8699     |\n",
            "|    policy_loss        | 1.77e+04 |\n",
            "|    std                | 0.909    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.64     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | 4.35e+04  |\n",
            "|    std                | 0.906     |\n",
            "|    value_loss         | 3.81e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 8800  |\n",
            "|    time_elapsed    | 378   |\n",
            "|    total_timesteps | 44000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 8900     |\n",
            "|    time_elapsed       | 379      |\n",
            "|    total_timesteps    | 44500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.62    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8899     |\n",
            "|    policy_loss        | 2.05e+04 |\n",
            "|    std                | 0.896    |\n",
            "|    value_loss         | 9.24e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 45000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.63     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8999      |\n",
            "|    policy_loss        | 4.02e+04  |\n",
            "|    std                | 0.901     |\n",
            "|    value_loss         | 2.71e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 9000  |\n",
            "|    time_elapsed    | 387   |\n",
            "|    total_timesteps | 45000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 9100     |\n",
            "|    time_elapsed       | 388      |\n",
            "|    total_timesteps    | 45500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.62    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9099     |\n",
            "|    policy_loss        | 1.32e+04 |\n",
            "|    std                | 0.896    |\n",
            "|    value_loss         | 6.01e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 46000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.61    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9199     |\n",
            "|    policy_loss        | 4.65e+04 |\n",
            "|    std                | 0.896    |\n",
            "|    value_loss         | 3.95e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 9200  |\n",
            "|    time_elapsed    | 395   |\n",
            "|    total_timesteps | 46000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 396       |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.61     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | 3.79e+04  |\n",
            "|    std                | 0.893     |\n",
            "|    value_loss         | 1.49e+08  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=47000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 47000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.61    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9399     |\n",
            "|    policy_loss        | 5.24e+04 |\n",
            "|    std                | 0.893    |\n",
            "|    value_loss         | 4.99e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 9400  |\n",
            "|    time_elapsed    | 403   |\n",
            "|    total_timesteps | 47000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 9500     |\n",
            "|    time_elapsed       | 405      |\n",
            "|    total_timesteps    | 47500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.58    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9499     |\n",
            "|    policy_loss        | 3.53e+04 |\n",
            "|    std                | 0.884    |\n",
            "|    value_loss         | 8.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.58     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | 5.54e+04  |\n",
            "|    std                | 0.882     |\n",
            "|    value_loss         | 4.75e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 9600  |\n",
            "|    time_elapsed    | 413   |\n",
            "|    total_timesteps | 48000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 9700     |\n",
            "|    time_elapsed       | 414      |\n",
            "|    total_timesteps    | 48500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.59    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9699     |\n",
            "|    policy_loss        | 8.94e+03 |\n",
            "|    std                | 0.885    |\n",
            "|    value_loss         | 2.08e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=49000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.59     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | 2.59e+04  |\n",
            "|    std                | 0.887     |\n",
            "|    value_loss         | 1.06e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 9800  |\n",
            "|    time_elapsed    | 421   |\n",
            "|    total_timesteps | 49000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 9900     |\n",
            "|    time_elapsed       | 422      |\n",
            "|    total_timesteps    | 49500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.59    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9899     |\n",
            "|    policy_loss        | 3.15e+04 |\n",
            "|    std                | 0.884    |\n",
            "|    value_loss         | 9.53e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 50000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.56    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9999     |\n",
            "|    policy_loss        | 4.02e+04 |\n",
            "|    std                | 0.872    |\n",
            "|    value_loss         | 4.28e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 10000 |\n",
            "|    time_elapsed    | 430   |\n",
            "|    total_timesteps | 50000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 10100     |\n",
            "|    time_elapsed       | 431       |\n",
            "|    total_timesteps    | 50500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.56     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10099     |\n",
            "|    policy_loss        | 2.74e+04  |\n",
            "|    std                | 0.87      |\n",
            "|    value_loss         | 1.61e+08  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=51000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 51000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.55    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10199    |\n",
            "|    policy_loss        | 2.95e+04 |\n",
            "|    std                | 0.868    |\n",
            "|    value_loss         | 2.88e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 10200 |\n",
            "|    time_elapsed    | 438   |\n",
            "|    total_timesteps | 51000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 10300     |\n",
            "|    time_elapsed       | 440       |\n",
            "|    total_timesteps    | 51500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.55     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10299     |\n",
            "|    policy_loss        | 1.83e+04  |\n",
            "|    std                | 0.867     |\n",
            "|    value_loss         | 5.24e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 52000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.56     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10399     |\n",
            "|    policy_loss        | 6.62e+04  |\n",
            "|    std                | 0.872     |\n",
            "|    value_loss         | 9.91e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 10400 |\n",
            "|    time_elapsed    | 447   |\n",
            "|    total_timesteps | 52000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 10500    |\n",
            "|    time_elapsed       | 448      |\n",
            "|    total_timesteps    | 52500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.55    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10499    |\n",
            "|    policy_loss        | 2.12e+04 |\n",
            "|    std                | 0.867    |\n",
            "|    value_loss         | 7.11e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=53000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 53000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.53    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10599    |\n",
            "|    policy_loss        | 4.76e+04 |\n",
            "|    std                | 0.858    |\n",
            "|    value_loss         | 5.07e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 10600 |\n",
            "|    time_elapsed    | 455   |\n",
            "|    total_timesteps | 53000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 10700    |\n",
            "|    time_elapsed       | 457      |\n",
            "|    total_timesteps    | 53500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.52    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10699    |\n",
            "|    policy_loss        | 2.86e+04 |\n",
            "|    std                | 0.855    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 54000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.51    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10799    |\n",
            "|    policy_loss        | 3.37e+04 |\n",
            "|    std                | 0.851    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 10800 |\n",
            "|    time_elapsed    | 464   |\n",
            "|    total_timesteps | 54000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 10900    |\n",
            "|    time_elapsed       | 465      |\n",
            "|    total_timesteps    | 54500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.52    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10899    |\n",
            "|    policy_loss        | 1.93e+04 |\n",
            "|    std                | 0.855    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=55000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 55000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.53    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10999    |\n",
            "|    policy_loss        | 3.99e+04 |\n",
            "|    std                | 0.857    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 11000 |\n",
            "|    time_elapsed    | 472   |\n",
            "|    total_timesteps | 55000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 11100    |\n",
            "|    time_elapsed       | 474      |\n",
            "|    total_timesteps    | 55500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.52    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11099    |\n",
            "|    policy_loss        | 1.53e+04 |\n",
            "|    std                | 0.855    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 56000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.51    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11199    |\n",
            "|    policy_loss        | 3.78e+04 |\n",
            "|    std                | 0.849    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 11200 |\n",
            "|    time_elapsed    | 481   |\n",
            "|    total_timesteps | 56000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 11300    |\n",
            "|    time_elapsed       | 482      |\n",
            "|    total_timesteps    | 56500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.51    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11299    |\n",
            "|    policy_loss        | 2.28e+04 |\n",
            "|    std                | 0.85     |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=57000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 57000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.49     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11399     |\n",
            "|    policy_loss        | 4.24e+04  |\n",
            "|    std                | 0.841     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 11400 |\n",
            "|    time_elapsed    | 489   |\n",
            "|    total_timesteps | 57000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 11500    |\n",
            "|    time_elapsed       | 491      |\n",
            "|    total_timesteps    | 57500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.47    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11499    |\n",
            "|    policy_loss        | 1.67e+04 |\n",
            "|    std                | 0.832    |\n",
            "|    value_loss         | 9.32e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=58000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 58000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.47     |\n",
            "|    explained_variance | -3.58e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11599     |\n",
            "|    policy_loss        | 4.09e+04  |\n",
            "|    std                | 0.832     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 11600 |\n",
            "|    time_elapsed    | 498   |\n",
            "|    total_timesteps | 58000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 11700    |\n",
            "|    time_elapsed       | 499      |\n",
            "|    total_timesteps    | 58500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.47    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11699    |\n",
            "|    policy_loss        | 2.16e+04 |\n",
            "|    std                | 0.832    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=59000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 59000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.46     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11799     |\n",
            "|    policy_loss        | 4.24e+04  |\n",
            "|    std                | 0.827     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 11800 |\n",
            "|    time_elapsed    | 506   |\n",
            "|    total_timesteps | 59000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 11900     |\n",
            "|    time_elapsed       | 508       |\n",
            "|    total_timesteps    | 59500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.45     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11899     |\n",
            "|    policy_loss        | 2.02e+04  |\n",
            "|    std                | 0.823     |\n",
            "|    value_loss         | 9.31e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 60000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.43    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11999    |\n",
            "|    policy_loss        | 4.81e+04 |\n",
            "|    std                | 0.815    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 12000 |\n",
            "|    time_elapsed    | 515   |\n",
            "|    total_timesteps | 60000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 12100    |\n",
            "|    time_elapsed       | 516      |\n",
            "|    total_timesteps    | 60500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.42    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12099    |\n",
            "|    policy_loss        | 2.56e+04 |\n",
            "|    std                | 0.812    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=61000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 61000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.39     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12199     |\n",
            "|    policy_loss        | 4.81e+04  |\n",
            "|    std                | 0.802     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 12200 |\n",
            "|    time_elapsed    | 523   |\n",
            "|    total_timesteps | 61000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 12300     |\n",
            "|    time_elapsed       | 525       |\n",
            "|    total_timesteps    | 61500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.39     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12299     |\n",
            "|    policy_loss        | 2.9e+04   |\n",
            "|    std                | 0.8       |\n",
            "|    value_loss         | 9.31e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=62000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 62000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.37    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12399    |\n",
            "|    policy_loss        | 4.04e+04 |\n",
            "|    std                | 0.793    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 12400 |\n",
            "|    time_elapsed    | 532   |\n",
            "|    total_timesteps | 62000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 12500    |\n",
            "|    time_elapsed       | 533      |\n",
            "|    total_timesteps    | 62500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.36    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12499    |\n",
            "|    policy_loss        | 2.14e+04 |\n",
            "|    std                | 0.788    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=63000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 63000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.35    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12599    |\n",
            "|    policy_loss        | 3.7e+04  |\n",
            "|    std                | 0.783    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 12600 |\n",
            "|    time_elapsed    | 540   |\n",
            "|    total_timesteps | 63000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 12700    |\n",
            "|    time_elapsed       | 542      |\n",
            "|    total_timesteps    | 63500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.33    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12699    |\n",
            "|    policy_loss        | 1.92e+04 |\n",
            "|    std                | 0.776    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 64000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.32    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12799    |\n",
            "|    policy_loss        | 3.54e+04 |\n",
            "|    std                | 0.773    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 12800 |\n",
            "|    time_elapsed    | 549   |\n",
            "|    total_timesteps | 64000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 12900    |\n",
            "|    time_elapsed       | 550      |\n",
            "|    total_timesteps    | 64500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.3     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 12899    |\n",
            "|    policy_loss        | 1.72e+04 |\n",
            "|    std                | 0.765    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=65000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 65000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.29     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 12999     |\n",
            "|    policy_loss        | 3.53e+04  |\n",
            "|    std                | 0.762     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 13000 |\n",
            "|    time_elapsed    | 557   |\n",
            "|    total_timesteps | 65000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 13100    |\n",
            "|    time_elapsed       | 559      |\n",
            "|    total_timesteps    | 65500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.28    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13099    |\n",
            "|    policy_loss        | 2.12e+04 |\n",
            "|    std                | 0.758    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=66000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 66000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.27    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13199    |\n",
            "|    policy_loss        | 5.16e+04 |\n",
            "|    std                | 0.753    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 13200 |\n",
            "|    time_elapsed    | 566   |\n",
            "|    total_timesteps | 66000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 13300    |\n",
            "|    time_elapsed       | 567      |\n",
            "|    total_timesteps    | 66500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.28    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13299    |\n",
            "|    policy_loss        | 2.27e+04 |\n",
            "|    std                | 0.758    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=67000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 67000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.26    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13399    |\n",
            "|    policy_loss        | 3.97e+04 |\n",
            "|    std                | 0.751    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 13400 |\n",
            "|    time_elapsed    | 574   |\n",
            "|    total_timesteps | 67000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 13500    |\n",
            "|    time_elapsed       | 576      |\n",
            "|    total_timesteps    | 67500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.25    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13499    |\n",
            "|    policy_loss        | 2.02e+04 |\n",
            "|    std                | 0.745    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=68000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 68000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.25    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13599    |\n",
            "|    policy_loss        | 4.89e+04 |\n",
            "|    std                | 0.746    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 13600 |\n",
            "|    time_elapsed    | 583   |\n",
            "|    total_timesteps | 68000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 13700    |\n",
            "|    time_elapsed       | 584      |\n",
            "|    total_timesteps    | 68500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.26    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13699    |\n",
            "|    policy_loss        | 2.58e+04 |\n",
            "|    std                | 0.75     |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=69000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 69000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.25    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13799    |\n",
            "|    policy_loss        | 2.99e+04 |\n",
            "|    std                | 0.747    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 13800 |\n",
            "|    time_elapsed    | 591   |\n",
            "|    total_timesteps | 69000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 13900    |\n",
            "|    time_elapsed       | 593      |\n",
            "|    total_timesteps    | 69500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.25    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13899    |\n",
            "|    policy_loss        | 1.36e+04 |\n",
            "|    std                | 0.744    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 70000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.23    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 13999    |\n",
            "|    policy_loss        | 5e+04    |\n",
            "|    std                | 0.738    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 14000 |\n",
            "|    time_elapsed    | 600   |\n",
            "|    total_timesteps | 70000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 14100    |\n",
            "|    time_elapsed       | 601      |\n",
            "|    total_timesteps    | 70500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.23    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14099    |\n",
            "|    policy_loss        | 2.2e+04  |\n",
            "|    std                | 0.739    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=71000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 71000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.21    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14199    |\n",
            "|    policy_loss        | 3.56e+04 |\n",
            "|    std                | 0.732    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 14200 |\n",
            "|    time_elapsed    | 608   |\n",
            "|    total_timesteps | 71000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 14300    |\n",
            "|    time_elapsed       | 610      |\n",
            "|    total_timesteps    | 71500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.2     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14299    |\n",
            "|    policy_loss        | 2.27e+04 |\n",
            "|    std                | 0.728    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 72000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.18    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14399    |\n",
            "|    policy_loss        | 3.23e+04 |\n",
            "|    std                | 0.72     |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 14400 |\n",
            "|    time_elapsed    | 617   |\n",
            "|    total_timesteps | 72000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 14500    |\n",
            "|    time_elapsed       | 618      |\n",
            "|    total_timesteps    | 72500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.15    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14499    |\n",
            "|    policy_loss        | 1.88e+04 |\n",
            "|    std                | 0.711    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=73000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 73000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.16    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14599    |\n",
            "|    policy_loss        | 3.37e+04 |\n",
            "|    std                | 0.712    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 14600 |\n",
            "|    time_elapsed    | 626   |\n",
            "|    total_timesteps | 73000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 14700    |\n",
            "|    time_elapsed       | 627      |\n",
            "|    total_timesteps    | 73500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.13    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14699    |\n",
            "|    policy_loss        | 1.91e+04 |\n",
            "|    std                | 0.704    |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=74000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 74000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.12    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14799    |\n",
            "|    policy_loss        | 3.46e+04 |\n",
            "|    std                | 0.701    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 14800 |\n",
            "|    time_elapsed    | 634   |\n",
            "|    total_timesteps | 74000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 14900    |\n",
            "|    time_elapsed       | 635      |\n",
            "|    total_timesteps    | 74500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.12    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14899    |\n",
            "|    policy_loss        | 2.07e+04 |\n",
            "|    std                | 0.7      |\n",
            "|    value_loss         | 9.31e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=75000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 75000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.13    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 14999    |\n",
            "|    policy_loss        | 2.09e+04 |\n",
            "|    std                | 0.704    |\n",
            "|    value_loss         | 2.41e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 15000 |\n",
            "|    time_elapsed    | 643   |\n",
            "|    total_timesteps | 75000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 15100    |\n",
            "|    time_elapsed       | 644      |\n",
            "|    total_timesteps    | 75500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.13    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 15099    |\n",
            "|    policy_loss        | 1.87e+04 |\n",
            "|    std                | 0.702    |\n",
            "|    value_loss         | 9.3e+07  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=76000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 76000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.1     |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 15199    |\n",
            "|    policy_loss        | 3.52e+04 |\n",
            "|    std                | 0.693    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 15200 |\n",
            "|    time_elapsed    | 651   |\n",
            "|    total_timesteps | 76000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 15300     |\n",
            "|    time_elapsed       | 652       |\n",
            "|    total_timesteps    | 76500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.1      |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15299     |\n",
            "|    policy_loss        | 9.36e+03  |\n",
            "|    std                | 0.691     |\n",
            "|    value_loss         | 2.59e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=77000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 77000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.08     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15399     |\n",
            "|    policy_loss        | 4.93e+04  |\n",
            "|    std                | 0.687     |\n",
            "|    value_loss         | 3.97e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 15400 |\n",
            "|    time_elapsed    | 660   |\n",
            "|    total_timesteps | 77000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 15500    |\n",
            "|    time_elapsed       | 661      |\n",
            "|    total_timesteps    | 77500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.08    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 15499    |\n",
            "|    policy_loss        | 3.09e+04 |\n",
            "|    std                | 0.685    |\n",
            "|    value_loss         | 3.14e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=78000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 78000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.09    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 15599    |\n",
            "|    policy_loss        | 2.4e+04  |\n",
            "|    std                | 0.687    |\n",
            "|    value_loss         | 2.74e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 15600 |\n",
            "|    time_elapsed    | 668   |\n",
            "|    total_timesteps | 78000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 15700    |\n",
            "|    time_elapsed       | 670      |\n",
            "|    total_timesteps    | 78500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.08    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 15699    |\n",
            "|    policy_loss        | 3.22e+03 |\n",
            "|    std                | 0.686    |\n",
            "|    value_loss         | 4.16e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=79000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 79000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.07     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15799     |\n",
            "|    policy_loss        | 1.35e+04  |\n",
            "|    std                | 0.683     |\n",
            "|    value_loss         | 4.45e+07  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 15800 |\n",
            "|    time_elapsed    | 677   |\n",
            "|    total_timesteps | 79000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 15900     |\n",
            "|    time_elapsed       | 678       |\n",
            "|    total_timesteps    | 79500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.06     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 15899     |\n",
            "|    policy_loss        | 3.75e+03  |\n",
            "|    std                | 0.679     |\n",
            "|    value_loss         | 2.85e+06  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 80000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.07    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 15999    |\n",
            "|    policy_loss        | 9.02e+03 |\n",
            "|    std                | 0.681    |\n",
            "|    value_loss         | 1.67e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 16000 |\n",
            "|    time_elapsed    | 686   |\n",
            "|    total_timesteps | 80000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 16100    |\n",
            "|    time_elapsed       | 687      |\n",
            "|    total_timesteps    | 80500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.07    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16099    |\n",
            "|    policy_loss        | 2.34e+03 |\n",
            "|    std                | 0.68     |\n",
            "|    value_loss         | 2.59e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=81000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 81000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.06    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16199    |\n",
            "|    policy_loss        | 5.63e+03 |\n",
            "|    std                | 0.679    |\n",
            "|    value_loss         | 1.87e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 16200 |\n",
            "|    time_elapsed    | 694   |\n",
            "|    total_timesteps | 81000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 16300    |\n",
            "|    time_elapsed       | 695      |\n",
            "|    total_timesteps    | 81500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.06    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16299    |\n",
            "|    policy_loss        | 3.12e+04 |\n",
            "|    std                | 0.68     |\n",
            "|    value_loss         | 1.5e+08  |\n",
            "------------------------------------\n",
            "Eval num_timesteps=82000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 82000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.06    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16399    |\n",
            "|    policy_loss        | 2.35e+04 |\n",
            "|    std                | 0.678    |\n",
            "|    value_loss         | 1.47e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 16400 |\n",
            "|    time_elapsed    | 703   |\n",
            "|    total_timesteps | 82000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 16500    |\n",
            "|    time_elapsed       | 704      |\n",
            "|    total_timesteps    | 82500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.04    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16499    |\n",
            "|    policy_loss        | 1.26e+04 |\n",
            "|    std                | 0.673    |\n",
            "|    value_loss         | 7.81e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=83000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 83000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.02    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16599    |\n",
            "|    policy_loss        | 9.03e+03 |\n",
            "|    std                | 0.667    |\n",
            "|    value_loss         | 4.14e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 16600 |\n",
            "|    time_elapsed    | 711   |\n",
            "|    total_timesteps | 83000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 16700    |\n",
            "|    time_elapsed       | 713      |\n",
            "|    total_timesteps    | 83500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.03    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16699    |\n",
            "|    policy_loss        | 5.02e+03 |\n",
            "|    std                | 0.669    |\n",
            "|    value_loss         | 6.76e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=84000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 84000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.04    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16799    |\n",
            "|    policy_loss        | 7.81e+04 |\n",
            "|    std                | 0.675    |\n",
            "|    value_loss         | 1.24e+09 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 16800 |\n",
            "|    time_elapsed    | 720   |\n",
            "|    total_timesteps | 84000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 16900    |\n",
            "|    time_elapsed       | 721      |\n",
            "|    total_timesteps    | 84500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.03    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 16899    |\n",
            "|    policy_loss        | 2.03e+04 |\n",
            "|    std                | 0.669    |\n",
            "|    value_loss         | 1.53e+08 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=85000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 85000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.03     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 16999     |\n",
            "|    policy_loss        | 1.65e+04  |\n",
            "|    std                | 0.668     |\n",
            "|    value_loss         | 1.29e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 17000 |\n",
            "|    time_elapsed    | 728   |\n",
            "|    total_timesteps | 85000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 17100    |\n",
            "|    time_elapsed       | 730      |\n",
            "|    total_timesteps    | 85500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.04    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17099    |\n",
            "|    policy_loss        | 4.85e+03 |\n",
            "|    std                | 0.674    |\n",
            "|    value_loss         | 3.75e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=86000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 86000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.05    |\n",
            "|    explained_variance | 1.79e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17199    |\n",
            "|    policy_loss        | 4.85e+04 |\n",
            "|    std                | 0.677    |\n",
            "|    value_loss         | 5.63e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 17200 |\n",
            "|    time_elapsed    | 737   |\n",
            "|    total_timesteps | 86000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 17300     |\n",
            "|    time_elapsed       | 738       |\n",
            "|    total_timesteps    | 86500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.07     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 17299     |\n",
            "|    policy_loss        | 9.33e+03  |\n",
            "|    std                | 0.684     |\n",
            "|    value_loss         | 2.46e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=87000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 87000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.07     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 17399     |\n",
            "|    policy_loss        | 7.24e+04  |\n",
            "|    std                | 0.684     |\n",
            "|    value_loss         | 6.67e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 17400 |\n",
            "|    time_elapsed    | 746   |\n",
            "|    total_timesteps | 87000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 17500    |\n",
            "|    time_elapsed       | 747      |\n",
            "|    total_timesteps    | 87500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.06    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17499    |\n",
            "|    policy_loss        | 5.91e+03 |\n",
            "|    std                | 0.679    |\n",
            "|    value_loss         | 1.12e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 88000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.07    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17599    |\n",
            "|    policy_loss        | 3.52e+04 |\n",
            "|    std                | 0.682    |\n",
            "|    value_loss         | 3.95e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 17600 |\n",
            "|    time_elapsed    | 754   |\n",
            "|    total_timesteps | 88000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 17700    |\n",
            "|    time_elapsed       | 756      |\n",
            "|    total_timesteps    | 88500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.06    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17699    |\n",
            "|    policy_loss        | 3.12e+03 |\n",
            "|    std                | 0.681    |\n",
            "|    value_loss         | 4.39e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=89000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 89000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.07     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 17799     |\n",
            "|    policy_loss        | 1.2e+04   |\n",
            "|    std                | 0.684     |\n",
            "|    value_loss         | 5.2e+07   |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 17800 |\n",
            "|    time_elapsed    | 763   |\n",
            "|    total_timesteps | 89000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 17900    |\n",
            "|    time_elapsed       | 764      |\n",
            "|    total_timesteps    | 89500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.06    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17899    |\n",
            "|    policy_loss        | 1.98e+03 |\n",
            "|    std                | 0.681    |\n",
            "|    value_loss         | 2.75e+06 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 90000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.05    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 17999    |\n",
            "|    policy_loss        | 6.49e+03 |\n",
            "|    std                | 0.678    |\n",
            "|    value_loss         | 1.59e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 18000 |\n",
            "|    time_elapsed    | 772   |\n",
            "|    total_timesteps | 90000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 18100     |\n",
            "|    time_elapsed       | 773       |\n",
            "|    total_timesteps    | 90500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.05     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 18099     |\n",
            "|    policy_loss        | 2.57e+04  |\n",
            "|    std                | 0.675     |\n",
            "|    value_loss         | 7.74e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=91000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 91000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.03    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18199    |\n",
            "|    policy_loss        | 7.11e+03 |\n",
            "|    std                | 0.671    |\n",
            "|    value_loss         | 1.82e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 18200 |\n",
            "|    time_elapsed    | 780   |\n",
            "|    total_timesteps | 91000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 18300    |\n",
            "|    time_elapsed       | 781      |\n",
            "|    total_timesteps    | 91500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.02    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18299    |\n",
            "|    policy_loss        | 1.39e+04 |\n",
            "|    std                | 0.667    |\n",
            "|    value_loss         | 4.96e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=92000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 92000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.02    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18399    |\n",
            "|    policy_loss        | 7.18e+03 |\n",
            "|    std                | 0.666    |\n",
            "|    value_loss         | 1.11e+07 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 18400 |\n",
            "|    time_elapsed    | 789   |\n",
            "|    total_timesteps | 92000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 18500     |\n",
            "|    time_elapsed       | 790       |\n",
            "|    total_timesteps    | 92500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.02     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 18499     |\n",
            "|    policy_loss        | 1.78e+04  |\n",
            "|    std                | 0.664     |\n",
            "|    value_loss         | 9.13e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=93000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 93000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.01    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18599    |\n",
            "|    policy_loss        | 2.33e+04 |\n",
            "|    std                | 0.663    |\n",
            "|    value_loss         | 2.62e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 18600 |\n",
            "|    time_elapsed    | 797   |\n",
            "|    total_timesteps | 93000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 18700    |\n",
            "|    time_elapsed       | 799      |\n",
            "|    total_timesteps    | 93500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.99    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18699    |\n",
            "|    policy_loss        | 1.68e+04 |\n",
            "|    std                | 0.655    |\n",
            "|    value_loss         | 9.29e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=94000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 94000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.97    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18799    |\n",
            "|    policy_loss        | 3.31e+04 |\n",
            "|    std                | 0.648    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 18800 |\n",
            "|    time_elapsed    | 806   |\n",
            "|    total_timesteps | 94000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 18900    |\n",
            "|    time_elapsed       | 807      |\n",
            "|    total_timesteps    | 94500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.97    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18899    |\n",
            "|    policy_loss        | 1.52e+04 |\n",
            "|    std                | 0.649    |\n",
            "|    value_loss         | 9.29e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=95000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 95000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.96    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 18999    |\n",
            "|    policy_loss        | 3.3e+04  |\n",
            "|    std                | 0.647    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 19000 |\n",
            "|    time_elapsed    | 814   |\n",
            "|    total_timesteps | 95000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 19100     |\n",
            "|    time_elapsed       | 816       |\n",
            "|    total_timesteps    | 95500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.96     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 19099     |\n",
            "|    policy_loss        | 2.37e+04  |\n",
            "|    std                | 0.646     |\n",
            "|    value_loss         | 9.29e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 1e+03     |\n",
            "|    mean_reward        | 3e+06     |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 96000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.95     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 19199     |\n",
            "|    policy_loss        | 3.53e+04  |\n",
            "|    std                | 0.643     |\n",
            "|    value_loss         | 3.78e+08  |\n",
            "-------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 19200 |\n",
            "|    time_elapsed    | 823   |\n",
            "|    total_timesteps | 96000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 19300    |\n",
            "|    time_elapsed       | 824      |\n",
            "|    total_timesteps    | 96500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.93    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19299    |\n",
            "|    policy_loss        | 9.61e+03 |\n",
            "|    std                | 0.635    |\n",
            "|    value_loss         | 9.29e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=97000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 97000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.92    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19399    |\n",
            "|    policy_loss        | 3.05e+04 |\n",
            "|    std                | 0.634    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 19400 |\n",
            "|    time_elapsed    | 831   |\n",
            "|    total_timesteps | 97000 |\n",
            "------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 19500     |\n",
            "|    time_elapsed       | 833       |\n",
            "|    total_timesteps    | 97500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.92     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 19499     |\n",
            "|    policy_loss        | 1.58e+04  |\n",
            "|    std                | 0.633     |\n",
            "|    value_loss         | 9.29e+07  |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=98000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 98000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.91    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19599    |\n",
            "|    policy_loss        | 3.19e+04 |\n",
            "|    std                | 0.631    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 19600 |\n",
            "|    time_elapsed    | 840   |\n",
            "|    total_timesteps | 98000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 19700    |\n",
            "|    time_elapsed       | 841      |\n",
            "|    total_timesteps    | 98500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.92    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19699    |\n",
            "|    policy_loss        | 1.91e+04 |\n",
            "|    std                | 0.634    |\n",
            "|    value_loss         | 9.29e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=99000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 99000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.9     |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19799    |\n",
            "|    policy_loss        | 2.58e+04 |\n",
            "|    std                | 0.627    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 116   |\n",
            "|    iterations      | 19800 |\n",
            "|    time_elapsed    | 849   |\n",
            "|    total_timesteps | 99000 |\n",
            "------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 19900    |\n",
            "|    time_elapsed       | 850      |\n",
            "|    total_timesteps    | 99500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.91    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19899    |\n",
            "|    policy_loss        | 1.39e+04 |\n",
            "|    std                | 0.628    |\n",
            "|    value_loss         | 9.29e+07 |\n",
            "------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 1e+03    |\n",
            "|    mean_reward        | 3e+06    |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 100000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.89    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 19999    |\n",
            "|    policy_loss        | 4.64e+04 |\n",
            "|    std                | 0.623    |\n",
            "|    value_loss         | 3.78e+08 |\n",
            "------------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    fps             | 116    |\n",
            "|    iterations      | 20000  |\n",
            "|    time_elapsed    | 857    |\n",
            "|    total_timesteps | 100000 |\n",
            "-------------------------------\n",
            "mean_reward:3003000.00 +/- 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x576 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mean_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/fps</td><td>█▁██▁▁▁▁█▁▁▁▁▁▁█████████████████████████</td></tr><tr><td>train/entropy_loss</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train/explained_variance</td><td>▁▆▆█▆█▆▆▆▆▆▇▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▆▆▆▆</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/policy_loss</td><td>▁▁▁▁▃▁▁▁▂▁▇▁█▅█▃▃▃▃▂▂▂▃▃▃▃▃▂▂▃▃▁▂▅▅▁▁▂▂▃</td></tr><tr><td>train/std</td><td>██████▇▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/value_loss</td><td>▁▁▁▁▂▁▁▁▁▁█▁▄▅▇▃▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁▁▅▃▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>1000.0</td></tr><tr><td>eval/mean_reward</td><td>3003000.0</td></tr><tr><td>global_step</td><td>100000</td></tr><tr><td>time/fps</td><td>116.0</td></tr><tr><td>train/entropy_loss</td><td>-1.88613</td></tr><tr><td>train/explained_variance</td><td>0.0</td></tr><tr><td>train/learning_rate</td><td>0.0007</td></tr><tr><td>train/policy_loss</td><td>46388.76172</td></tr><tr><td>train/std</td><td>0.62263</td></tr><tr><td>train/value_loss</td><td>377772448.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">swept-elevator-8</strong>: <a href=\"https://wandb.ai/nishamdev/StockTrading/runs/1wvgfvb0\" target=\"_blank\">https://wandb.ai/nishamdev/StockTrading/runs/1wvgfvb0</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 3 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221124_221107-1wvgfvb0/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9NnLQ0sijs_"
      },
      "source": [
        "# C - SAC Algorithm - Soft Actor Critic (SAC) Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\n",
        "\n",
        "Soft Actor Critic, or SAC, is an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment#5 - Running SAC algorithm over 100 time steps\n",
        "\n",
        "Experiment SAC algo with with default params - gamma=0.99 , learning_rate=0.0007 , ent_coef=0.4"
      ],
      "metadata": {
        "id": "zeBkTbwsji2T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v0M3awN-ijs_",
        "outputId": "81f5e339-2d37-4fc6-c328-2bee1d83d233"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Reinforcement-learning-Live-Trading/wandb/run-20221124_222600-3rxikqj0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/nishamdev/StockTrading/runs/3rxikqj0\" target=\"_blank\">colorful-haze-9</a></strong> to <a href=\"https://wandb.ai/nishamdev/StockTrading\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
            "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to runs/3rxikqj0/SAC_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=1000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1000      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.16e+04 |\n",
            "|    critic_loss     | 4.1e+06   |\n",
            "|    ent_coef        | 1.26      |\n",
            "|    ent_coef_loss   | -2.76     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 899       |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2000      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.77e+04 |\n",
            "|    critic_loss     | 2.95e+06  |\n",
            "|    ent_coef        | 1.74      |\n",
            "|    ent_coef_loss   | -5.8      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 1899      |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3000      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.21e+04 |\n",
            "|    critic_loss     | 2.95e+06  |\n",
            "|    ent_coef        | 2.33      |\n",
            "|    ent_coef_loss   | -8.46     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 2899      |\n",
            "----------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 4000      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.52e+04 |\n",
            "|    critic_loss     | 2.98e+06  |\n",
            "|    ent_coef        | 3.11      |\n",
            "|    ent_coef_loss   | -11       |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 3899      |\n",
            "----------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 4    |\n",
            "|    fps             | 50   |\n",
            "|    time_elapsed    | 79   |\n",
            "|    total_timesteps | 4000 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=5000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 5000      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.76e+04 |\n",
            "|    critic_loss     | 3.1e+06   |\n",
            "|    ent_coef        | 4.16      |\n",
            "|    ent_coef_loss   | -13.3     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 4899      |\n",
            "----------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 6000      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.87e+04 |\n",
            "|    critic_loss     | 2.7e+06   |\n",
            "|    ent_coef        | 5.55      |\n",
            "|    ent_coef_loss   | -15       |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 5899      |\n",
            "----------------------------------\n",
            "Eval num_timesteps=7000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 7000      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.98e+04 |\n",
            "|    critic_loss     | 2.61e+06  |\n",
            "|    ent_coef        | 7.34      |\n",
            "|    ent_coef_loss   | -16.1     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 6899      |\n",
            "----------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1e+05   |\n",
            "|    critic_loss     | 2.78e+06 |\n",
            "|    ent_coef        | 9.66     |\n",
            "|    ent_coef_loss   | -16.5    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 7899     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 8    |\n",
            "|    fps             | 49   |\n",
            "|    time_elapsed    | 161  |\n",
            "|    total_timesteps | 8000 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=9000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.1e+05 |\n",
            "|    critic_loss     | 5.08e+07 |\n",
            "|    ent_coef        | 12.6     |\n",
            "|    ent_coef_loss   | -15      |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 8899     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.17e+05 |\n",
            "|    critic_loss     | 2.4e+06   |\n",
            "|    ent_coef        | 16.1      |\n",
            "|    ent_coef_loss   | -12.9     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 9899      |\n",
            "----------------------------------\n",
            "Eval num_timesteps=11000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 11000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.25e+05 |\n",
            "|    critic_loss     | 1.26e+08  |\n",
            "|    ent_coef        | 20.2      |\n",
            "|    ent_coef_loss   | -10.2     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 10899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 12000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.33e+05 |\n",
            "|    critic_loss     | 2.17e+06  |\n",
            "|    ent_coef        | 23.5      |\n",
            "|    ent_coef_loss   | -1.72     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 11899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 12    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 243   |\n",
            "|    total_timesteps | 12000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=13000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 13000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.4e+05 |\n",
            "|    critic_loss     | 2.44e+06 |\n",
            "|    ent_coef        | 25       |\n",
            "|    ent_coef_loss   | -3.13    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 12899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 14000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.49e+05 |\n",
            "|    critic_loss     | 9.15e+07  |\n",
            "|    ent_coef        | 31.2      |\n",
            "|    ent_coef_loss   | -4.86     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 13899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 15000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.55e+05 |\n",
            "|    critic_loss     | 1.85e+06  |\n",
            "|    ent_coef        | 38.5      |\n",
            "|    ent_coef_loss   | -2.76     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 14899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 16000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.61e+05 |\n",
            "|    critic_loss     | 1.96e+06  |\n",
            "|    ent_coef        | 44.8      |\n",
            "|    ent_coef_loss   | -1.97     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 15899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 16    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 325   |\n",
            "|    total_timesteps | 16000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=17000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 17000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.66e+05 |\n",
            "|    critic_loss     | 2.31e+06  |\n",
            "|    ent_coef        | 51.8      |\n",
            "|    ent_coef_loss   | -0.702    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 16899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 18000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.75e+05 |\n",
            "|    critic_loss     | 2.62e+06  |\n",
            "|    ent_coef        | 52.8      |\n",
            "|    ent_coef_loss   | -0.987    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 17899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=19000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 19000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.79e+05 |\n",
            "|    critic_loss     | 1.8e+06   |\n",
            "|    ent_coef        | 48.2      |\n",
            "|    ent_coef_loss   | 1.11      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 18899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.86e+05 |\n",
            "|    critic_loss     | 2.01e+06  |\n",
            "|    ent_coef        | 37.2      |\n",
            "|    ent_coef_loss   | -0.577    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 19899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 20    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 406   |\n",
            "|    total_timesteps | 20000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=21000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 21000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.89e+05 |\n",
            "|    critic_loss     | 2.19e+06  |\n",
            "|    ent_coef        | 30.1      |\n",
            "|    ent_coef_loss   | 1.19      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 20899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 22000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.96e+05 |\n",
            "|    critic_loss     | 1.7e+08   |\n",
            "|    ent_coef        | 21.8      |\n",
            "|    ent_coef_loss   | 4.69      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 21899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=23000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 23000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.02e+05 |\n",
            "|    critic_loss     | 2.77e+06  |\n",
            "|    ent_coef        | 20.1      |\n",
            "|    ent_coef_loss   | -2.67     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 22899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 24000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.04e+05 |\n",
            "|    critic_loss     | 1.87e+08  |\n",
            "|    ent_coef        | 24.2      |\n",
            "|    ent_coef_loss   | 0.785     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 23899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 24    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 487   |\n",
            "|    total_timesteps | 24000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 25000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.09e+05 |\n",
            "|    critic_loss     | 3.87e+08  |\n",
            "|    ent_coef        | 22.7      |\n",
            "|    ent_coef_loss   | -0.822    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 24899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 26000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.15e+05 |\n",
            "|    critic_loss     | 2.04e+08  |\n",
            "|    ent_coef        | 20.2      |\n",
            "|    ent_coef_loss   | 4.66      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 25899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=27000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 27000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.18e+05 |\n",
            "|    critic_loss     | 2.54e+06  |\n",
            "|    ent_coef        | 18.2      |\n",
            "|    ent_coef_loss   | -0.465    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 26899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 28000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.17e+05 |\n",
            "|    critic_loss     | 2.49e+06  |\n",
            "|    ent_coef        | 22.3      |\n",
            "|    ent_coef_loss   | 0.66      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 27899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 28    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 569   |\n",
            "|    total_timesteps | 28000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=29000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 29000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.22e+05 |\n",
            "|    critic_loss     | 2.93e+06  |\n",
            "|    ent_coef        | 21.7      |\n",
            "|    ent_coef_loss   | 1.21      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 28899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.22e+05 |\n",
            "|    critic_loss     | 3.89e+06  |\n",
            "|    ent_coef        | 20.5      |\n",
            "|    ent_coef_loss   | -3.2      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 29899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=31000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 31000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.25e+05 |\n",
            "|    critic_loss     | 4.15e+06  |\n",
            "|    ent_coef        | 15.1      |\n",
            "|    ent_coef_loss   | 3.82      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 30899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 32000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.28e+05 |\n",
            "|    critic_loss     | 3.7e+06   |\n",
            "|    ent_coef        | 13.4      |\n",
            "|    ent_coef_loss   | -3.18     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 31899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 32    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 650   |\n",
            "|    total_timesteps | 32000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=33000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 33000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.34e+05 |\n",
            "|    critic_loss     | 4.09e+06  |\n",
            "|    ent_coef        | 14.6      |\n",
            "|    ent_coef_loss   | -3.91     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 32899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 34000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.34e+05 |\n",
            "|    critic_loss     | 4.26e+06  |\n",
            "|    ent_coef        | 15.8      |\n",
            "|    ent_coef_loss   | 4.61      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 33899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 35000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.41e+05 |\n",
            "|    critic_loss     | 7.04e+06  |\n",
            "|    ent_coef        | 17.2      |\n",
            "|    ent_coef_loss   | 5.1       |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 34899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 36000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.37e+05 |\n",
            "|    critic_loss     | 4.43e+06  |\n",
            "|    ent_coef        | 15.9      |\n",
            "|    ent_coef_loss   | 1.07      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 35899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 36    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 732   |\n",
            "|    total_timesteps | 36000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=37000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 37000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.42e+05 |\n",
            "|    critic_loss     | 5.66e+06  |\n",
            "|    ent_coef        | 17.6      |\n",
            "|    ent_coef_loss   | -2.21     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 36899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 38000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.43e+05 |\n",
            "|    critic_loss     | 4.99e+06  |\n",
            "|    ent_coef        | 23.1      |\n",
            "|    ent_coef_loss   | -3.3      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 37899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=39000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 39000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.45e+05 |\n",
            "|    critic_loss     | 4.53e+06  |\n",
            "|    ent_coef        | 23.1      |\n",
            "|    ent_coef_loss   | -0.659    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 38899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.51e+05 |\n",
            "|    critic_loss     | 5.4e+06   |\n",
            "|    ent_coef        | 20.8      |\n",
            "|    ent_coef_loss   | -3.05     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 39899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 40    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 813   |\n",
            "|    total_timesteps | 40000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=41000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 41000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.52e+05 |\n",
            "|    critic_loss     | 7.81e+06  |\n",
            "|    ent_coef        | 19.1      |\n",
            "|    ent_coef_loss   | 1.6       |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 40899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 42000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.49e+05 |\n",
            "|    critic_loss     | 1.04e+07  |\n",
            "|    ent_coef        | 19.2      |\n",
            "|    ent_coef_loss   | -4.53     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 41899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=43000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 43000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.61e+05 |\n",
            "|    critic_loss     | 7.62e+06  |\n",
            "|    ent_coef        | 23.5      |\n",
            "|    ent_coef_loss   | 1.71      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 42899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 44000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.57e+05 |\n",
            "|    critic_loss     | 5.51e+06  |\n",
            "|    ent_coef        | 18.1      |\n",
            "|    ent_coef_loss   | 5.41      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 43899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 44    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 895   |\n",
            "|    total_timesteps | 44000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 45000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.49e+05 |\n",
            "|    critic_loss     | 7.52e+06  |\n",
            "|    ent_coef        | 18.6      |\n",
            "|    ent_coef_loss   | -1.56     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 44899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 46000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.62e+05 |\n",
            "|    critic_loss     | 1.13e+07  |\n",
            "|    ent_coef        | 18.9      |\n",
            "|    ent_coef_loss   | -2.37     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 45899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=47000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 47000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.52e+05 |\n",
            "|    critic_loss     | 8.22e+06  |\n",
            "|    ent_coef        | 16.9      |\n",
            "|    ent_coef_loss   | 2.22      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 46899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 48000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.65e+05 |\n",
            "|    critic_loss     | 6.24e+06  |\n",
            "|    ent_coef        | 13.6      |\n",
            "|    ent_coef_loss   | -0.615    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 47899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 48    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 976   |\n",
            "|    total_timesteps | 48000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=49000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 49000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.59e+05 |\n",
            "|    critic_loss     | 9.15e+06  |\n",
            "|    ent_coef        | 12.8      |\n",
            "|    ent_coef_loss   | -1.83     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 48899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.62e+05 |\n",
            "|    critic_loss     | 8.72e+06  |\n",
            "|    ent_coef        | 13.9      |\n",
            "|    ent_coef_loss   | 3.86      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 49899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=51000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 51000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.64e+05 |\n",
            "|    critic_loss     | 9.79e+06  |\n",
            "|    ent_coef        | 12.4      |\n",
            "|    ent_coef_loss   | -8.17     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 50899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 52000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.69e+05 |\n",
            "|    critic_loss     | 8.24e+06  |\n",
            "|    ent_coef        | 13.6      |\n",
            "|    ent_coef_loss   | 0.836     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 51899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 52    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1057  |\n",
            "|    total_timesteps | 52000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=53000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 53000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.71e+05 |\n",
            "|    critic_loss     | 1.27e+07  |\n",
            "|    ent_coef        | 16.6      |\n",
            "|    ent_coef_loss   | -0.877    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 52899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 54000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.62e+05 |\n",
            "|    critic_loss     | 6.88e+06  |\n",
            "|    ent_coef        | 18.5      |\n",
            "|    ent_coef_loss   | -5.43     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 53899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=55000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 55000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.71e+05 |\n",
            "|    critic_loss     | 3.95e+08  |\n",
            "|    ent_coef        | 18.3      |\n",
            "|    ent_coef_loss   | 5.76      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 54899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 56000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.58e+05 |\n",
            "|    critic_loss     | 4.05e+08  |\n",
            "|    ent_coef        | 19.2      |\n",
            "|    ent_coef_loss   | -0.455    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 55899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 56    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1139  |\n",
            "|    total_timesteps | 56000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=57000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 57000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.57e+05 |\n",
            "|    critic_loss     | 1.1e+07   |\n",
            "|    ent_coef        | 19.4      |\n",
            "|    ent_coef_loss   | 1.83      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 56899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 58000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.68e+05 |\n",
            "|    critic_loss     | 1.06e+07  |\n",
            "|    ent_coef        | 17.6      |\n",
            "|    ent_coef_loss   | -1.46     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 57899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=59000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 59000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.64e+05 |\n",
            "|    critic_loss     | 4.11e+08  |\n",
            "|    ent_coef        | 15.6      |\n",
            "|    ent_coef_loss   | -6.21     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 58899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.73e+05 |\n",
            "|    critic_loss     | 1.02e+07  |\n",
            "|    ent_coef        | 17.3      |\n",
            "|    ent_coef_loss   | -2.03     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 59899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 60    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1220  |\n",
            "|    total_timesteps | 60000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=61000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 61000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -2.8e+05 |\n",
            "|    critic_loss     | 1.19e+07 |\n",
            "|    ent_coef        | 14.8     |\n",
            "|    ent_coef_loss   | 4.27     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 60899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=62000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 62000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.66e+05 |\n",
            "|    critic_loss     | 1.12e+07  |\n",
            "|    ent_coef        | 13.4      |\n",
            "|    ent_coef_loss   | -5.98     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 61899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=63000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 63000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.68e+05 |\n",
            "|    critic_loss     | 1.13e+07  |\n",
            "|    ent_coef        | 15.7      |\n",
            "|    ent_coef_loss   | -2.13     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 62899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 64000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.65e+05 |\n",
            "|    critic_loss     | 4.3e+08   |\n",
            "|    ent_coef        | 16        |\n",
            "|    ent_coef_loss   | 5.04      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 63899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 64    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1302  |\n",
            "|    total_timesteps | 64000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=65000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 65000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.73e+05 |\n",
            "|    critic_loss     | 1.65e+07  |\n",
            "|    ent_coef        | 14.6      |\n",
            "|    ent_coef_loss   | 1.09      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 64899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 66000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.71e+05 |\n",
            "|    critic_loss     | 1.09e+07  |\n",
            "|    ent_coef        | 14.1      |\n",
            "|    ent_coef_loss   | -2.84     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 65899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 67000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.75e+05 |\n",
            "|    critic_loss     | 1.15e+07  |\n",
            "|    ent_coef        | 15.9      |\n",
            "|    ent_coef_loss   | -4.35     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 66899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 68000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.71e+05 |\n",
            "|    critic_loss     | 1.25e+07  |\n",
            "|    ent_coef        | 21.5      |\n",
            "|    ent_coef_loss   | 1.23      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 67899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 68    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1383  |\n",
            "|    total_timesteps | 68000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=69000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 69000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.74e+05 |\n",
            "|    critic_loss     | 4.51e+08  |\n",
            "|    ent_coef        | 18.1      |\n",
            "|    ent_coef_loss   | 4.69      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 68899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.82e+05 |\n",
            "|    critic_loss     | 4.53e+08  |\n",
            "|    ent_coef        | 19.4      |\n",
            "|    ent_coef_loss   | 1.1       |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 69899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=71000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 71000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.74e+05 |\n",
            "|    critic_loss     | 1.2e+07   |\n",
            "|    ent_coef        | 20.4      |\n",
            "|    ent_coef_loss   | 4.28      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 70899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 72000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.67e+05 |\n",
            "|    critic_loss     | 1.27e+07  |\n",
            "|    ent_coef        | 24.7      |\n",
            "|    ent_coef_loss   | -0.302    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 71899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 72    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1465  |\n",
            "|    total_timesteps | 72000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=73000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 73000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.83e+05 |\n",
            "|    critic_loss     | 1.22e+07  |\n",
            "|    ent_coef        | 21.1      |\n",
            "|    ent_coef_loss   | 2.18      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 72899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=74000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 74000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.83e+05 |\n",
            "|    critic_loss     | 4.68e+08  |\n",
            "|    ent_coef        | 20.8      |\n",
            "|    ent_coef_loss   | -4.4      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 73899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 75000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.82e+05 |\n",
            "|    critic_loss     | 1.38e+07  |\n",
            "|    ent_coef        | 23.1      |\n",
            "|    ent_coef_loss   | 4.16      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 74899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 76000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.82e+05 |\n",
            "|    critic_loss     | 1.3e+07   |\n",
            "|    ent_coef        | 22.4      |\n",
            "|    ent_coef_loss   | 1.64      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 75899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 76    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1547  |\n",
            "|    total_timesteps | 76000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=77000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 77000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.87e+05 |\n",
            "|    critic_loss     | 1.4e+07   |\n",
            "|    ent_coef        | 21        |\n",
            "|    ent_coef_loss   | 0.0521    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 76899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 78000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.64e+05 |\n",
            "|    critic_loss     | 1.6e+07   |\n",
            "|    ent_coef        | 17.8      |\n",
            "|    ent_coef_loss   | -1.62     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 77899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=79000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 79000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.79e+05 |\n",
            "|    critic_loss     | 1.3e+07   |\n",
            "|    ent_coef        | 19.1      |\n",
            "|    ent_coef_loss   | -5.89     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 78899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.81e+05 |\n",
            "|    critic_loss     | 1.64e+07  |\n",
            "|    ent_coef        | 23.3      |\n",
            "|    ent_coef_loss   | -0.756    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 79899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 80    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1628  |\n",
            "|    total_timesteps | 80000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=81000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 81000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.82e+05 |\n",
            "|    critic_loss     | 1.48e+07  |\n",
            "|    ent_coef        | 22        |\n",
            "|    ent_coef_loss   | 1.27      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 80899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 82000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.88e+05 |\n",
            "|    critic_loss     | 1.55e+07  |\n",
            "|    ent_coef        | 25.6      |\n",
            "|    ent_coef_loss   | 0.518     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 81899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=83000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 83000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.68e+05 |\n",
            "|    critic_loss     | 1.31e+07  |\n",
            "|    ent_coef        | 25.9      |\n",
            "|    ent_coef_loss   | 0.219     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 82899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=84000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 84000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.77e+05 |\n",
            "|    critic_loss     | 4.87e+08  |\n",
            "|    ent_coef        | 26.8      |\n",
            "|    ent_coef_loss   | -0.367    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 83899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 84    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1710  |\n",
            "|    total_timesteps | 84000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=85000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 85000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.66e+05 |\n",
            "|    critic_loss     | 1.62e+07  |\n",
            "|    ent_coef        | 28.7      |\n",
            "|    ent_coef_loss   | 3.89      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 84899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 86000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.72e+05 |\n",
            "|    critic_loss     | 4.91e+08  |\n",
            "|    ent_coef        | 24.5      |\n",
            "|    ent_coef_loss   | 5.12      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 85899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=87000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 87000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.74e+05 |\n",
            "|    critic_loss     | 1.35e+07  |\n",
            "|    ent_coef        | 22.9      |\n",
            "|    ent_coef_loss   | -0.563    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 86899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 88000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.79e+05 |\n",
            "|    critic_loss     | 4.97e+08  |\n",
            "|    ent_coef        | 28.5      |\n",
            "|    ent_coef_loss   | 1.86      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 87899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 88    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1793  |\n",
            "|    total_timesteps | 88000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=89000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 89000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.82e+05 |\n",
            "|    critic_loss     | 1.41e+07  |\n",
            "|    ent_coef        | 33.9      |\n",
            "|    ent_coef_loss   | 0.484     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 88899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.85e+05 |\n",
            "|    critic_loss     | 1.16e+07  |\n",
            "|    ent_coef        | 37.2      |\n",
            "|    ent_coef_loss   | -0.743    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 89899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=91000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 91000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.81e+05 |\n",
            "|    critic_loss     | 5.07e+08  |\n",
            "|    ent_coef        | 28.7      |\n",
            "|    ent_coef_loss   | -0.136    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 90899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 92000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.77e+05 |\n",
            "|    critic_loss     | 1.85e+07  |\n",
            "|    ent_coef        | 29.5      |\n",
            "|    ent_coef_loss   | 5.45      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 91899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 92    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1874  |\n",
            "|    total_timesteps | 92000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=93000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 93000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.84e+05 |\n",
            "|    critic_loss     | 1.51e+07  |\n",
            "|    ent_coef        | 21.7      |\n",
            "|    ent_coef_loss   | -2.87     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 92899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 94000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.74e+05 |\n",
            "|    critic_loss     | 2.08e+07  |\n",
            "|    ent_coef        | 23.5      |\n",
            "|    ent_coef_loss   | 4.83      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 93899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=95000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 95000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.74e+05 |\n",
            "|    critic_loss     | 1.58e+07  |\n",
            "|    ent_coef        | 20.7      |\n",
            "|    ent_coef_loss   | 1.07      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 94899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 96000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.75e+05 |\n",
            "|    critic_loss     | 1.53e+07  |\n",
            "|    ent_coef        | 17.8      |\n",
            "|    ent_coef_loss   | -3.03     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 95899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 96    |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 1956  |\n",
            "|    total_timesteps | 96000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=97000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 97000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.96e+05 |\n",
            "|    critic_loss     | 5.15e+08  |\n",
            "|    ent_coef        | 22.4      |\n",
            "|    ent_coef_loss   | -5.25     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 96899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=98000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 98000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.82e+05 |\n",
            "|    critic_loss     | 6.36e+08  |\n",
            "|    ent_coef        | 26.8      |\n",
            "|    ent_coef_loss   | -1.12     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 97899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=99000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 99000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.87e+05 |\n",
            "|    critic_loss     | 1.37e+07  |\n",
            "|    ent_coef        | 26.2      |\n",
            "|    ent_coef_loss   | -1.39     |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 98899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.81e+05 |\n",
            "|    critic_loss     | 5.19e+08  |\n",
            "|    ent_coef        | 28        |\n",
            "|    ent_coef_loss   | 2.58      |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 99899     |\n",
            "----------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    episodes        | 100    |\n",
            "|    fps             | 49     |\n",
            "|    time_elapsed    | 2037   |\n",
            "|    total_timesteps | 100000 |\n",
            "-------------------------------\n",
            "mean_reward:3003000.00 +/- 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x576 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mean_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/fps</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/actor_loss</td><td>█▇▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▂▁▁▂▁▁</td></tr><tr><td>train/critic_loss</td><td>▁▁▁▁▃▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▆▇▁▇▁▇▁▇▁▁▁█▁▁▁▁██</td></tr><tr><td>train/ent_coef</td><td>▁▁▂▂▄▄▇█▅▄▄▄▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▃▄▄▄▅▅▄▄▅</td></tr><tr><td>train/ent_coef_loss</td><td>▅▄▁▁▃▅▆▆▇▅█▆▇█▇▆▇█▆▆▄▅▆▄██▅█▆▅▆▄▆▆▆▆██▅▇</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>1000.0</td></tr><tr><td>eval/mean_reward</td><td>3003000.0</td></tr><tr><td>global_step</td><td>100000</td></tr><tr><td>time/fps</td><td>49.0</td></tr><tr><td>train/actor_loss</td><td>-281224.8125</td></tr><tr><td>train/critic_loss</td><td>519465664.0</td></tr><tr><td>train/ent_coef</td><td>28.0086</td></tr><tr><td>train/ent_coef_loss</td><td>2.57584</td></tr><tr><td>train/learning_rate</td><td>0.0003</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">colorful-haze-9</strong>: <a href=\"https://wandb.ai/nishamdev/StockTrading/runs/3rxikqj0\" target=\"_blank\">https://wandb.ai/nishamdev/StockTrading/runs/3rxikqj0</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 3 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221124_222600-3rxikqj0/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#### 6.3 Running SAC algorithm over 100 time steps\n",
        "from stocktrade import Stocktrade\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    timesteps = 100000\n",
        "    algo = \"SAC\"\n",
        "    Stocktrade.stocktrade(algo,timesteps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment#6 - A2C over 100 steps (Hyper Parameter Tuning)\n",
        "\n",
        "**Experiment SAC algo with gamma=0.8 , learning_rate=0.000010 , ent_coef=0.3**"
      ],
      "metadata": {
        "id": "WVjs3MFYLg3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stocktrade1 import Stocktrade\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    timesteps = 100000\n",
        "    algorithm = \"SAC\"\n",
        "    hparam = \"T\"\n",
        "    Stocktrade.stocktrade(algorithm,timesteps,hparam)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3hhD8O1TGoqR",
        "outputId": "e5cb7912-0401-444b-d51f-16770566010a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Reinforcement-learning-Live-Trading/wandb/run-20221124_230035-3qxhwiyg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/nishamdev/StockTrading/runs/3qxhwiyg\" target=\"_blank\">eager-gorge-10</a></strong> to <a href=\"https://wandb.ai/nishamdev/StockTrading\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
            "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to runs/3qxhwiyg/SAC_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=1000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.63    |\n",
            "|    critic_loss     | 3.38e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 899      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -5.14    |\n",
            "|    critic_loss     | 2.53e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 1899     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -12.5    |\n",
            "|    critic_loss     | 2.46e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 2899     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -29.1    |\n",
            "|    critic_loss     | 2.48e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 3899     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 4    |\n",
            "|    fps             | 50   |\n",
            "|    time_elapsed    | 78   |\n",
            "|    total_timesteps | 4000 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=5000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -59.2    |\n",
            "|    critic_loss     | 2.44e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 4899     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -103     |\n",
            "|    critic_loss     | 2.35e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 5899     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=7000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -158     |\n",
            "|    critic_loss     | 2.12e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 6899     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -224     |\n",
            "|    critic_loss     | 1.79e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 7899     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 8    |\n",
            "|    fps             | 50   |\n",
            "|    time_elapsed    | 157  |\n",
            "|    total_timesteps | 8000 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=9000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -303     |\n",
            "|    critic_loss     | 1.95e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 8899     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -397     |\n",
            "|    critic_loss     | 1.95e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 9899     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=11000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 11000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -506     |\n",
            "|    critic_loss     | 1.54e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 10899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -632     |\n",
            "|    critic_loss     | 1.77e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 11899    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 12    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 237   |\n",
            "|    total_timesteps | 12000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=13000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 13000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -776     |\n",
            "|    critic_loss     | 1.5e+07  |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 12899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -940     |\n",
            "|    critic_loss     | 2.04e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 13899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 15000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.12e+03 |\n",
            "|    critic_loss     | 1.74e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 14899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 16000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.32e+03 |\n",
            "|    critic_loss     | 1.61e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 15899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 16    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 316   |\n",
            "|    total_timesteps | 16000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=17000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 17000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.55e+03 |\n",
            "|    critic_loss     | 1.46e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 16899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.8e+03 |\n",
            "|    critic_loss     | 1.62e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 17899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=19000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 19000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.07e+03 |\n",
            "|    critic_loss     | 1.51e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 18899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.36e+03 |\n",
            "|    critic_loss     | 1.89e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 19899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 20    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 395   |\n",
            "|    total_timesteps | 20000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=21000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 21000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.68e+03 |\n",
            "|    critic_loss     | 1.33e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 20899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 22000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.03e+03 |\n",
            "|    critic_loss     | 1.26e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 21899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=23000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 23000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -3.4e+03 |\n",
            "|    critic_loss     | 1.31e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 22899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 24000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -3.8e+03 |\n",
            "|    critic_loss     | 1.44e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 23899    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 24    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 474   |\n",
            "|    total_timesteps | 24000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 25000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.23e+03 |\n",
            "|    critic_loss     | 1.6e+07   |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 24899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 26000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.68e+03 |\n",
            "|    critic_loss     | 1.43e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 25899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=27000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 27000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.16e+03 |\n",
            "|    critic_loss     | 1.4e+07   |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 26899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 28000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.68e+03 |\n",
            "|    critic_loss     | 1.33e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 27899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 28    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 552   |\n",
            "|    total_timesteps | 28000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=29000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 29000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.21e+03 |\n",
            "|    critic_loss     | 1.3e+07   |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 28899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.78e+03 |\n",
            "|    critic_loss     | 1.16e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 29899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=31000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 31000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.39e+03 |\n",
            "|    critic_loss     | 1.25e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 30899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 32000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.01e+03 |\n",
            "|    critic_loss     | 1.06e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 31899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 32    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 631   |\n",
            "|    total_timesteps | 32000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=33000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 33000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.65e+03 |\n",
            "|    critic_loss     | 9.82e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 32899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 34000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.34e+03 |\n",
            "|    critic_loss     | 1.08e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 33899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 35000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1e+04   |\n",
            "|    critic_loss     | 1.1e+07  |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 34899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 36000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.08e+04 |\n",
            "|    critic_loss     | 1.06e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 35899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 36    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 710   |\n",
            "|    total_timesteps | 36000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=37000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 37000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.15e+04 |\n",
            "|    critic_loss     | 1.05e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 36899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 38000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.23e+04 |\n",
            "|    critic_loss     | 9.21e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 37899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=39000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 39000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.3e+04 |\n",
            "|    critic_loss     | 1.25e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 38899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.37e+04 |\n",
            "|    critic_loss     | 1.19e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 39899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 40    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 788   |\n",
            "|    total_timesteps | 40000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=41000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 41000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.45e+04 |\n",
            "|    critic_loss     | 1.34e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 40899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 42000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.51e+04 |\n",
            "|    critic_loss     | 1.24e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 41899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=43000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 43000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.56e+04 |\n",
            "|    critic_loss     | 1.02e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 42899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 44000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.6e+04 |\n",
            "|    critic_loss     | 1.21e+07 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 43899    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 44    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 867   |\n",
            "|    total_timesteps | 44000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 45000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.63e+04 |\n",
            "|    critic_loss     | 1.13e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 44899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 46000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.65e+04 |\n",
            "|    critic_loss     | 1.1e+07   |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 45899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=47000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 47000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.67e+04 |\n",
            "|    critic_loss     | 1.19e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 46899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 48000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.68e+04 |\n",
            "|    critic_loss     | 6.93e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 47899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 48    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 948   |\n",
            "|    total_timesteps | 48000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=49000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 49000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.68e+04 |\n",
            "|    critic_loss     | 7.79e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 48899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.69e+04 |\n",
            "|    critic_loss     | 7.99e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 49899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=51000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 51000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.69e+04 |\n",
            "|    critic_loss     | 1.22e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 50899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 52000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.69e+04 |\n",
            "|    critic_loss     | 1.06e+07  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 51899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 52    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1026  |\n",
            "|    total_timesteps | 52000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=53000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 53000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.68e+04 |\n",
            "|    critic_loss     | 7.51e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 52899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 54000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.68e+04 |\n",
            "|    critic_loss     | 8.04e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 53899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=55000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 55000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.68e+04 |\n",
            "|    critic_loss     | 8.22e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 54899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 56000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.67e+04 |\n",
            "|    critic_loss     | 7.48e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 55899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 56    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1105  |\n",
            "|    total_timesteps | 56000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=57000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 57000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.67e+04 |\n",
            "|    critic_loss     | 7.99e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 56899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 58000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.66e+04 |\n",
            "|    critic_loss     | 6.19e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 57899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=59000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 59000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.66e+04 |\n",
            "|    critic_loss     | 5.94e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 58899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.65e+04 |\n",
            "|    critic_loss     | 6.82e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 59899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 60    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1184  |\n",
            "|    total_timesteps | 60000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=61000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 61000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.65e+04 |\n",
            "|    critic_loss     | 6.9e+06   |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 60899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 62000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.65e+04 |\n",
            "|    critic_loss     | 9.08e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 61899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=63000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 63000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.64e+04 |\n",
            "|    critic_loss     | 6.14e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 62899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 64000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.64e+04 |\n",
            "|    critic_loss     | 6.61e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 63899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 64    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1264  |\n",
            "|    total_timesteps | 64000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=65000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 65000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.64e+04 |\n",
            "|    critic_loss     | 6.11e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 64899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 66000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.63e+04 |\n",
            "|    critic_loss     | 5.33e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 65899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 67000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.62e+04 |\n",
            "|    critic_loss     | 6.58e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 66899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 68000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.62e+04 |\n",
            "|    critic_loss     | 6.14e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 67899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 68    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1343  |\n",
            "|    total_timesteps | 68000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=69000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 69000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.61e+04 |\n",
            "|    critic_loss     | 5.85e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 68899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.61e+04 |\n",
            "|    critic_loss     | 4.26e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 69899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=71000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 71000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.61e+04 |\n",
            "|    critic_loss     | 5.81e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 70899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 72000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.6e+04 |\n",
            "|    critic_loss     | 5.35e+06 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 71899    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 72    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1422  |\n",
            "|    total_timesteps | 72000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=73000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 73000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.6e+04 |\n",
            "|    critic_loss     | 4.55e+06 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 72899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=74000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 74000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.59e+04 |\n",
            "|    critic_loss     | 4.21e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 73899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 75000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.59e+04 |\n",
            "|    critic_loss     | 5.02e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 74899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 76000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.58e+04 |\n",
            "|    critic_loss     | 3.72e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 75899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 76    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1502  |\n",
            "|    total_timesteps | 76000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=77000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 77000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.58e+04 |\n",
            "|    critic_loss     | 5.63e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 76899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 78000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.57e+04 |\n",
            "|    critic_loss     | 4.16e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 77899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=79000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 79000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.57e+04 |\n",
            "|    critic_loss     | 4.08e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 78899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.56e+04 |\n",
            "|    critic_loss     | 3.39e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 79899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 80    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1581  |\n",
            "|    total_timesteps | 80000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=81000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 81000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.56e+04 |\n",
            "|    critic_loss     | 4.05e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 80899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 82000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.56e+04 |\n",
            "|    critic_loss     | 3.96e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 81899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=83000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 83000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.55e+04 |\n",
            "|    critic_loss     | 4.6e+06   |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 82899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=84000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 84000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.55e+04 |\n",
            "|    critic_loss     | 4.47e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 83899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 84    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1660  |\n",
            "|    total_timesteps | 84000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=85000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 85000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.55e+04 |\n",
            "|    critic_loss     | 3.72e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 84899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 86000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.54e+04 |\n",
            "|    critic_loss     | 3.51e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 85899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=87000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 87000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.54e+04 |\n",
            "|    critic_loss     | 2.86e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 86899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 88000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.54e+04 |\n",
            "|    critic_loss     | 3.33e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 87899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 88    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1739  |\n",
            "|    total_timesteps | 88000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=89000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 89000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.53e+04 |\n",
            "|    critic_loss     | 3.59e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 88899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.53e+04 |\n",
            "|    critic_loss     | 3.72e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 89899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=91000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 91000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.53e+04 |\n",
            "|    critic_loss     | 4.08e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 90899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 92000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.54e+04 |\n",
            "|    critic_loss     | 3.93e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 91899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 92    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1818  |\n",
            "|    total_timesteps | 92000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=93000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 93000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.56e+04 |\n",
            "|    critic_loss     | 3.93e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 92899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 94000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.58e+04 |\n",
            "|    critic_loss     | 3.81e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 93899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=95000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | 3e+06    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.6e+04 |\n",
            "|    critic_loss     | 3.04e+06 |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    learning_rate   | 1e-05    |\n",
            "|    n_updates       | 94899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 96000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.62e+04 |\n",
            "|    critic_loss     | 3.23e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 95899     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 96    |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1897  |\n",
            "|    total_timesteps | 96000 |\n",
            "------------------------------\n",
            "Eval num_timesteps=97000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 97000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.64e+04 |\n",
            "|    critic_loss     | 3.15e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 96899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=98000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 98000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.66e+04 |\n",
            "|    critic_loss     | 3.22e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 97899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=99000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 99000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.66e+04 |\n",
            "|    critic_loss     | 3.15e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 98899     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=3003000.00 +/- 0.00\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 1e+03     |\n",
            "|    mean_reward     | 3e+06     |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.67e+04 |\n",
            "|    critic_loss     | 4.34e+06  |\n",
            "|    ent_coef        | 0.3       |\n",
            "|    learning_rate   | 1e-05     |\n",
            "|    n_updates       | 99899     |\n",
            "----------------------------------\n",
            "-------------------------------\n",
            "| time/              |        |\n",
            "|    episodes        | 100    |\n",
            "|    fps             | 50     |\n",
            "|    time_elapsed    | 1976   |\n",
            "|    total_timesteps | 100000 |\n",
            "-------------------------------\n",
            "mean_reward:3003000.00 +/- 0.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x576 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mean_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/fps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/actor_loss</td><td>██████▇▇▇▇▆▆▅▄▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▁▁▁</td></tr><tr><td>train/critic_loss</td><td>█▆▆▄▄▄▄▄▃▃▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/ent_coef</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>1000.0</td></tr><tr><td>eval/mean_reward</td><td>3003000.0</td></tr><tr><td>global_step</td><td>100000</td></tr><tr><td>time/fps</td><td>50.0</td></tr><tr><td>train/actor_loss</td><td>-16688.41797</td></tr><tr><td>train/critic_loss</td><td>4337561.0</td></tr><tr><td>train/ent_coef</td><td>0.3</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">eager-gorge-10</strong>: <a href=\"https://wandb.ai/nishamdev/StockTrading/runs/3qxhwiyg\" target=\"_blank\">https://wandb.ai/nishamdev/StockTrading/runs/3qxhwiyg</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 3 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221124_230035-3qxhwiyg/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run this first to hyperparamer experiments"
      ],
      "metadata": {
        "id": "4yjxCgL2ZQRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Reinforcement-learning-Live-Trading/stocktrade1.py\n",
        "\n",
        "\"\"\"\n",
        "Final Project: Stock Trading -Reinforcement Learning\n",
        "Author: Nisha Mohan Devadiga\n",
        "        Akanksha Rawat\n",
        "        Karishma Kuria.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import sys\n",
        "# from stable_baselines3.common.policies import MlpPolicy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3 import PPO, A2C , SAC\n",
        "\n",
        "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback\n",
        "from stable_baselines3.common.callbacks import EvalCallback \n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "from env.stock_trading_env import StockTradingEnv\n",
        "\n",
        "\n",
        "TOTAL_TIMESTEPS = 100000 # 1000000 for PPO, 100 for A2C , 100 for SAC \n",
        "\n",
        "class Stocktrade:\n",
        "    \n",
        "    def stocktrade(algo,timesteps,hparam ):\n",
        "    \n",
        "        config = {\n",
        "             \"policy_type\": \"MlpPolicy\",\n",
        "             \"total_timesteps\": timesteps,\n",
        "             \"learning_rate\": 0.01,\n",
        "              \"momentum\" : 0.2,\n",
        "              \"env_name\": \"stock-v0\",\n",
        "             }\n",
        "         \n",
        "         # Initialize wandb\n",
        "        run = wandb.init(\n",
        "               project=\"StockTrading\",\n",
        "               config=config,\n",
        "               sync_tensorboard=True,  # auto-upload StockTrading's tensorboard metrics\n",
        "               monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "               save_code=True,  # optional\n",
        "             )\n",
        "     \n",
        "        # Load data\n",
        "        df = pd.read_csv('./data/AAPL.csv')\n",
        "        df = df.sort_values('Date')\n",
        "     \n",
        "        # The algorithms require a vectorized environment to run\n",
        "        env = DummyVecEnv([lambda: StockTradingEnv(df)])\n",
        "         \n",
        "         # env = make_vec_env([lambda: StockTradingEnv(df)],n_envs=10)\n",
        "         # print(\"Check env: \",check_env(StockTradingEnv(df)))\n",
        "         \n",
        "         # Instantiate the agent model\n",
        "        if algo == \"PPO\":\n",
        "          if hparam == \"T\":\n",
        "               model = PPO(config[\"policy_type\"],env,gamma=0.80,learning_rate=0.000010,ent_coef=0.3, verbose=1, tensorboard_log=f\"runs/{run.id}\")\n",
        "          else: \n",
        "               model = PPO(config[\"policy_type\"],env, verbose=1, tensorboard_log=f\"runs/{run.id}\")\n",
        "     \n",
        "        elif algo == \"A2C\":\n",
        "          if hparam == 'T':\n",
        "             model = A2C(config[\"policy_type\"],env,gamma=0.80 , learning_rate=0.000010 , ent_coef=0.3,verbose=1, tensorboard_log=f\"runs/{run.id}\")\n",
        "          else:\n",
        "             model = A2C(config[\"policy_type\"],env, verbose=1, tensorboard_log=f\"runs/{run.id}\")\n",
        "                \n",
        "        elif algo == \"SAC\":\n",
        "          policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[400, 300]))\n",
        "\n",
        "          if hparam == 'T':\n",
        "             model = SAC(config[\"policy_type\"], env, gamma=0.80 , learning_rate=0.000010 , ent_coef=0.3, policy_kwargs=policy_kwargs, verbose=1,tensorboard_log=f\"runs/{run.id}\" )\n",
        "          else:\n",
        "             model = SAC(config[\"policy_type\"], env, policy_kwargs=policy_kwargs, verbose=1,tensorboard_log=f\"runs/{run.id}\" )\n",
        "\n",
        "        else:\n",
        "             print(\"Program Terminated. Please enter valid algorithm - PPO / A2C / SAC\")\n",
        "             sys.exit()\n",
        "     \n",
        "        checkpoint_callback = CheckpointCallback(save_freq=1e4, save_path='./model_checkpoints/')\n",
        "        eval_callback = EvalCallback(env,\n",
        "                                      best_model_save_path=f\"./model_checkpoints/best_model/\",\n",
        "                                      log_path=\"./logs/results\",\n",
        "                                      eval_freq=1000,verbose=1)\n",
        "        wandb_callback = WandbCallback(\n",
        "             model_save_path=f\"models/{run.id}\",\n",
        "                         # gradient_save_freq=1000\n",
        "             verbose=1,\n",
        "             )\n",
        "         # Create the callback list\n",
        "        callback = CallbackList([checkpoint_callback, eval_callback,wandb_callback])\n",
        "         \n",
        "         # Train the agent\n",
        "        model.learn(\n",
        "             total_timesteps=config[\"total_timesteps\"],\n",
        "             callback=callback,\n",
        "             )\n",
        "         # print rewards\n",
        "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "        print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "     \n",
        "        obs = env.reset()\n",
        "     \n",
        "        for i in range(200):\n",
        "             action, _states = model.predict(obs)\n",
        "             obs, rewards, done, info = env.step(action)\n",
        "             env.render(mode='file')\n",
        "        env.render(mode='static')\n",
        "        run.finish()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Initialize parser\n",
        "    parser = argparse.ArgumentParser(description='Stock Trade Inference')\n",
        "    parser.add_argument('-a', \"--algorithm\", type=str, required=True,\n",
        "                        default=\"PPO\", help=\"mention algorithm  - PPO / A2C / SAC\")\n",
        "    parser.add_argument('-t', \"--timesteps\", type=int, required=True,\n",
        "                        default=\"PPO\", help=\"enter timesteps\")\n",
        "    parser.add_argument('-h', \"--hparam\", type=str, required=True,\n",
        "                        default=\"F\", help=\"enter T/F\")\n",
        "    args = parser.parse_args()\n",
        "    algo = args.algorithm\n",
        "    timesteps = args.timesteps\n",
        "    hparam = args.hparam\n",
        "    print(\"hparam:\", hparam)\n",
        "    Stocktrade.stocktrade(algo,timesteps,hparam)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BBPm3BhOjzS",
        "outputId": "463c9239-2c6b-4b04-c878-b28874535067"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Reinforcement-learning-Live-Trading/stocktrade1.py\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "4yjxCgL2ZQRo"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "069f97044e2f4c7b96ca3c5da019d5ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7aeeb78053f8467ebe86ac237884a012",
              "IPY_MODEL_259631baca6641b0923baa7d760a5b9c"
            ],
            "layout": "IPY_MODEL_01316c1804dd480ab714d3978a72a83a"
          }
        },
        "7aeeb78053f8467ebe86ac237884a012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc0e854f234644aa8702575e15a67c03",
            "placeholder": "​",
            "style": "IPY_MODEL_c78bdd79a91446879b8d8333bfeb6c32",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "259631baca6641b0923baa7d760a5b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_454ffe0fb1ab4a48b13ec4caf0339f94",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f00f85464cb4664af2bcf7448823cb5",
            "value": 1
          }
        },
        "01316c1804dd480ab714d3978a72a83a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc0e854f234644aa8702575e15a67c03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c78bdd79a91446879b8d8333bfeb6c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "454ffe0fb1ab4a48b13ec4caf0339f94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f00f85464cb4664af2bcf7448823cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}