Using cpu device
-------------------------------------
| time/                 |           |
|    fps                | 1600      |
|    iterations         | 100       |
|    time_elapsed       | 0         |
|    total_timesteps    | 500       |
| train/                |           |
|    entropy_loss       | -2.83     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 99        |
|    policy_loss        | 5.45e+03  |
|    std                | 0.998     |
|    value_loss         | 3.43e+06  |
-------------------------------------
/Users/nishadevadiga/Downloads/Stock-Trading-Environment-master/env/StockTradingEnv.py:97: RuntimeWarning: invalid value encountered in double_scalars
  self.cost_basis = (
/Users/nishadevadiga/miniforge3/envs/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
/Users/nishadevadiga/Downloads/Stock-Trading-Environment-master/env/StockTradingEnv.py:97: RuntimeWarning: invalid value encountered in double_scalars
  self.cost_basis = (
/Users/nishadevadiga/miniforge3/envs/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=1000, episode_reward=15296199.15 +/- 765.13
Episode length: 5354.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.35e+03 |
|    mean_reward        | 1.53e+07 |
| time/                 |          |
|    total_timesteps    | 1000     |
| train/                |          |
|    entropy_loss       | -2.84    |
|    explained_variance | 5.66e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | 1.24e+04 |
|    std                | 1        |
|    value_loss         | 1.17e+07 |
------------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 122  |
|    iterations      | 200  |
|    time_elapsed    | 8    |
|    total_timesteps | 1000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 178      |
|    iterations         | 300      |
|    time_elapsed       | 8        |
|    total_timesteps    | 1500     |
| train/                |          |
|    entropy_loss       | -2.83    |
|    explained_variance | 2.38e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | 5.16e+03 |
|    std                | 0.999    |
|    value_loss         | 3.46e+06 |
------------------------------------
Eval num_timesteps=2000, episode_reward=15001446.04 +/- 792.29
Episode length: 5354.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 5.35e+03  |
|    mean_reward        | 1.5e+07   |
| time/                 |           |
|    total_timesteps    | 2000      |
| train/                |           |
|    entropy_loss       | -2.84     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 399       |
|    policy_loss        | 8.8e+03   |
|    std                | 1         |
|    value_loss         | 1.06e+07  |
-------------------------------------
-----------------------------
| time/              |      |
|    fps             | 123  |
|    iterations      | 400  |
|    time_elapsed    | 16   |
|    total_timesteps | 2000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 151      |
|    iterations         | 500      |
|    time_elapsed       | 16       |
|    total_timesteps    | 2500     |
| train/                |          |
|    entropy_loss       | -2.83    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 499      |
|    policy_loss        | 3.35e+03 |
|    std                | 0.998    |
|    value_loss         | 2.6e+06  |
------------------------------------
Eval num_timesteps=3000, episode_reward=21503002.50 +/- 0.00
Episode length: 5354.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.35e+03 |
|    mean_reward        | 2.15e+07 |
| time/                 |          |
|    total_timesteps    | 3000     |
| train/                |          |
|    entropy_loss       | -2.81    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 599      |
|    policy_loss        | 9.29e+03 |
|    std                | 0.986    |
|    value_loss         | 1.06e+07 |
------------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 123  |
|    iterations      | 600  |
|    time_elapsed    | 24   |
|    total_timesteps | 3000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 142      |
|    iterations         | 700      |
|    time_elapsed       | 24       |
|    total_timesteps    | 3500     |
| train/                |          |
|    entropy_loss       | -2.8     |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 699      |
|    policy_loss        | 3.56e+03 |
|    std                | 0.979    |
|    value_loss         | 2.59e+06 |
------------------------------------
Eval num_timesteps=4000, episode_reward=21503002.50 +/- 0.00
Episode length: 5354.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 5.35e+03  |
|    mean_reward        | 2.15e+07  |
| time/                 |           |
|    total_timesteps    | 4000      |
| train/                |           |
|    entropy_loss       | -2.78     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 799       |
|    policy_loss        | 8.33e+03  |
|    std                | 0.973     |
|    value_loss         | 1.05e+07  |
-------------------------------------
-----------------------------
| time/              |      |
|    fps             | 123  |
|    iterations      | 800  |
|    time_elapsed    | 32   |
|    total_timesteps | 4000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 137      |
|    iterations         | 900      |
|    time_elapsed       | 32       |
|    total_timesteps    | 4500     |
| train/                |          |
|    entropy_loss       | -2.77    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 899      |
|    policy_loss        | 4.69e+03 |
|    std                | 0.966    |
|    value_loss         | 3.31e+06 |
------------------------------------
Eval num_timesteps=5000, episode_reward=21503002.50 +/- 0.00
Episode length: 5354.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.35e+03 |
|    mean_reward        | 2.15e+07 |
| time/                 |          |
|    total_timesteps    | 5000     |
| train/                |          |
|    entropy_loss       | -2.76    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 999      |
|    policy_loss        | 1.02e+04 |
|    std                | 0.961    |
|    value_loss         | 1.05e+07 |
------------------------------------
-----------------------------
| time/              |      |
|    fps             | 123  |
|    iterations      | 1000 |
|    time_elapsed    | 40   |
|    total_timesteps | 5000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 135      |
|    iterations         | 1100     |
|    time_elapsed       | 40       |
|    total_timesteps    | 5500     |
| train/                |          |
|    entropy_loss       | -2.75    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 1099     |
|    policy_loss        | 4e+03    |
|    std                | 0.956    |
|    value_loss         | 2.6e+06  |
------------------------------------
Eval num_timesteps=6000, episode_reward=21503002.50 +/- 0.00
Episode length: 5354.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.35e+03 |
|    mean_reward        | 2.15e+07 |
| time/                 |          |
|    total_timesteps    | 6000     |
| train/                |          |
|    entropy_loss       | -2.73    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1199     |
|    policy_loss        | 9.07e+03 |
|    std                | 0.95     |
|    value_loss         | 1.17e+07 |
------------------------------------
-----------------------------
| time/              |      |
|    fps             | 124  |
|    iterations      | 1200 |
|    time_elapsed    | 48   |
|    total_timesteps | 6000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 133      |
|    iterations         | 1300     |
|    time_elapsed       | 48       |
|    total_timesteps    | 6500     |
| train/                |          |
|    entropy_loss       | -2.73    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 1299     |
|    policy_loss        | 3.64e+03 |
|    std                | 0.947    |
|    value_loss         | 2.6e+06  |
------------------------------------
Eval num_timesteps=7000, episode_reward=21503002.50 +/- 0.00
Episode length: 5354.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 5.35e+03  |
|    mean_reward        | 2.15e+07  |
| time/                 |           |
|    total_timesteps    | 7000      |
| train/                |           |
|    entropy_loss       | -2.74     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1399      |
|    policy_loss        | 1.02e+04  |
|    std                | 0.952     |
|    value_loss         | 1.06e+07  |
-------------------------------------
-----------------------------
| time/              |      |
|    fps             | 124  |
|    iterations      | 1400 |
|    time_elapsed    | 56   |
|    total_timesteps | 7000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 132      |
|    iterations         | 1500     |
|    time_elapsed       | 56       |
|    total_timesteps    | 7500     |
| train/                |          |
|    entropy_loss       | -2.75    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1499     |
|    policy_loss        | 3.5e+03  |
|    std                | 0.957    |
|    value_loss         | 3.78e+06 |
------------------------------------
Eval num_timesteps=8000, episode_reward=21503002.50 +/- 0.00
Episode length: 5354.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 5.35e+03  |
|    mean_reward        | 2.15e+07  |
| time/                 |           |
|    total_timesteps    | 8000      |
| train/                |           |
|    entropy_loss       | -2.74     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1599      |
|    policy_loss        | 9.52e+03  |
|    std                | 0.954     |
|    value_loss         | 1.05e+07  |
-------------------------------------
-----------------------------
| time/              |      |
|    fps             | 124  |
|    iterations      | 1600 |
|    time_elapsed    | 64   |
|    total_timesteps | 8000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 131      |
|    iterations         | 1700     |
|    time_elapsed       | 64       |
|    total_timesteps    | 8500     |
| train/                |          |
|    entropy_loss       | -2.73    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 1699     |
|    policy_loss        | 4.48e+03 |
|    std                | 0.948    |
|    value_loss         | 2.6e+06  |
------------------------------------
Eval num_timesteps=9000, episode_reward=21503002.50 +/- 0.00
Episode length: 5354.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 5.35e+03  |
|    mean_reward        | 2.15e+07  |
| time/                 |           |
|    total_timesteps    | 9000      |
| train/                |           |
|    entropy_loss       | -2.73     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1799      |
|    policy_loss        | 6.73e+03  |
|    std                | 0.949     |
|    value_loss         | 1.05e+07  |
-------------------------------------
Stopping training because there was no new best model in the last 4 evaluations