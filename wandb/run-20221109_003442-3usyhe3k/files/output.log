/Users/nishadevadiga/miniforge3/envs/venv/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float16
  logger.warn(
/Users/nishadevadiga/Downloads/Stock-Trading-Environment-master/env/stock_trading_env.py:109: RuntimeWarning: invalid value encountered in double_scalars
  self.cost_basis = (
/Users/nishadevadiga/miniforge3/envs/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Using cpu device
Logging to runs/A2C_1
------------------------------------
| time/                 |          |
|    fps                | 1459     |
|    iterations         | 100      |
|    time_elapsed       | 0        |
|    total_timesteps    | 500      |
| train/                |          |
|    entropy_loss       | -2.82    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | 3.78e+03 |
|    std                | 0.992    |
|    value_loss         | 2.77e+06 |
------------------------------------
Eval num_timesteps=1000, episode_reward=21503002.50 +/- 0.00
Episode length: 5354.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.35e+03 |
|    mean_reward        | 2.15e+07 |
| time/                 |          |
|    total_timesteps    | 1000     |
| train/                |          |
|    entropy_loss       | -2.83    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | 7.66e+03 |
|    std                | 0.994    |
|    value_loss         | 1.06e+07 |
------------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 120  |
|    iterations      | 200  |
|    time_elapsed    | 8    |
|    total_timesteps | 1000 |
-----------------------------
-------------------------------------
| time/                 |           |
|    fps                | 174       |
|    iterations         | 300       |
|    time_elapsed       | 8         |
|    total_timesteps    | 1500      |
| train/                |           |
|    entropy_loss       | -2.82     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 299       |
|    policy_loss        | 4.56e+03  |
|    std                | 0.993     |
|    value_loss         | 2.6e+06   |
-------------------------------------
/Users/nishadevadiga/Downloads/Stock-Trading-Environment-master/env/stock_trading_env.py:109: RuntimeWarning: invalid value encountered in double_scalars
  self.cost_basis = (
/Users/nishadevadiga/miniforge3/envs/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=2000, episode_reward=14416571.77 +/- 251.87
Episode length: 5354.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.35e+03 |
|    mean_reward        | 1.44e+07 |
| time/                 |          |
|    total_timesteps    | 2000     |
| train/                |          |
|    entropy_loss       | -2.82    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 399      |
|    policy_loss        | 6.79e+03 |
|    std                | 0.99     |
|    value_loss         | 1.05e+07 |
------------------------------------
-----------------------------
| time/              |      |
|    fps             | 121  |
|    iterations      | 400  |
|    time_elapsed    | 16   |
|    total_timesteps | 2000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 149      |
|    iterations         | 500      |
|    time_elapsed       | 16       |
|    total_timesteps    | 2500     |
| train/                |          |
|    entropy_loss       | -2.81    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 499      |
|    policy_loss        | 5.66e+03 |
|    std                | 0.986    |
|    value_loss         | 4.06e+06 |
------------------------------------
Eval num_timesteps=3000, episode_reward=14431058.67 +/- 143.77
Episode length: 5354.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 5.35e+03  |
|    mean_reward        | 1.44e+07  |
| time/                 |           |
|    total_timesteps    | 3000      |
| train/                |           |
|    entropy_loss       | -2.8      |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 599       |
|    policy_loss        | 7.27e+03  |
|    std                | 0.983     |
|    value_loss         | 1.09e+07  |
-------------------------------------
-----------------------------
| time/              |      |
|    fps             | 121  |
|    iterations      | 600  |
|    time_elapsed    | 24   |
|    total_timesteps | 3000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 139      |
|    iterations         | 700      |
|    time_elapsed       | 25       |
|    total_timesteps    | 3500     |
| train/                |          |
|    entropy_loss       | -2.81    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 699      |
|    policy_loss        | 3.63e+03 |
|    std                | 0.984    |
|    value_loss         | 2.61e+06 |
------------------------------------
Eval num_timesteps=4000, episode_reward=14551361.35 +/- 183.70
Episode length: 5354.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.35e+03 |
|    mean_reward        | 1.46e+07 |
| time/                 |          |
|    total_timesteps    | 4000     |
| train/                |          |
|    entropy_loss       | -2.82    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 799      |
|    policy_loss        | 8.21e+03 |
|    std                | 0.991    |
|    value_loss         | 1.06e+07 |
------------------------------------
-----------------------------
| time/              |      |
|    fps             | 120  |
|    iterations      | 800  |
|    time_elapsed    | 33   |
|    total_timesteps | 4000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 134      |
|    iterations         | 900      |
|    time_elapsed       | 33       |
|    total_timesteps    | 4500     |
| train/                |          |
|    entropy_loss       | -2.82    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 899      |
|    policy_loss        | 6.28e+03 |
|    std                | 0.991    |
|    value_loss         | 3.02e+06 |
------------------------------------
Eval num_timesteps=5000, episode_reward=14564900.04 +/- 2459.16
Episode length: 5354.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 5.35e+03  |
|    mean_reward        | 1.46e+07  |
| time/                 |           |
|    total_timesteps    | 5000      |
| train/                |           |
|    entropy_loss       | -2.8      |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 999       |
|    policy_loss        | 7.34e+03  |
|    std                | 0.983     |
|    value_loss         | 1.1e+07   |
-------------------------------------
-----------------------------
| time/              |      |
|    fps             | 120  |
|    iterations      | 1000 |
|    time_elapsed    | 41   |
|    total_timesteps | 5000 |
-----------------------------
-------------------------------------
| time/                 |           |
|    fps                | 131       |
|    iterations         | 1100      |
|    time_elapsed       | 41        |
|    total_timesteps    | 5500      |
| train/                |           |
|    entropy_loss       | -2.81     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1099      |
|    policy_loss        | 4.99e+03  |
|    std                | 0.986     |
|    value_loss         | 3.32e+06  |
-------------------------------------
Eval num_timesteps=6000, episode_reward=21503002.50 +/- 0.00
Episode length: 5354.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 5.35e+03  |
|    mean_reward        | 2.15e+07  |
| time/                 |           |
|    total_timesteps    | 6000      |
| train/                |           |
|    entropy_loss       | -2.8      |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1199      |
|    policy_loss        | 7.29e+03  |
|    std                | 0.983     |
|    value_loss         | 1.05e+07  |
-------------------------------------
-----------------------------
| time/              |      |
|    fps             | 120  |
|    iterations      | 1200 |
|    time_elapsed    | 49   |
|    total_timesteps | 6000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 129      |
|    iterations         | 1300     |
|    time_elapsed       | 50       |
|    total_timesteps    | 6500     |
| train/                |          |
|    entropy_loss       | -2.81    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 1299     |
|    policy_loss        | 3.09e+03 |
|    std                | 0.983    |
|    value_loss         | 2.65e+06 |
------------------------------------
Eval num_timesteps=7000, episode_reward=21503002.50 +/- 0.00
Episode length: 5354.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.35e+03 |
|    mean_reward        | 2.15e+07 |
| time/                 |          |
|    total_timesteps    | 7000     |
| train/                |          |
|    entropy_loss       | -2.81    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1399     |
|    policy_loss        | 8.45e+03 |
|    std                | 0.985    |
|    value_loss         | 1.05e+07 |
------------------------------------
-----------------------------
| time/              |      |
|    fps             | 119  |
|    iterations      | 1400 |
|    time_elapsed    | 58   |
|    total_timesteps | 7000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 127      |
|    iterations         | 1500     |
|    time_elapsed       | 58       |
|    total_timesteps    | 7500     |
| train/                |          |
|    entropy_loss       | -2.81    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1499     |
|    policy_loss        | 4.24e+03 |
|    std                | 0.984    |
|    value_loss         | 2.6e+06  |
------------------------------------
Eval num_timesteps=8000, episode_reward=14430948.91 +/- 249.20
Episode length: 5354.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.35e+03 |
|    mean_reward        | 1.44e+07 |
| time/                 |          |
|    total_timesteps    | 8000     |
| train/                |          |
|    entropy_loss       | -2.81    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1599     |
|    policy_loss        | 8.84e+03 |
|    std                | 0.986    |
|    value_loss         | 1.05e+07 |
------------------------------------
-----------------------------
| time/              |      |
|    fps             | 119  |
|    iterations      | 1600 |
|    time_elapsed    | 67   |
|    total_timesteps | 8000 |
-----------------------------
------------------------------------
| time/                 |          |
|    fps                | 126      |
|    iterations         | 1700     |
|    time_elapsed       | 67       |
|    total_timesteps    | 8500     |
| train/                |          |
|    entropy_loss       | -2.8     |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 1699     |
|    policy_loss        | 6.18e+03 |
|    std                | 0.983    |
|    value_loss         | 3.43e+06 |
------------------------------------
Eval num_timesteps=9000, episode_reward=14452232.17 +/- 174.86
Episode length: 5354.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 5.35e+03  |
|    mean_reward        | 1.45e+07  |
| time/                 |           |
|    total_timesteps    | 9000      |
| train/                |           |
|    entropy_loss       | -2.81     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1799      |
|    policy_loss        | 1.21e+04  |
|    std                | 0.986     |
|    value_loss         | 1.33e+07  |
-------------------------------------
Stopping training because there was no new best model in the last 4 evaluations